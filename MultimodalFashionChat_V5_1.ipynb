{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReynadelYolo/ML_Projects/blob/main/MultimodalFashionChat_V5_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EqKVzwhSabDv",
        "outputId": "c07f13c0-ca56-45d7-fbe3-29dc7280d9ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/803.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m405.7/405.7 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m310.8/310.8 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m110.8/110.8 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m915.7/915.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m139.1/139.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#CELL 1:  DEPENDENCIES\n",
        "\n",
        "# ML & Deep Learning\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q transformers datasets\n",
        "\n",
        "# Image Processing\n",
        "!pip install -q opencv-python-headless Pillow\n",
        "\n",
        "# Audio Processing & Recognition\n",
        "!pip install -q openai-whisper librosa soundfile  # or faster-whisper\n",
        "\n",
        "# NLP\n",
        "!pip install -q sentence-transformers\n",
        "\n",
        "# Vector Search & Similarity\n",
        "!pip install -q faiss-cpu\n",
        "\n",
        "# UI\n",
        "!pip install -q streamlit\n",
        "!pip install -q fastapi uvicorn  # optional for API backend\n",
        "\n",
        "# Data Visualization\n",
        "!pip install -q pandas numpy scikit-learn matplotlib seaborn tqdm\n",
        "\n",
        "# LLM & Efficient Inference (Optional - for intent/response generation)\n",
        "!pip install -q unsloth\n",
        "!pip install -q --no-deps xformers trl peft accelerate bitsandbytes\n",
        "\n",
        "print(\"‚úÖ All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2: IMPORTS AND CONFIGURATION\n",
        "\n",
        "# Core Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from typing import Optional, List, Dict, Tuple\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Computer Vision & Image Processing\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# NLP & Embeddings\n",
        "from sentence_transformers import SentenceTransformer  # CRITICAL ADDITION\n",
        "\n",
        "# Hugging Face\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    CLIPProcessor, CLIPModel,\n",
        "    BertTokenizer, BertModel,\n",
        "    WhisperProcessor, WhisperForConditionalGeneration,\n",
        "    AutoTokenizer, AutoModelForSeq2SeqLM  # For optional LLM\n",
        ")\n",
        "\n",
        "# Audio Processing\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "# Vector Search\n",
        "import faiss\n",
        "from faiss import IndexFlatIP, IndexFlatL2, normalize_L2\n",
        "\n",
        "# UI & API (for later integration - COMMENTED OUT TO AVOID ERRORS)\n",
        "# import streamlit as st  # Uncomment when building UI\n",
        "# from fastapi import FastAPI, UploadFile, File  # Uncomment for API\n",
        "\n",
        "# Logging Configuration\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Device Configuration\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üñ•Ô∏è Using device: {DEVICE}\")\n",
        "\n",
        "# Configuration - ALIGNED WITH MULTIMODAL CHATBOT DOCUMENT\n",
        "CONFIG = {\n",
        "    # Vision Models (Document Section 3.1)\n",
        "    \"CLIP_MODEL\": \"openai/clip-vit-base-patch32\",\n",
        "\n",
        "    # Text Models (Document Section 3.3)\n",
        "    \"SENTENCE_MODEL\": \"all-MiniLM-L6-v2\",  # CRITICAL: Document specifies this\n",
        "    \"BERT_MODEL\": \"bert-base-uncased\",  # For optional intent classification\n",
        "\n",
        "    # Speech Models (Document Section 3.2)\n",
        "    \"WHISPER_MODEL\": \"openai/whisper-small\",  # FIXED: Document recommends small for accuracy/speed balance\n",
        "    # \"FASTER_WHISPER\": True,  # Option to use faster-whisper later\n",
        "\n",
        "    # Optional LLM (for intent/response generation - Document mentions but not required)\n",
        "    \"LLM_MODEL\": \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",  # Optional\n",
        "\n",
        "    # Dimensions & Limits\n",
        "    \"CLIP_EMBEDDING_DIM\": 512,\n",
        "    \"SENTENCE_EMBEDDING_DIM\": 384,  # For all-MiniLM-L6-v2\n",
        "    \"MAX_TEXT_LENGTH\": 128,\n",
        "    \"IMAGE_SIZE\": 224,  # CLIP standard input size\n",
        "\n",
        "    # Search & Results\n",
        "    \"TOP_K_RESULTS\": 10,\n",
        "    \"FAISS_INDEX_TYPE\": \"IndexFlatIP\",  # Inner Product for cosine similarity\n",
        "\n",
        "    # Intent Classification (if not using LLM)\n",
        "    \"NUM_INTENTS\": 6  # Updated to match document intents\n",
        "}\n",
        "\n",
        "# Intent labels (ALIGNED WITH DOCUMENT SECTION 3.3)\n",
        "INTENT_LABELS = {\n",
        "    0: \"FIND_SIMILAR\",       # \"find_similar\" from document\n",
        "    1: \"FILTER_PRICE\",       # \"filter_price\" from document\n",
        "    2: \"FILTER_COLOR\",       # \"filter_color\" from document\n",
        "    3: \"COMPARE_PRODUCTS\",   # \"compare_products\" from document\n",
        "    4: \"GET_PRODUCT_INFO\",   # \"faq\" equivalent from document\n",
        "    5: \"FILTER_CATEGORY\"     # Additional useful filter\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Configuration loaded!\")"
      ],
      "metadata": {
        "id": "W1r4v0cg0qFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: DATA LOADING & PREPROCESSING - HYBRID APPROACH\n",
        "\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import random\n",
        "\n",
        "# ================================\n",
        "# CONFIGURATION\n",
        "# ================================\n",
        "DATASET_MODE = \"hybrid\"  # Options: \"product\", \"fashionpedia\", \"hybrid\", \"synthetic\"\n",
        "SYNTHETIC_PRICES = True   # Generate prices for Fashionpedia items\n",
        "SAMPLE_SIZE = 5000        # Limit samples for faster processing (None for all)\n",
        "\n",
        "print(f\"üìä Dataset Mode: {DATASET_MODE}\")\n",
        "print(f\"üí∞ Synthetic Prices: {SYNTHETIC_PRICES}\")\n",
        "\n",
        "# ================================\n",
        "# 1. LOAD FASHIONPEDIA DATASET\n",
        "# ================================\n",
        "fashionpedia_df = None\n",
        "if DATASET_MODE in [\"fashionpedia\", \"hybrid\"]:\n",
        "    print(\"\\nüì• Loading Fashionpedia dataset...\")\n",
        "    try:\n",
        "        # Load Fashionpedia dataset\n",
        "        fashionpedia = load_dataset(\"detection-datasets/fashionpedia\", split=\"train\")\n",
        "\n",
        "        # Convert to pandas for easier manipulation\n",
        "        fashionpedia_df = fashionpedia.to_pandas()\n",
        "\n",
        "        # Take a sample if specified\n",
        "        if SAMPLE_SIZE and len(fashionpedia_df) > SAMPLE_SIZE:\n",
        "            fashionpedia_df = fashionpedia_df.sample(SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
        "\n",
        "        # Extract basic information from annotations\n",
        "        print(\"üîç Processing Fashionpedia annotations...\")\n",
        "\n",
        "        # Parse the complex annotations\n",
        "        def parse_fashionpedia_annotations(ann):\n",
        "            \"\"\"Extract useful information from Fashionpedia annotations\"\"\"\n",
        "            if not ann or len(ann) == 0:\n",
        "                return {\"categories\": [], \"attributes\": [], \"object_count\": 0}\n",
        "\n",
        "            categories = []\n",
        "            attributes = []\n",
        "\n",
        "            for obj in ann:\n",
        "                if 'category_name' in obj:\n",
        "                    categories.append(obj['category_name'])\n",
        "                if 'attribute_names' in obj:\n",
        "                    attributes.extend(obj['attribute_names'])\n",
        "\n",
        "            return {\n",
        "                \"categories\": list(set(categories)),\n",
        "                \"attributes\": list(set(attributes)),\n",
        "                \"object_count\": len(ann)\n",
        "            }\n",
        "\n",
        "        # Apply parsing\n",
        "        annotations_data = fashionpedia_df['objects'].apply(parse_fashionpedia_annotations)\n",
        "\n",
        "        # Create new columns\n",
        "        fashionpedia_df['categories'] = annotations_data.apply(lambda x: x['categories'])\n",
        "        fashionpedia_df['attributes'] = annotations_data.apply(lambda x: x['attributes'])\n",
        "        fashionpedia_df['object_count'] = annotations_data.apply(lambda x: x['object_count'])\n",
        "\n",
        "        # Create text description from categories and attributes\n",
        "        fashionpedia_df['description'] = fashionpedia_df.apply(\n",
        "            lambda row: f\"{', '.join(row['categories'])} with {', '.join(row['attributes'][:5])}\"\n",
        "            if row['categories'] and row['attributes'] else \"Fashion item\",\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        # Create product display name\n",
        "        fashionpedia_df['productDisplayName'] = fashionpedia_df['categories'].apply(\n",
        "            lambda cats: f\"{cats[0] if cats else 'Fashion Item'} from Fashionpedia\"\n",
        "        )\n",
        "\n",
        "        # Generate synthetic prices if needed\n",
        "        if SYNTHETIC_PRICES:\n",
        "            print(\"üí∞ Generating synthetic prices for Fashionpedia items...\")\n",
        "            # Price ranges based on categories\n",
        "            price_ranges = {\n",
        "                'dress': (49.99, 199.99),\n",
        "                'shirt': (29.99, 89.99),\n",
        "                'pants': (39.99, 129.99),\n",
        "                'shoes': (59.99, 249.99),\n",
        "                'jacket': (79.99, 299.99),\n",
        "                'skirt': (34.99, 99.99),\n",
        "                'default': (19.99, 149.99)\n",
        "            }\n",
        "\n",
        "            def generate_price(categories):\n",
        "                if not categories:\n",
        "                    return round(random.uniform(29.99, 99.99), 2)\n",
        "\n",
        "                main_category = categories[0].lower()\n",
        "                for key in price_ranges:\n",
        "                    if key in main_category:\n",
        "                        min_price, max_price = price_ranges[key]\n",
        "                        return round(random.uniform(min_price, max_price), 2)\n",
        "\n",
        "                min_price, max_price = price_ranges['default']\n",
        "                return round(random.uniform(min_price, max_price), 2)\n",
        "\n",
        "            fashionpedia_df['price'] = fashionpedia_df['categories'].apply(generate_price)\n",
        "\n",
        "        # Create image path/URL (Fashionpedia uses COCO URLs)\n",
        "        fashionpedia_df['image_url'] = fashionpedia_df['coco_url']\n",
        "        fashionpedia_df['image_path'] = fashionpedia_df['image_url']  # Using URL as path for now\n",
        "\n",
        "        # Create color from attributes\n",
        "        color_keywords = ['red', 'blue', 'green', 'black', 'white', 'yellow',\n",
        "                         'pink', 'purple', 'orange', 'brown', 'gray', 'grey']\n",
        "\n",
        "        def extract_color(attributes):\n",
        "            if not attributes:\n",
        "                return random.choice(['black', 'white', 'blue', 'red'])\n",
        "            attrs_lower = [attr.lower() for attr in attributes]\n",
        "            for color in color_keywords:\n",
        "                if color in ' '.join(attrs_lower):\n",
        "                    return color\n",
        "            return random.choice(['black', 'white', 'blue', 'red'])\n",
        "\n",
        "        fashionpedia_df['baseColour'] = fashionpedia_df['attributes'].apply(extract_color)\n",
        "\n",
        "        # Create category columns\n",
        "        fashionpedia_df['masterCategory'] = fashionpedia_df['categories'].apply(\n",
        "            lambda cats: cats[0] if cats else 'Clothing'\n",
        "        )\n",
        "        fashionpedia_df['subCategory'] = fashionpedia_df['categories'].apply(\n",
        "            lambda cats: ', '.join(cats[1:3]) if len(cats) > 1 else 'General'\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ Fashionpedia loaded: {len(fashionpedia_df)} images\")\n",
        "        print(f\"   Objects per image: {fashionpedia_df['object_count'].mean():.1f} avg\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not load Fashionpedia: {e}\")\n",
        "        fashionpedia_df = None\n",
        "\n",
        "# ================================\n",
        "# 2. LOAD PRODUCT DATASET\n",
        "# ================================\n",
        "product_df = None\n",
        "if DATASET_MODE in [\"product\", \"hybrid\"]:\n",
        "    print(\"\\nüì• Loading Fashion Product Images dataset...\")\n",
        "    try:\n",
        "        # Load the product dataset\n",
        "        product_dataset = load_dataset(\"ashraq/fashion-product-images-small\", split=\"train\")\n",
        "        product_df = product_dataset.to_pandas()\n",
        "\n",
        "        # Take a sample if specified\n",
        "        if SAMPLE_SIZE and len(product_df) > SAMPLE_SIZE:\n",
        "            product_df = product_df.sample(SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
        "\n",
        "        # Create image paths\n",
        "        product_df['image_path'] = product_df['id'].apply(lambda x: f\"images/{x}.jpg\")\n",
        "\n",
        "        # Create combined text field\n",
        "        product_df['combined_text'] = product_df['productDisplayName'] + \". \" + product_df['productDescription'].fillna('')\n",
        "\n",
        "        # Standardize price\n",
        "        product_df['price'] = pd.to_numeric(product_df['price'], errors='coerce')\n",
        "        product_df['price'] = product_df['price'].fillna(product_df['price'].median())\n",
        "\n",
        "        # For Fashionpedia compatibility, create categories field\n",
        "        product_df['categories'] = product_df.apply(\n",
        "            lambda row: [row['masterCategory'], row['subCategory']]\n",
        "            if pd.notna(row['masterCategory']) and pd.notna(row['subCategory'])\n",
        "            else ['Clothing'],\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        # Create attributes list\n",
        "        product_df['attributes'] = product_df.apply(\n",
        "            lambda row: [row['baseColour'], row.get('season', 'all season'), row.get('usage', 'casual')]\n",
        "            if pd.notna(row['baseColour']) else ['standard'],\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ Product dataset loaded: {len(product_df)} products\")\n",
        "        print(f\"   Price range: ${product_df['price'].min():.2f} - ${product_df['price'].max():.2f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not load product dataset: {e}\")\n",
        "        product_df = None\n",
        "\n",
        "# ================================\n",
        "# 3. CREATE SYNTHETIC DATASET (Fallback)\n",
        "# ================================\n",
        "if DATASET_MODE == \"synthetic\" or (fashionpedia_df is None and product_df is None):\n",
        "    print(\"\\nüß™ Creating synthetic dataset...\")\n",
        "\n",
        "    synthetic_data = {\n",
        "        'id': [f'syn_{i:03d}' for i in range(100)],\n",
        "        'productDisplayName': [\n",
        "            'Slim Fit Black Jacket', 'Blue Denim Jeans', 'White Running Shoes',\n",
        "            'Red Summer Dress', 'Brown Leather Belt', 'Gray Wool Sweater',\n",
        "            'Navy Blue Blazer', 'Pink Silk Scarf', 'Green Cargo Pants',\n",
        "            'Beige Trench Coat'\n",
        "        ] * 10,\n",
        "        'productDescription': [\n",
        "            'Classic slim jacket made from premium cotton',\n",
        "            'Comfortable denim jeans with stretch fabric',\n",
        "            'Lightweight running shoes with cushion technology',\n",
        "            'Floral summer dress perfect for casual occasions',\n",
        "            'Genuine leather belt with polished metal buckle',\n",
        "            'Warm wool sweater for winter seasons',\n",
        "            'Formal blazer for business occasions',\n",
        "            'Elegant silk scarf with floral pattern',\n",
        "            'Utility cargo pants with multiple pockets',\n",
        "            'Classic trench coat for all weather conditions'\n",
        "        ] * 10,\n",
        "        'masterCategory': ['Jackets', 'Bottoms', 'Footwear', 'Dresses', 'Accessories',\n",
        "                          'Tops', 'Jackets', 'Accessories', 'Bottoms', 'Outerwear'] * 10,\n",
        "        'baseColour': ['Black', 'Blue', 'White', 'Red', 'Brown', 'Gray',\n",
        "                      'Navy', 'Pink', 'Green', 'Beige'] * 10,\n",
        "        'price': [69.99, 49.99, 89.99, 59.99, 29.99, 79.99, 129.99, 24.99, 54.99, 149.99] * 10,\n",
        "        'image_path': [f'synthetic/item_{i%10}.jpg' for i in range(100)],\n",
        "        'categories': [['Jackets'], ['Bottoms'], ['Footwear'], ['Dresses'], ['Accessories'],\n",
        "                      ['Tops'], ['Jackets'], ['Accessories'], ['Bottoms'], ['Outerwear']] * 10,\n",
        "        'attributes': [['Black', 'Cotton'], ['Blue', 'Denim'], ['White', 'Sports'],\n",
        "                      ['Red', 'Floral'], ['Brown', 'Leather'], ['Gray', 'Wool'],\n",
        "                      ['Navy', 'Formal'], ['Pink', 'Silk'], ['Green', 'Utility'],\n",
        "                      ['Beige', 'Classic']] * 10\n",
        "    }\n",
        "\n",
        "    synthetic_df = pd.DataFrame(synthetic_data)\n",
        "    synthetic_df['combined_text'] = synthetic_df['productDisplayName'] + \". \" + synthetic_df['productDescription']\n",
        "    synthetic_df['object_count'] = 1\n",
        "\n",
        "    print(f\"‚úÖ Synthetic dataset created: {len(synthetic_df)} products\")\n",
        "\n",
        "# ================================\n",
        "# 4. COMBINE DATASETS (Hybrid Mode)\n",
        "# ================================\n",
        "if DATASET_MODE == \"hybrid\" and fashionpedia_df is not None and product_df is not None:\n",
        "    print(\"\\nüîÑ Combining datasets for hybrid mode...\")\n",
        "\n",
        "    # Select common columns\n",
        "    common_cols = ['id', 'productDisplayName', 'description', 'masterCategory',\n",
        "                   'baseColour', 'price', 'image_path', 'categories', 'attributes', 'object_count']\n",
        "\n",
        "    # Prepare Fashionpedia data\n",
        "    fp_common = fashionpedia_df.copy()\n",
        "    fp_common['id'] = ['fp_' + str(i) for i in range(len(fp_common))]\n",
        "    fp_common['description'] = fp_common['description'].fillna('Fashion item from street photography')\n",
        "\n",
        "    # Prepare Product data\n",
        "    prod_common = product_df.copy()\n",
        "    prod_common['description'] = prod_common['combined_text']\n",
        "    prod_common['object_count'] = 1  # Product images typically show one item\n",
        "\n",
        "    # Merge\n",
        "    df = pd.concat([\n",
        "        prod_common[common_cols],\n",
        "        fp_common[common_cols]\n",
        "    ], ignore_index=True)\n",
        "\n",
        "    print(f\"‚úÖ Hybrid dataset created: {len(df)} total items\")\n",
        "    print(f\"   - Product images: {len(prod_common)}\")\n",
        "    print(f\"   - Fashionpedia images: {len(fp_common)}\")\n",
        "\n",
        "elif DATASET_MODE == \"product\" and product_df is not None:\n",
        "    df = product_df\n",
        "    print(f\"‚úÖ Using product dataset only: {len(df)} items\")\n",
        "\n",
        "elif DATASET_MODE == \"fashionpedia\" and fashionpedia_df is not None:\n",
        "    df = fashionpedia_df\n",
        "    print(f\"‚úÖ Using Fashionpedia dataset only: {len(df)} items\")\n",
        "\n",
        "elif DATASET_MODE == \"synthetic\" or (fashionpedia_df is None and product_df is None):\n",
        "    df = synthetic_df\n",
        "    print(f\"‚úÖ Using synthetic dataset: {len(df)} items\")\n",
        "\n",
        "# ================================\n",
        "# 5. FINAL PREPROCESSING\n",
        "# ================================\n",
        "print(\"\\nüîÑ Final preprocessing...\")\n",
        "\n",
        "# Ensure required columns exist\n",
        "required_cols = ['productDisplayName', 'description', 'price', 'image_path', 'categories', 'attributes']\n",
        "\n",
        "for col in required_cols:\n",
        "    if col not in df.columns:\n",
        "        print(f\"‚ö†Ô∏è Creating missing column: {col}\")\n",
        "        if col == 'productDisplayName':\n",
        "            df[col] = df.get('masterCategory', 'Fashion Item')\n",
        "        elif col == 'description':\n",
        "            df[col] = df.get('combined_text', 'No description available')\n",
        "        elif col == 'price':\n",
        "            df[col] = np.random.uniform(19.99, 199.99, len(df)).round(2)\n",
        "        elif col == 'image_path':\n",
        "            df[col] = f\"image_{df.index}.jpg\"\n",
        "        elif col == 'categories':\n",
        "            df[col] = df.get('masterCategory', 'Clothing').apply(lambda x: [x])\n",
        "        elif col == 'attributes':\n",
        "            df[col] = df.get('baseColour', 'standard').apply(lambda x: [x])\n",
        "\n",
        "# Create combined text for embeddings\n",
        "df['combined_text'] = df['productDisplayName'] + \". \" + df['description']\n",
        "\n",
        "# Color normalization (Document Step 3.3)\n",
        "color_mapping = {\n",
        "    'grey': 'gray',\n",
        "    'navy blue': 'navy',\n",
        "    'charcoal': 'dark gray',\n",
        "    'beige': 'tan',\n",
        "    'off white': 'white'\n",
        "}\n",
        "if 'baseColour' in df.columns:\n",
        "    df['baseColour'] = df['baseColour'].str.lower().replace(color_mapping)\n",
        "\n",
        "# Add unique ID if not present\n",
        "if 'id' not in df.columns:\n",
        "    df['id'] = [f'item_{i:04d}' for i in range(len(df))]\n",
        "\n",
        "# ================================\n",
        "# 6. SAVE AND DISPLAY RESULTS\n",
        "# ================================\n",
        "# Save metadata to CSV\n",
        "metadata_cols = ['id', 'productDisplayName', 'description', 'masterCategory',\n",
        "                 'baseColour', 'price', 'image_path', 'object_count']\n",
        "available_cols = [col for col in metadata_cols if col in df.columns]\n",
        "\n",
        "df[available_cols].to_csv('product_catalog.csv', index=False)\n",
        "print(f\"\\nüíæ Product catalog saved to 'product_catalog.csv'\")\n",
        "\n",
        "# Display statistics\n",
        "print(f\"\\nüìä DATASET STATISTICS:\")\n",
        "print(f\"   Total items: {len(df)}\")\n",
        "print(f\"   Price range: ${df['price'].min():.2f} - ${df['price'].max():.2f}\")\n",
        "print(f\"   Average price: ${df['price'].mean():.2f}\")\n",
        "print(f\"   Unique categories: {df['masterCategory'].nunique() if 'masterCategory' in df.columns else 'N/A'}\")\n",
        "print(f\"   Unique colors: {df['baseColour'].nunique() if 'baseColour' in df.columns else 'N/A'}\")\n",
        "\n",
        "# Sample display\n",
        "print(f\"\\nüìã SAMPLE ITEMS:\")\n",
        "sample = df.head(3)\n",
        "for idx, row in sample.iterrows():\n",
        "    source = \"Product\" if DATASET_MODE == \"product\" else \"Fashionpedia\" if DATASET_MODE == \"fashionpedia\" else \"Hybrid\"\n",
        "    print(f\"  {idx+1}. {row['productDisplayName'][:50]}...\")\n",
        "    print(f\"     Price: ${row['price']:.2f} | Color: {row.get('baseColour', 'N/A')} | Source: {source}\")\n",
        "\n",
        "# Optional visualization\n",
        "if len(df) > 10:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    # Price distribution\n",
        "    axes[0].hist(df['price'].dropna(), bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
        "    axes[0].set_xlabel('Price ($)')\n",
        "    axes[0].set_ylabel('Count')\n",
        "    axes[0].set_title(f'Price Distribution ({DATASET_MODE})')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Category distribution (top 10)\n",
        "    if 'masterCategory' in df.columns:\n",
        "        top_categories = df['masterCategory'].value_counts().head(10)\n",
        "        colors = plt.cm.Set3(np.linspace(0, 1, len(top_categories)))\n",
        "        axes[1].bar(range(len(top_categories)), top_categories.values, alpha=0.8, color=colors)\n",
        "        axes[1].set_xticks(range(len(top_categories)))\n",
        "        axes[1].set_xticklabels(top_categories.index, rotation=45, ha='right')\n",
        "        axes[1].set_ylabel('Count')\n",
        "        axes[1].set_title(f'Top Product Categories ({DATASET_MODE})')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Data loading and preprocessing COMPLETED!\")"
      ],
      "metadata": {
        "id": "kefXrhUp24k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3B: EDA - EXPLORATORY DATA ANALYSIS\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "print(\"üîç Starting EDA (Exploratory Data Analysis)...\")\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# ================================\n",
        "# 1. PREPARE EDA DATAFRAME\n",
        "# ================================\n",
        "print(\"\\nüìä Preparing EDA dataset...\")\n",
        "\n",
        "# Create a comprehensive DataFrame for analysis\n",
        "eda_data = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    item = {\n",
        "        'id': row.get('id', f'item_{idx}'),\n",
        "        'category': row.get('masterCategory', 'Unknown'),\n",
        "        'subcategory': row.get('subCategory', 'Unknown'),\n",
        "        'color': row.get('baseColour', 'Unknown'),\n",
        "        'price': float(row.get('price', 0)),\n",
        "        'source': 'Product' if DATASET_MODE in ['product', 'hybrid'] and idx < (len(df) - (len(fashionpedia_df) if 'fashionpedia_df' in locals() and fashionpedia_df is not None else 0)) else 'Fashionpedia' if DATASET_MODE in ['fashionpedia', 'hybrid'] else 'Synthetic',\n",
        "        'object_count': int(row.get('object_count', 1)),\n",
        "        'has_image': True if pd.notna(row.get('image_path')) else False\n",
        "    }\n",
        "\n",
        "    # Add description length (for text analysis)\n",
        "    if 'description' in row:\n",
        "        item['desc_length'] = len(str(row['description']))\n",
        "    elif 'combined_text' in row:\n",
        "        item['desc_length'] = len(str(row['combined_text']))\n",
        "    else:\n",
        "        item['desc_length'] = 0\n",
        "\n",
        "    eda_data.append(item)\n",
        "\n",
        "df_eda = pd.DataFrame(eda_data)\n",
        "\n",
        "print(f\"‚úÖ EDA DataFrame created: {len(df_eda)} items\")\n",
        "print(f\"   - Data sources: {df_eda['source'].value_counts().to_dict()}\")\n",
        "print(f\"   - Unique categories: {df_eda['category'].nunique()}\")\n",
        "\n",
        "# ================================\n",
        "# 2. BASIC STATISTICS\n",
        "# ================================\n",
        "print(\"\\nüìà Basic Statistics:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Numerical statistics\n",
        "print(\"\\nüí∞ PRICE ANALYSIS:\")\n",
        "print(f\"   Mean price: ${df_eda['price'].mean():.2f}\")\n",
        "print(f\"   Median price: ${df_eda['price'].median():.2f}\")\n",
        "print(f\"   Min price: ${df_eda['price'].min():.2f}\")\n",
        "print(f\"   Max price: ${df_eda['price'].max():.2f}\")\n",
        "print(f\"   Std price: ${df_eda['price'].std():.2f}\")\n",
        "\n",
        "print(\"\\nüìù TEXT ANALYSIS:\")\n",
        "print(f\"   Mean description length: {df_eda['desc_length'].mean():.0f} chars\")\n",
        "print(f\"   Max description length: {df_eda['desc_length'].max():.0f} chars\")\n",
        "\n",
        "print(\"\\nüéØ CATEGORY ANALYSIS:\")\n",
        "top_5_cats = df_eda['category'].value_counts().head(5)\n",
        "for cat, count in top_5_cats.items():\n",
        "    print(f\"   {cat}: {count} items ({count/len(df_eda)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nüé® COLOR ANALYSIS:\")\n",
        "top_5_colors = df_eda['color'].value_counts().head(5)\n",
        "for color, count in top_5_colors.items():\n",
        "    print(f\"   {color}: {count} items\")\n",
        "\n",
        "# ================================\n",
        "# 3. VISUALIZATIONS\n",
        "# ================================\n",
        "print(\"\\nüìä Creating visualizations...\")\n",
        "\n",
        "# Create a 2x2 subplot grid\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle(f'Multimodal Fashion Dataset Analysis ({DATASET_MODE.upper()} Mode)',\n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "\n",
        "# ================================\n",
        "# 3A. CATEGORY DISTRIBUTION\n",
        "# ================================\n",
        "top_categories = df_eda['category'].value_counts().head(15)\n",
        "\n",
        "# Use color palette based on dataset mode\n",
        "if DATASET_MODE == 'hybrid':\n",
        "    colors = plt.cm.Paired(np.linspace(0, 1, len(top_categories)))\n",
        "else:\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(top_categories)))\n",
        "\n",
        "axes[0, 0].barh(range(len(top_categories)), top_categories.values, color=colors, edgecolor='black')\n",
        "axes[0, 0].set_yticks(range(len(top_categories)))\n",
        "axes[0, 0].set_yticklabels(top_categories.index)\n",
        "axes[0, 0].set_xlabel('Number of Items')\n",
        "axes[0, 0].set_title('Top 15 Product Categories', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].invert_yaxis()  # Highest count on top\n",
        "axes[0, 0].grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Add percentage labels\n",
        "for i, v in enumerate(top_categories.values):\n",
        "    percentage = (v / len(df_eda)) * 100\n",
        "    axes[0, 0].text(v + 5, i, f'{v} ({percentage:.1f}%)', va='center', fontsize=9)\n",
        "\n",
        "# ================================\n",
        "# 3B. PRICE DISTRIBUTION\n",
        "# ================================\n",
        "# Histogram with KDE\n",
        "price_data = df_eda['price'].dropna()\n",
        "if len(price_data) > 0:\n",
        "    axes[0, 1].hist(price_data, bins=30, edgecolor='black', alpha=0.7,\n",
        "                   color='skyblue', density=True)\n",
        "\n",
        "    # Add KDE\n",
        "    from scipy import stats\n",
        "    if len(price_data) > 1:\n",
        "        kde = stats.gaussian_kde(price_data)\n",
        "        x_range = np.linspace(price_data.min(), price_data.max(), 100)\n",
        "        axes[0, 1].plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')\n",
        "\n",
        "    axes[0, 1].set_xlabel('Price ($)')\n",
        "    axes[0, 1].set_ylabel('Density')\n",
        "    axes[0, 1].set_title('Price Distribution with KDE', fontsize=12, fontweight='bold')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Add statistics text box\n",
        "    stats_text = f'Mean: ${price_data.mean():.2f}\\nMedian: ${price_data.median():.2f}\\nStd: ${price_data.std():.2f}'\n",
        "    axes[0, 1].text(0.95, 0.95, stats_text, transform=axes[0, 1].transAxes,\n",
        "                   fontsize=9, verticalalignment='top', horizontalalignment='right',\n",
        "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "# ================================\n",
        "# 3C. COLOR DISTRIBUTION\n",
        "# ================================\n",
        "top_colors = df_eda['color'].value_counts().head(10)\n",
        "color_map = {\n",
        "    'black': '#000000', 'white': '#FFFFFF', 'blue': '#0000FF', 'red': '#FF0000',\n",
        "    'green': '#008000', 'yellow': '#FFFF00', 'pink': '#FFC0CB', 'purple': '#800080',\n",
        "    'orange': '#FFA500', 'brown': '#A52A2A', 'gray': '#808080', 'navy': '#000080'\n",
        "}\n",
        "\n",
        "# Get actual colors for the bars\n",
        "bar_colors = []\n",
        "for color in top_colors.index:\n",
        "    color_lower = color.lower()\n",
        "    if color_lower in color_map:\n",
        "        bar_colors.append(color_map[color_lower])\n",
        "    else:\n",
        "        # Generate a random color for unknown colors\n",
        "        bar_colors.append(f'#{random.randint(0, 0xFFFFFF):06x}')\n",
        "\n",
        "axes[1, 0].bar(range(len(top_colors)), top_colors.values, color=bar_colors, edgecolor='black')\n",
        "axes[1, 0].set_xticks(range(len(top_colors)))\n",
        "axes[1, 0].set_xticklabels(top_colors.index, rotation=45, ha='right')\n",
        "axes[1, 0].set_xlabel('Color')\n",
        "axes[1, 0].set_ylabel('Count')\n",
        "axes[1, 0].set_title('Top 10 Product Colors', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# ================================\n",
        "# 3D. DATASET SOURCE COMPARISON\n",
        "# ================================\n",
        "if DATASET_MODE == 'hybrid':\n",
        "    source_counts = df_eda['source'].value_counts()\n",
        "\n",
        "    # Pie chart for source distribution\n",
        "    wedges, texts, autotexts = axes[1, 1].pie(\n",
        "        source_counts.values,\n",
        "        labels=source_counts.index,\n",
        "        autopct='%1.1f%%',\n",
        "        startangle=90,\n",
        "        colors=['#FF9999', '#66B2FF'],\n",
        "        explode=[0.05, 0.05] if len(source_counts) == 2 else [0.05]\n",
        "    )\n",
        "\n",
        "    # Make percentages bold\n",
        "    for autotext in autotexts:\n",
        "        autotext.set_color('white')\n",
        "        autotext.set_fontweight('bold')\n",
        "\n",
        "    axes[1, 1].set_title('Dataset Source Distribution', fontsize=12, fontweight='bold')\n",
        "\n",
        "    # Add summary text\n",
        "    summary_text = f\"Total Items: {len(df_eda)}\\n\"\n",
        "    for source, count in source_counts.items():\n",
        "        summary_text += f\"{source}: {count} items\\n\"\n",
        "\n",
        "    axes[1, 1].text(-1.5, -1.2, summary_text, fontsize=10,\n",
        "                   bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
        "else:\n",
        "    # If not hybrid, show description length distribution\n",
        "    desc_lengths = df_eda['desc_length']\n",
        "    axes[1, 1].hist(desc_lengths, bins=30, edgecolor='black', alpha=0.7, color='lightgreen')\n",
        "    axes[1, 1].set_xlabel('Description Length (characters)')\n",
        "    axes[1, 1].set_ylabel('Count')\n",
        "    axes[1, 1].set_title('Description Length Distribution', fontsize=12, fontweight='bold')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Add statistics\n",
        "    stats_text = f'Mean: {desc_lengths.mean():.0f}\\nMedian: {desc_lengths.median():.0f}\\nMax: {desc_lengths.max():.0f}'\n",
        "    axes[1, 1].text(0.95, 0.95, stats_text, transform=axes[1, 1].transAxes,\n",
        "                   fontsize=9, verticalalignment='top', horizontalalignment='right',\n",
        "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ================================\n",
        "# 4. ADDITIONAL ANALYSIS: PRICE BY CATEGORY\n",
        "# ================================\n",
        "print(\"\\nüì¶ Additional Analysis: Price by Category\")\n",
        "\n",
        "# Boxplot of prices by top categories\n",
        "top_8_cats = df_eda['category'].value_counts().head(8).index.tolist()\n",
        "df_top_cats = df_eda[df_eda['category'].isin(top_8_cats)]\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "boxplot = sns.boxplot(x='category', y='price', data=df_top_cats, palette='Set2')\n",
        "plt.title('Price Distribution by Top Categories', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Price ($)')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add mean price points\n",
        "means = df_top_cats.groupby('category')['price'].mean()\n",
        "for i, cat in enumerate(top_8_cats):\n",
        "    if cat in means.index:\n",
        "        plt.scatter(i, means[cat], color='red', s=100, zorder=5,\n",
        "                   label='Mean' if i == 0 else \"\")\n",
        "\n",
        "plt.legend(['Mean Price'])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ================================\n",
        "# 5. SAMPLE IMAGE DISPLAY\n",
        "# ================================\n",
        "print(\"\\nüñºÔ∏è Displaying Sample Images...\")\n",
        "\n",
        "def display_sample_images(df_sample, num_samples=6, title=\"Sample Products\"):\n",
        "    \"\"\"\n",
        "    Display sample images with their metadata\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Get random samples\n",
        "    if len(df_sample) > num_samples:\n",
        "        samples = df_sample.sample(num_samples, random_state=42)\n",
        "    else:\n",
        "        samples = df_sample\n",
        "\n",
        "    for i, (idx, row) in enumerate(samples.iterrows()):\n",
        "        plt.subplot(2, 3, i + 1)\n",
        "\n",
        "        # Try to load and display image if possible\n",
        "        try:\n",
        "            # For demo purposes, we'll create a placeholder image\n",
        "            # In real implementation, you would load from image_path\n",
        "\n",
        "            # Create a color block based on product color\n",
        "            color_name = row.get('color', 'gray').lower()\n",
        "            color = color_map.get(color_name, '#808080')  # Default to gray\n",
        "\n",
        "            # Create a simple image placeholder\n",
        "            img_array = np.zeros((100, 100, 3), dtype=np.uint8)\n",
        "\n",
        "            # Fill with the color (convert hex to RGB)\n",
        "            if color.startswith('#'):\n",
        "                rgb = tuple(int(color[i:i+2], 16) for i in (1, 3, 5))\n",
        "                img_array[:, :] = rgb\n",
        "\n",
        "            plt.imshow(img_array)\n",
        "\n",
        "            # Add text overlay\n",
        "            title_text = f\"{row.get('category', 'Item')}\\n${row.get('price', 0):.2f}\"\n",
        "            plt.text(50, 50, title_text, ha='center', va='center',\n",
        "                    fontsize=10, fontweight='bold', color='white',\n",
        "                    bbox=dict(boxstyle='round', facecolor='black', alpha=0.7))\n",
        "\n",
        "            # Add source label if hybrid\n",
        "            if DATASET_MODE == 'hybrid':\n",
        "                source_text = f\"Source: {row.get('source', 'Unknown')}\"\n",
        "                plt.text(50, 90, source_text, ha='center', va='center',\n",
        "                        fontsize=8, color='yellow', fontweight='bold')\n",
        "\n",
        "        except Exception as e:\n",
        "            # If image loading fails, show placeholder\n",
        "            plt.imshow(np.zeros((100, 100, 3), dtype=np.uint8))\n",
        "            plt.text(50, 50, \"Image\\nNot\\nAvailable\", ha='center', va='center',\n",
        "                    fontsize=9, color='white', fontweight='bold')\n",
        "\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"ID: {row.get('id', 'N/A')[:10]}...\", fontsize=9)\n",
        "\n",
        "    plt.suptitle(title, fontsize=16, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Display samples from different sources if hybrid\n",
        "if DATASET_MODE == 'hybrid':\n",
        "    print(\"üì∏ Product Dataset Samples:\")\n",
        "    product_samples = df_eda[df_eda['source'] == 'Product']\n",
        "    if len(product_samples) > 0:\n",
        "        display_sample_images(product_samples, 6, \"Sample Product Images (Clean Catalog)\")\n",
        "\n",
        "    print(\"üì∏ Fashionpedia Dataset Samples:\")\n",
        "    fp_samples = df_eda[df_eda['source'] == 'Fashionpedia']\n",
        "    if len(fp_samples) > 0:\n",
        "        display_sample_images(fp_samples, 6, \"Sample Fashionpedia Images (Street Fashion)\")\n",
        "else:\n",
        "    # Display general samples\n",
        "    display_sample_images(df_eda, 6, f\"Sample Images ({DATASET_MODE.upper()} Mode)\")\n",
        "\n",
        "# ================================\n",
        "# 6. CORRELATION ANALYSIS\n",
        "# ================================\n",
        "print(\"\\nüìê Correlation Analysis:\")\n",
        "\n",
        "# Create numerical features for correlation\n",
        "correlation_data = []\n",
        "for idx, row in df_eda.iterrows():\n",
        "    correlation_data.append({\n",
        "        'price': row['price'],\n",
        "        'desc_length': row['desc_length'],\n",
        "        'object_count': row['object_count'],\n",
        "        'category_encoded': hash(row['category']) % 100,  # Simple encoding\n",
        "        'has_image': 1 if row['has_image'] else 0\n",
        "    })\n",
        "\n",
        "df_corr = pd.DataFrame(correlation_data)\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = df_corr.corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ================================\n",
        "# 7. SUMMARY REPORT\n",
        "# ================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üìã EDA SUMMARY REPORT\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(f\"\\nüìä DATASET OVERVIEW:\")\n",
        "print(f\"   - Total Items: {len(df_eda)}\")\n",
        "print(f\"   - Unique Categories: {df_eda['category'].nunique()}\")\n",
        "print(f\"   - Unique Colors: {df_eda['color'].nunique()}\")\n",
        "print(f\"   - Data Sources: {dict(df_eda['source'].value_counts())}\")\n",
        "\n",
        "print(f\"\\nüí∞ PRICE INSIGHTS:\")\n",
        "print(f\"   - Price Range: ${df_eda['price'].min():.2f} - ${df_eda['price'].max():.2f}\")\n",
        "print(f\"   - Average Price: ${df_eda['price'].mean():.2f}\")\n",
        "print(f\"   - Most Common Price Range: ${df_eda['price'].median()-20:.2f} - ${df_eda['price'].median()+20:.2f}\")\n",
        "\n",
        "print(f\"\\nüè∑Ô∏è CATEGORY INSIGHTS:\")\n",
        "top_category = df_eda['category'].value_counts().index[0]\n",
        "top_category_count = df_eda['category'].value_counts().iloc[0]\n",
        "print(f\"   - Most Common Category: '{top_category}' ({top_category_count} items, {top_category_count/len(df_eda)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nüé® COLOR INSIGHTS:\")\n",
        "top_color = df_eda['color'].value_counts().index[0]\n",
        "top_color_count = df_eda['color'].value_counts().iloc[0]\n",
        "print(f\"   - Most Common Color: '{top_color}' ({top_color_count} items)\")\n",
        "\n",
        "print(f\"\\nüìù TEXT INSIGHTS:\")\n",
        "print(f\"   - Average Description Length: {df_eda['desc_length'].mean():.0f} characters\")\n",
        "print(f\"   - Description Coverage: {100 * (df_eda['desc_length'] > 0).sum() / len(df_eda):.1f}%\")\n",
        "\n",
        "print(f\"\\n‚úÖ EDA COMPLETE - Key Findings:\")\n",
        "print(f\"   1. Dataset is {'well-balanced' if df_eda['category'].value_counts().std() < 100 else 'imbalanced'} across categories\")\n",
        "print(f\"   2. Price distribution is {'normal' if abs(df_eda['price'].skew()) < 1 else 'skewed'}\")\n",
        "print(f\"   3. {'Hybrid dataset provides visual diversity' if DATASET_MODE == 'hybrid' else 'Single dataset provides consistency'}\")\n",
        "print(f\"   4. Metadata completeness: {100 * df_eda[['category', 'price', 'color']].notna().all(axis=1).sum() / len(df_eda):.0f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)"
      ],
      "metadata": {
        "id": "PCf-CS5d78ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: CLIP ENCODER FOR SEMANTIC SEARCH (PROFESSIONAL VERSION)\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import logging\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from torchvision import transforms\n",
        "from typing import List, Union, Optional\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class CLIPEncoder:\n",
        "    \"\"\"CLIP encoder for generating aligned image and text embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = CONFIG[\"CLIP_MODEL\"]):\n",
        "        self.device = DEVICE\n",
        "        self.model_name = model_name\n",
        "        self.embedding_dim = CONFIG[\"CLIP_EMBEDDING_DIM\"]\n",
        "\n",
        "        logger.info(f\"Loading CLIP model: {model_name}\")\n",
        "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
        "        self.model = CLIPModel.from_pretrained(model_name).to(self.device)\n",
        "        self.model.eval()\n",
        "        logger.info(\"CLIP model loaded successfully\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_image(self, image: Image.Image) -> np.ndarray:\n",
        "        inputs = self.processor(images=image, return_tensors=\"pt\").to(self.device)\n",
        "        features = self.model.get_image_features(**inputs)\n",
        "        features = features / features.norm(p=2, dim=-1, keepdim=True)\n",
        "        return features.cpu().numpy()[0]\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_text(self, text: str) -> np.ndarray:\n",
        "        inputs = self.processor(text=text, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
        "        features = self.model.get_text_features(**inputs)\n",
        "        features = features / features.norm(p=2, dim=-1, keepdim=True)\n",
        "        return features.cpu().numpy()[0]\n",
        "\n",
        "    def encode_multimodal(self, image=None, text: str = \"\") -> np.ndarray:\n",
        "        embeddings = []\n",
        "        if image is not None:\n",
        "            embeddings.append(self.encode_image(image))\n",
        "        if text and text.strip():\n",
        "            embeddings.append(self.encode_text(text))\n",
        "        if not embeddings:\n",
        "            return np.zeros(self.embedding_dim, dtype=np.float32)\n",
        "        combined = np.mean(embeddings, axis=0)\n",
        "        return (combined / (np.linalg.norm(combined) + 1e-8)).astype(np.float32)\n",
        "\n",
        "# Initialize\n",
        "print(\"=\"*50)\n",
        "print(\"INITIALIZING CLIP ENCODER\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "clip_encoder = CLIPEncoder()\n",
        "print(\"‚úÖ CLIP encoder ready!\")"
      ],
      "metadata": {
        "id": "XVlzJmKS92Pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: BERT INTENT CLASSIFIER (Simplified & Document-Compliant)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Update intent labels to match DOCUMENT requirements\n",
        "INTENT_LABELS = {\n",
        "    0: \"FIND_SIMILAR\",       # Document: \"find_similar\"\n",
        "    1: \"FILTER_PRICE\",       # Document: \"filter_price\"\n",
        "    2: \"FILTER_COLOR\",       # Document: \"filter_color\"\n",
        "    3: \"COMPARE_PRODUCTS\",   # Document: \"compare_products\"\n",
        "    4: \"GET_PRODUCT_INFO\",   # Document: \"faq\"\n",
        "}\n",
        "\n",
        "NUM_INTENTS = len(INTENT_LABELS)\n",
        "\n",
        "class IntentClassifier:\n",
        "    \"\"\"Simple intent classifier using BERT.\n",
        "\n",
        "    As per document Section 3.3: \"DistilBERT or BERT fine-tuned for intent labels\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"bert-base-uncased\"):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model_name = model_name\n",
        "\n",
        "        # Use pretrained classification model (simpler than custom class)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.model = BertForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=NUM_INTENTS\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.model.eval()\n",
        "        logger.info(f\"Intent classifier loaded: {model_name}\")\n",
        "\n",
        "    def predict(self, text: str) -> dict:\n",
        "        \"\"\"Predict intent from text.\"\"\"\n",
        "        # Simple text preprocessing (as per document Step 3.3)\n",
        "        text = str(text).strip().lower()\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            padding=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Predict\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            probs = torch.softmax(outputs.logits, dim=-1)\n",
        "            pred_idx = torch.argmax(probs, dim=-1).item()\n",
        "            confidence = probs[0][pred_idx].item()\n",
        "\n",
        "        return {\n",
        "            \"intent\": INTENT_LABELS[pred_idx],\n",
        "            \"confidence\": round(confidence, 3),\n",
        "            \"all_probs\": {INTENT_LABELS[i]: round(probs[0][i].item(), 3)\n",
        "                         for i in range(NUM_INTENTS)}\n",
        "        }\n",
        "\n",
        "    def batch_predict(self, texts: list) -> list:\n",
        "        \"\"\"Predict intents for multiple texts.\"\"\"\n",
        "        return [self.predict(text) for text in texts]\n",
        "\n",
        "# Initialize\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"INITIALIZING INTENT CLASSIFIER\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "intent_classifier = IntentClassifier()\n",
        "\n",
        "# Test with examples from document\n",
        "test_queries = [\n",
        "    \"find similar jackets\",           # FIND_SIMILAR\n",
        "    \"show me products under $100\",    # FILTER_PRICE\n",
        "    \"I want blue shirts\",             # FILTER_COLOR\n",
        "    \"compare these two products\",     # COMPARE_PRODUCTS\n",
        "    \"what material is this jacket?\"   # GET_PRODUCT_INFO\n",
        "]\n",
        "\n",
        "print(\"\\nüß™ Testing intent classification:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for query in test_queries:\n",
        "    result = intent_classifier.predict(query)\n",
        "    print(f\"'{query[:30]}...' ‚Üí {result['intent']} (conf: {result['confidence']:.2f})\")\n",
        "\n",
        "print(\"\\n‚úÖ Intent classifier ready!\")\n",
        "print(f\"   Model: bert-base-uncased\")\n",
        "print(f\"   Intents: {list(INTENT_LABELS.values())}\")\n",
        "print(f\"   Device: {intent_classifier.device}\")"
      ],
      "metadata": {
        "id": "WbjEXgkE-oxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: WHISPER AUDIO TRANSCRIPTION (Enhanced but Simple)\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import logging\n",
        "import librosa\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class AudioTranscriber:\n",
        "    \"\"\"Whisper-based audio transcription.\n",
        "\n",
        "    As per document Section 3.2: \"Whisper for robust STT\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = CONFIG[\"WHISPER_MODEL\"]):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model_name = model_name\n",
        "\n",
        "        logger.info(f\"Loading Whisper model: {model_name}\")\n",
        "        self.processor = WhisperProcessor.from_pretrained(model_name)\n",
        "        self.model = WhisperForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        logger.info(f\"‚úÖ Whisper model loaded on {self.device}\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def transcribe(self, audio_array: np.ndarray, sampling_rate: int = 16000) -> str:\n",
        "        \"\"\"Transcribe audio array to text.\"\"\"\n",
        "        try:\n",
        "            # Ensure 16kHz sampling rate (Whisper requirement)\n",
        "            if sampling_rate != 16000:\n",
        "                audio_array = librosa.resample(audio_array, orig_sr=sampling_rate, target_sr=16000)\n",
        "\n",
        "            # Process audio\n",
        "            input_features = self.processor(\n",
        "                audio_array,\n",
        "                sampling_rate=16000,\n",
        "                return_tensors=\"pt\"\n",
        "            ).input_features.to(self.device)\n",
        "\n",
        "            # Generate transcription\n",
        "            predicted_ids = self.model.generate(input_features)\n",
        "            transcription = self.processor.batch_decode(\n",
        "                predicted_ids,\n",
        "                skip_special_tokens=True\n",
        "            )[0]\n",
        "\n",
        "            return transcription.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Transcription failed: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def transcribe_file(self, audio_path: str) -> str:\n",
        "        \"\"\"Transcribe audio file to text.\"\"\"\n",
        "        try:\n",
        "            audio_array, sr = librosa.load(audio_path, sr=16000)\n",
        "            return self.transcribe(audio_array, sr)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Failed to load audio file: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def transcribe_microphone(self, duration: int = 5):\n",
        "        \"\"\"Optional: Record from microphone and transcribe.\"\"\"\n",
        "        try:\n",
        "            import sounddevice as sd\n",
        "            import soundfile as sf\n",
        "\n",
        "            logger.info(f\"üé§ Recording for {duration} seconds...\")\n",
        "            recording = sd.rec(int(duration * 16000), samplerate=16000, channels=1)\n",
        "            sd.wait()\n",
        "\n",
        "            # Convert to mono and float32\n",
        "            audio_array = recording.flatten().astype(np.float32)\n",
        "            return self.transcribe(audio_array, 16000)\n",
        "\n",
        "        except ImportError:\n",
        "            logger.warning(\"‚ö†Ô∏è Install sounddevice for microphone recording\")\n",
        "            return \"\"\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Microphone recording failed: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "# Initialize\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"INITIALIZING WHISPER TRANSCRIBER\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "audio_transcriber = AudioTranscriber()\n",
        "\n",
        "# Quick test with dummy audio\n",
        "print(\"\\nüß™ Quick test (simulated)...\")\n",
        "test_audio = np.random.randn(16000 * 3)  # 3 seconds of silence/random noise\n",
        "try:\n",
        "    # Note: Real audio would produce better results\n",
        "    test_text = audio_transcriber.transcribe(test_audio)\n",
        "    print(f\"   Test transcription: '{test_text[:50]}...'\")\n",
        "except:\n",
        "    print(\"   Test skipped (needs real audio for meaningful test)\")\n",
        "\n",
        "print(\"\\n‚úÖ Whisper transcriber ready!\")\n",
        "print(f\"   Model: {CONFIG['WHISPER_MODEL']}\")\n",
        "print(f\"   Device: {audio_transcriber.device}\")\n",
        "print(f\"   Supports: File loading, microphone (optional)\")"
      ],
      "metadata": {
        "id": "rZkiIcNf--BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: FAISS VECTOR STORE (Simplified & Document-Compliant)\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import logging\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class FAISSVectorStore:\n",
        "    \"\"\"Simple FAISS vector store for multimodal product search.\n",
        "\n",
        "    As per document: \"FAISS for fast similarity search in embedding spaces\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dimension: int = CONFIG[\"CLIP_EMBEDDING_DIM\"],\n",
        "                 index_type: str = \"IndexFlatIP\"):\n",
        "        \"\"\"\n",
        "        Initialize FAISS index.\n",
        "\n",
        "        Args:\n",
        "            dimension: Embedding dimension\n",
        "            index_type: \"IndexFlatIP\" for cosine similarity or \"IndexFlatL2\" for L2\n",
        "        \"\"\"\n",
        "        self.dimension = dimension\n",
        "        self.index_type = index_type\n",
        "\n",
        "        # Create index (Inner Product for cosine similarity)\n",
        "        if index_type == \"IndexFlatIP\":\n",
        "            self.index = faiss.IndexFlatIP(dimension)\n",
        "        else:\n",
        "            self.index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "        # Store metadata separately (more efficient than storing images)\n",
        "        self.metadata = []\n",
        "        self.embedding_count = 0\n",
        "\n",
        "        logger.info(f\"FAISS index initialized: {index_type}, dim={dimension}\")\n",
        "\n",
        "    def add_embeddings(self, embeddings: np.ndarray, metadata_list: List[Dict]):\n",
        "        \"\"\"\n",
        "        Add embeddings and their metadata to the index.\n",
        "\n",
        "        Args:\n",
        "            embeddings: numpy array of shape (n, dimension)\n",
        "            metadata_list: list of metadata dicts for each embedding\n",
        "        \"\"\"\n",
        "        if len(embeddings) != len(metadata_list):\n",
        "            raise ValueError(\"Number of embeddings must match number of metadata entries\")\n",
        "\n",
        "        # Normalize for cosine similarity (if using IndexFlatIP)\n",
        "        if self.index_type == \"IndexFlatIP\":\n",
        "            faiss.normalize_L2(embeddings)\n",
        "\n",
        "        # Add to index\n",
        "        self.index.add(embeddings.astype('float32'))\n",
        "        self.metadata.extend(metadata_list)\n",
        "        self.embedding_count = self.index.ntotal\n",
        "\n",
        "        logger.info(f\"Added {len(embeddings)} embeddings. Total: {self.embedding_count}\")\n",
        "\n",
        "    def search(self, query_embedding: np.ndarray, k: int = CONFIG[\"TOP_K_RESULTS\"]) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Search for similar embeddings.\n",
        "\n",
        "        Args:\n",
        "            query_embedding: numpy array of shape (dimension,) or (1, dimension)\n",
        "            k: number of results to return\n",
        "\n",
        "        Returns:\n",
        "            List of result dicts with metadata and scores\n",
        "        \"\"\"\n",
        "        if self.embedding_count == 0:\n",
        "            logger.warning(\"Index is empty\")\n",
        "            return []\n",
        "\n",
        "        # Reshape if needed\n",
        "        if query_embedding.ndim == 1:\n",
        "            query_embedding = query_embedding.reshape(1, -1)\n",
        "\n",
        "        # Normalize query for cosine similarity\n",
        "        if self.index_type == \"IndexFlatIP\":\n",
        "            faiss.normalize_L2(query_embedding)\n",
        "\n",
        "        # Search\n",
        "        distances, indices = self.index.search(query_embedding.astype('float32'), k)\n",
        "\n",
        "        # Format results\n",
        "        results = []\n",
        "        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
        "            if idx >= 0 and idx < len(self.metadata):  # Valid index\n",
        "                result = self.metadata[idx].copy()\n",
        "                result['score'] = float(distance)\n",
        "                result['rank'] = i + 1\n",
        "                results.append(result)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def hybrid_search(self, image_embedding: Optional[np.ndarray] = None,\n",
        "                     text_embedding: Optional[np.ndarray] = None,\n",
        "                     k: int = CONFIG[\"TOP_K_RESULTS\"],\n",
        "                     image_weight: float = 0.7,\n",
        "                     text_weight: float = 0.3) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Hybrid search combining image and text embeddings.\n",
        "\n",
        "        As per document Section 3.4: \"Late fusion of separate spaces\"\n",
        "\n",
        "        Args:\n",
        "            image_embedding: Optional image embedding\n",
        "            text_embedding: Optional text embedding\n",
        "            k: number of results\n",
        "            image_weight: weight for image similarity\n",
        "            text_weight: weight for text similarity\n",
        "        \"\"\"\n",
        "        if image_embedding is None and text_embedding is None:\n",
        "            raise ValueError(\"At least one embedding must be provided\")\n",
        "\n",
        "        # Get separate results if both provided\n",
        "        results_by_id = {}\n",
        "\n",
        "        # Image search\n",
        "        if image_embedding is not None:\n",
        "            image_results = self.search(image_embedding, k * 2)  # Get more for fusion\n",
        "            for result in image_results:\n",
        "                item_id = result.get('id', result.get('product_id', 'unknown'))\n",
        "                if item_id not in results_by_id:\n",
        "                    results_by_id[item_id] = {'image_score': 0, 'text_score': 0, 'metadata': result}\n",
        "                results_by_id[item_id]['image_score'] = result['score']\n",
        "\n",
        "        # Text search\n",
        "        if text_embedding is not None:\n",
        "            text_results = self.search(text_embedding, k * 2)\n",
        "            for result in text_results:\n",
        "                item_id = result.get('id', result.get('product_id', 'unknown'))\n",
        "                if item_id not in results_by_id:\n",
        "                    results_by_id[item_id] = {'image_score': 0, 'text_score': 0, 'metadata': result}\n",
        "                results_by_id[item_id]['text_score'] = result['score']\n",
        "\n",
        "        # Combine scores (weighted sum as per document)\n",
        "        combined_results = []\n",
        "        for item_id, scores in results_by_id.items():\n",
        "            combined_score = (scores['image_score'] * image_weight +\n",
        "                            scores['text_score'] * text_weight)\n",
        "\n",
        "            result = scores['metadata'].copy()\n",
        "            result['combined_score'] = combined_score\n",
        "            result['image_score'] = scores['image_score']\n",
        "            result['text_score'] = scores['text_score']\n",
        "            combined_results.append(result)\n",
        "\n",
        "        # Sort by combined score\n",
        "        combined_results.sort(key=lambda x: x['combined_score'], reverse=True)\n",
        "\n",
        "        return combined_results[:k]\n",
        "\n",
        "    def save(self, index_path: str, metadata_path: str):\n",
        "        \"\"\"Save index and metadata to disk.\"\"\"\n",
        "        # Save FAISS index\n",
        "        faiss.write_index(self.index, index_path)\n",
        "\n",
        "        # Save metadata\n",
        "        with open(metadata_path, 'wb') as f:\n",
        "            pickle.dump(self.metadata, f)\n",
        "\n",
        "        logger.info(f\"Saved index to {index_path}, metadata to {metadata_path}\")\n",
        "\n",
        "    def load(self, index_path: str, metadata_path: str):\n",
        "        \"\"\"Load index and metadata from disk.\"\"\"\n",
        "        # Load FAISS index\n",
        "        self.index = faiss.read_index(index_path)\n",
        "\n",
        "        # Load metadata\n",
        "        with open(metadata_path, 'rb') as f:\n",
        "            self.metadata = pickle.load(f)\n",
        "\n",
        "        self.embedding_count = self.index.ntotal\n",
        "        logger.info(f\"Loaded index with {self.embedding_count} embeddings\")\n",
        "\n",
        "    def get_stats(self) -> Dict:\n",
        "        \"\"\"Get index statistics.\"\"\"\n",
        "        return {\n",
        "            'total_embeddings': self.embedding_count,\n",
        "            'dimension': self.dimension,\n",
        "            'index_type': self.index_type,\n",
        "            'metadata_fields': list(self.metadata[0].keys()) if self.metadata else []\n",
        "        }\n",
        "\n",
        "# ================================\n",
        "# BUILD IMAGE INDEX FROM DATASET\n",
        "# ================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"BUILDING FAISS IMAGE INDEX\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Initialize image index\n",
        "image_index = FAISSVectorStore(\n",
        "    dimension=CONFIG[\"CLIP_EMBEDDING_DIM\"],\n",
        "    index_type=\"IndexFlatIP\"\n",
        ")\n",
        "\n",
        "# Prepare embeddings from our dataset (DF from Cell 3)\n",
        "print(f\"\\nüìä Preparing {len(df)} product embeddings...\")\n",
        "\n",
        "# For demo: Create dummy embeddings (in real use, encode actual images)\n",
        "# Note: Replace with actual image encoding when images are available\n",
        "dummy_embeddings = np.random.randn(len(df), CONFIG[\"CLIP_EMBEDDING_DIM\"]).astype('float32')\n",
        "\n",
        "# Prepare metadata\n",
        "metadata_list = []\n",
        "for idx, row in df.iterrows():\n",
        "    metadata = {\n",
        "        'id': row.get('id', f'item_{idx}'),\n",
        "        'name': row.get('productDisplayName', 'Unknown'),\n",
        "        'category': row.get('masterCategory', 'Unknown'),\n",
        "        'color': row.get('baseColour', 'Unknown'),\n",
        "        'price': float(row.get('price', 0)),\n",
        "        'description': row.get('description', '')[:100],  # Truncate\n",
        "        'image_path': row.get('image_path', ''),\n",
        "        'source': 'product' if DATASET_MODE in ['product', 'hybrid'] else 'fashionpedia'\n",
        "    }\n",
        "    metadata_list.append(metadata)\n",
        "\n",
        "# Add to index\n",
        "image_index.add_embeddings(dummy_embeddings, metadata_list)\n",
        "\n",
        "print(f\"‚úÖ Image index built: {image_index.embedding_count} embeddings\")\n",
        "\n",
        "# ================================\n",
        "# BUILD TEXT INDEX FROM DESCRIPTIONS\n",
        "# ================================\n",
        "print(\"\\nüìù Building text index from descriptions...\")\n",
        "\n",
        "# Initialize text index (using SentenceTransformer dimension)\n",
        "text_index = FAISSVectorStore(\n",
        "    dimension=CONFIG[\"SENTENCE_EMBEDDING_DIM\"],\n",
        "    index_type=\"IndexFlatIP\"\n",
        ")\n",
        "\n",
        "# For demo: Create dummy text embeddings\n",
        "# In real use: encode with SentenceTransformer\n",
        "dummy_text_embeddings = np.random.randn(len(df), CONFIG[\"SENTENCE_EMBEDDING_DIM\"]).astype('float32')\n",
        "\n",
        "# Add to text index (same metadata)\n",
        "text_index.add_embeddings(dummy_text_embeddings, metadata_list)\n",
        "\n",
        "print(f\"‚úÖ Text index built: {text_index.embedding_count} embeddings\")\n",
        "\n",
        "# ================================\n",
        "# TEST SEARCH FUNCTIONALITY\n",
        "# ================================\n",
        "print(\"\\nüß™ Testing search functionality...\")\n",
        "\n",
        "# Test 1: Image search (dummy query)\n",
        "print(\"\\n1. Image similarity search:\")\n",
        "dummy_image_query = np.random.randn(CONFIG[\"CLIP_EMBEDDING_DIM\"]).astype('float32')\n",
        "image_results = image_index.search(dummy_image_query, k=3)\n",
        "\n",
        "for i, result in enumerate(image_results):\n",
        "    print(f\"   {i+1}. {result['name'][:30]}... | Score: {result['score']:.3f} | ${result['price']}\")\n",
        "\n",
        "# Test 2: Hybrid search\n",
        "print(\"\\n2. Hybrid search (image + text):\")\n",
        "# Create dummy embeddings for both modalities\n",
        "dummy_text_query = np.random.randn(CONFIG[\"SENTENCE_EMBEDDING_DIM\"]).astype('float32')\n",
        "\n",
        "hybrid_results = image_index.hybrid_search(\n",
        "    image_embedding=dummy_image_query,\n",
        "    text_embedding=dummy_text_query,\n",
        "    k=3\n",
        ")\n",
        "\n",
        "for i, result in enumerate(hybrid_results):\n",
        "    print(f\"   {i+1}. {result['name'][:30]}... | Combined: {result['combined_score']:.3f}\")\n",
        "\n",
        "# Test 3: Index statistics\n",
        "print(\"\\n3. Index statistics:\")\n",
        "stats = image_index.get_stats()\n",
        "for key, value in stats.items():\n",
        "    if key != 'metadata_fields':\n",
        "        print(f\"   {key}: {value}\")\n",
        "\n",
        "print(\"\\n‚úÖ FAISS vector stores ready!\")\n",
        "print(f\"   - Image index: {image_index.embedding_count} vectors\")\n",
        "print(f\"   - Text index: {text_index.embedding_count} vectors\")\n",
        "print(f\"   - Supports: Similarity search, hybrid search, save/load\")"
      ],
      "metadata": {
        "id": "ENOqa2cp_4nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xk_NVo0vPLPH",
        "outputId": "ff043229-f793-4bca-e18c-3da6135f50e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Saved 3000 products, 3000 with images\n"
          ]
        }
      ],
      "source": [
        "# CELL 7B: SAVE INDEX & DATA\n",
        "\n",
        "import pickle\n",
        "import io\n",
        "from PIL import Image\n",
        "\n",
        "def image_to_bytes(img):\n",
        "    buf = io.BytesIO()\n",
        "    img.save(buf, format='JPEG', quality=85)\n",
        "    return buf.getvalue()\n",
        "\n",
        "# Get products with images\n",
        "save_data = {\n",
        "    'index': faiss.serialize_index(search_index.index),\n",
        "    'products': []\n",
        "}\n",
        "\n",
        "for i, meta in enumerate(search_index.metadata):\n",
        "    product = {\n",
        "        'name': meta['name'],\n",
        "        'category': meta['category'],\n",
        "        'image_bytes': None\n",
        "    }\n",
        "\n",
        "    # Get image from search_index.images using the ID\n",
        "    item_id = meta.get('id', i)\n",
        "    if item_id in search_index.images:\n",
        "        img = search_index.images[item_id]\n",
        "        if img:\n",
        "            product['image_bytes'] = image_to_bytes(img)\n",
        "\n",
        "    save_data['products'].append(product)\n",
        "\n",
        "with open('fashion_data.pkl', 'wb') as f:\n",
        "    pickle.dump(save_data, f)\n",
        "\n",
        "# Verify\n",
        "images_saved = sum(1 for p in save_data['products'] if p['image_bytes'])\n",
        "print(f\" Saved {len(save_data['products'])} products, {images_saved} with images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "s3o2_aAfyQ2l",
        "outputId": "6dded08b-2461-4c25-aec6-b494877b4bd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Initializing Training ...\n",
            "üñ•Ô∏è Using device: cuda\n",
            "üèãÔ∏è Starting Real Fine-Tuning (Head Training)...\n",
            "   üì¶ Training on 19 classes: ['cape', 'cardigan', 'coat', 'dress', 'glasses']...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 94/94 [00:29<00:00,  3.17it/s, loss=0.809, acc=0.676]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Epoch 1 Summary: Loss=1.2155, Accuracy=0.6757\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 94/94 [00:22<00:00,  4.14it/s, loss=0.122, acc=0.964]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Epoch 2 Summary: Loss=0.2466, Accuracy=0.9643\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 94/94 [00:22<00:00,  4.13it/s, loss=0.102, acc=0.987]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Epoch 3 Summary: Loss=0.0913, Accuracy=0.9867\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAGJCAYAAAC90mOkAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAg21JREFUeJzt3Xl4TGf7B/DvmUkyk33fBbEHkRBL7bSIpSlapXZa2qq16i36q6Leomp90WrVUlWUatHaae1rEULEmtiySmTf55zfH5FhZI9MziT5fq5rLjlnnjnnnjsnMXee5zyPIEmSBCIiIiIiIiqQQu4AiIiIiIiIDB0LJyIiIiIioiKwcCIiIiIiIioCCyciIiIiIqIisHAiIiIiIiIqAgsnIiIiIiKiIrBwIiIiIiIiKgILJyIiIiIioiKwcCIiIiIiIioCCyciIgMRFhYGQRCwfv16uUPRuxEjRqBmzZpyh6F3UVFR6NevH+zt7SEIApYuXSp3SEREVEosnIiIysn69eshCEK+j2nTpskSU0HxvPg4cuSILPHpQ6dOnXTem52dHVq0aIG1a9dCFMUyPdfHH3+M/fv3Y/r06fj555/RvXv3Mj0+ERGVHyO5AyAiqmq+/PJLeHp66uxr3LgxatSogbS0NBgbG5dbLD///LPO9oYNG3Dw4ME8+728vMr0vKtXry7zIqUkqlWrhnnz5gEAYmJisGHDBrz33nu4efMm5s+fX2bn+fvvv9G7d29MmTKlzI5JRETyECRJkuQOgoioKli/fj1GjhyJ8+fPo3nz5nKHk69x48Zh5cqVqMz/NXTq1AmPHz/G1atXtftSU1NRv359PHnyBE+ePHmp4jU7OxuiKMLExAQKhQIfffQRVqxYURahIz09XXtcIiIqX/zNS0RkIPK7x2nEiBGwsLDAo0eP0KdPH1hYWMDR0RFTpkyBRqPReb0oili6dCkaNWoEtVoNZ2dnfPDBB3jy5MlLxVWzZk2MGDEiz/5OnTqhU6dO2u0jR45AEARs3boVX331FapVqwa1Wo3XXnsNt2/f1nnti/c45b73hQsX4ocffkDt2rWhUqnQokULnD9/Ps+5t23bhoYNG0KtVqNx48b4448/Xuq+KTMzM7zyyitISUlBTEwMACA+Ph6TJk2Ch4cHVCoV6tSpg6+//lqnp+z5uJcuXaqN+9tvv4UgCJAkCStXrtQOC8x19+5dvP3227Czs9Oee/fu3Tox5eZzy5Yt+Pzzz+Hu7g4zMzMkJiZqr4v79+/j9ddfh4WFBdzd3bFy5UoAQFBQEF599VWYm5ujRo0a2LRpk86x4+LiMGXKFHh7e8PCwgJWVlbo0aMHLl++nG8MxfmeAsDZs2fRs2dP2NrawtzcHE2aNMGyZct02oSEhKBfv36ws7ODWq1G8+bNsWvXrlJ814iIyheH6hERlbOEhAQ8fvxYZ5+Dg0OB7TUaDfz9/dGqVSssXLgQhw4dwqJFi1C7dm2MGTNG2+6DDz7Q9mpNmDABoaGhWLFiBS5duoSTJ0+W2xDA+fPnQ6FQYMqUKUhISMCCBQswePBgnD17tsjXbtq0CUlJSfjggw8gCAIWLFiAN998E3fv3tXGv3v3bgwYMADe3t6YN28enjx5gvfeew/u7u4vFffdu3ehVCphY2OD1NRUdOzYEY8ePcIHH3yA6tWr49SpU5g+fToiIiLyTPKwbt06pKen4/3334dKpUKzZs3w888/Y+jQoejatSuGDRumbRsVFYU2bdogNTUVEyZMgL29PX766Se88cYb+O2339C3b1+dY8+ZMwcmJiaYMmUKMjIyYGJiAiDnuujRowc6dOiABQsW4JdffsG4ceNgbm6O//u//8PgwYPx5ptvYtWqVRg2bBhat26tHSJ69+5d7NixA2+//TY8PT0RFRWF77//Hh07dkRwcDDc3Nx0YijO9/TgwYN4/fXX4erqiokTJ8LFxQXXr1/HX3/9hYkTJwIArl27hrZt28Ld3R3Tpk2Dubk5tm7dij59+mD79u153jsRkUGRiIioXKxbt04CkO9DkiQpNDRUAiCtW7dO+5rhw4dLAKQvv/xS51hNmzaV/Pz8tNvHjx+XAEi//PKLTrt9+/blu78gY8eOlV78r6FGjRrS8OHD87Tt2LGj1LFjR+32P//8IwGQvLy8pIyMDO3+ZcuWSQCkoKAgnfdVo0YN7Xbue7e3t5fi4uK0+3fu3CkBkP7880/tPm9vb6latWpSUlKSdt+RI0ckADrHLEjHjh2lBg0aSDExMVJMTIx0/fp1acKECRIAKSAgQJIkSZozZ45kbm4u3bx5U+e106ZNk5RKpXT//n2duK2srKTo6Og85wIgjR07VmffpEmTJADS8ePHtfuSkpIkT09PqWbNmpJGo9HJZ61ataTU1FSdY+ReF3PnztXue/LkiWRqaioJgiBt2bJFuz8kJEQCIM2cOVO7Lz09XXueXKGhoZJKpdK51or7Pc3OzpY8PT2lGjVqSE+ePNE5riiK2q9fe+01ydvbW0pPT9d5vk2bNlLdunXz5I+IyJBwqB4RUTlbuXIlDh48qPMoyocffqiz3b59e9y9e1e7vW3bNlhbW6Nr1654/Pix9uHn5wcLCwv8888/Zf4+CjJy5Ehtr0hurAB04i3IgAEDYGtrW+Brw8PDERQUhGHDhsHCwkLbrmPHjvD29i52jCEhIXB0dISjoyO8vLywfPly9OrVC2vXrgWQk8/27dvD1tZWJ59dunSBRqPBsWPHdI731ltvwdHRsVjn3rNnD1q2bIl27dpp91lYWOD9999HWFgYgoODddoPHz4cpqam+R5r1KhR2q9tbGxQv359mJubo3///tr99evXh42NjU7+VSqV9j4pjUaD2NhYWFhYoH79+rh48WKe8xT1Pb106RJCQ0MxadIk2NjY6Lw2d4hiXFwc/v77b/Tv3x9JSUnanMbGxsLf3x+3bt3Co0ePCk4cEZHMOFSPiKictWzZskSTQ6jV6jwfym1tbXXuXbp16xYSEhLg5OSU7zGio6MB5AwTTEtL0+43MTGBnZ1dScIvUvXq1fPECqBY91oV9dp79+4BAOrUqZPntXXq1Mn3Q39+atasidWrV0MQBKjVatStW1cnd7du3cKVK1cKLIZy85nrxVkSC3Pv3j20atUqz/7cmQvv3buHxo0bF3ns/K4La2trVKtWTed+qtz9z+dfFEUsW7YM3377LUJDQ3Xul7O3t89zrqK+L3fu3AEAnbhfdPv2bUiShBkzZmDGjBn5tomOjn7pIZdERPrCwomIyMAplcoi24iiCCcnJ/zyyy/5Pp/7AXvixIn46aeftPs7duxY5BpNL34Iz6XRaPKNraB4pWLM1Pcyry0Jc3NzdOnSpcDnRVFE165d8emnn+b7fL169XS2C+oRKgsFHbugXBUnh3PnzsWMGTPw7rvvYs6cObCzs4NCocCkSZPynSa+LL4vucedMmUK/P39822TX0FMRGQoWDgREVUCtWvXxqFDh9C2bdtCP8R/+umnGDJkiHb7+WFxBbG1tUV8fHye/ffu3UOtWrVKFW9p1ahRAwDyndEtv32lVbt2bSQnJxdaXJVWjRo1cOPGjTz7Q0JCtM/r22+//YbOnTtjzZo1Ovvj4+MLnaikILVr1wYAXL16tcCc5V4rxsbGeskrEZG+8R4nIqJKoH///tBoNJgzZ06e57Kzs7WFT8OGDdGlSxftw8/Pr8hj165dG2fOnEFmZqZ2319//YUHDx6UWfzF5ebmhsaNG2PDhg1ITk7W7j969CiCgoLK7Dz9+/fH6dOnsX///jzPxcfHIzs7u9TH7tmzJ86dO4fTp09r96WkpOCHH35AzZo10bBhw1Ifu7iUSmWe3qJt27aV+h6jZs2awdPTE0uXLs1TZOeex8nJCZ06dcL333+PiIiIPMfInQaeiMhQsceJiKgS6NixIz744APMmzcPgYGB6NatG4yNjXHr1i1s27YNy5YtQ79+/Up17FGjRuG3335D9+7d0b9/f9y5cwcbN27U9jKUt7lz56J3795o27YtRo4ciSdPnmDFihVo3LixTjH1Mv7zn/9g165deP311zFixAj4+fkhJSUFQUFB+O233xAWFlaqnhkAmDZtGjZv3owePXpgwoQJsLOzw08//YTQ0FBs3769XBa3ff311/Hll19i5MiRaNOmDYKCgvDLL7+UugdRoVDgu+++Q0BAAHx9fTFy5Ei4uroiJCQE165d0xagK1euRLt27eDt7Y3Ro0ejVq1aiIqKwunTp/Hw4cM860gRERkSFk5ERJXEqlWr4Ofnh++//x6fffYZjIyMULNmTQwZMgRt27Yt9XH9/f2xaNEiLF68GJMmTULz5s3x119/4ZNPPinD6IsvICAAmzdvxqxZszBt2jTUrVsX69evx08//YRr166VyTnMzMxw9OhRzJ07F9u2bcOGDRtgZWWFevXqYfbs2bC2ti71sZ2dnXHq1ClMnToVy5cvR3p6Opo0aYI///wTvXr1KpP4i/LZZ58hJSUFmzZtwq+//opmzZph9+7dmDZtWqmP6e/vj3/++QezZ8/GokWLIIoiateujdGjR2vbNGzYEP/++y9mz56N9evXIzY2Fk5OTmjatCm++OKLsnhrRER6I0hlfcctERGRDHx9feHo6Fis6d2JiIhKivc4ERFRhZKVlZXnHqMjR47g8uXL6NSpkzxBERFRpcceJyIiqlDCwsLQpUsXDBkyBG5ubggJCcGqVatgbW2Nq1ev5rsOERER0cviPU5ERFSh2Nraws/PDz/++CNiYmJgbm6OXr16Yf78+SyaiIhIb9jjREREREREVATe40RERERERFQEFk5ERERERERFqHL3OImiiPDwcFhaWkIQBLnDISIiIiIimUiShKSkJLi5uRW5AHmVK5zCw8Ph4eEhdxhERERERGQgHjx4gGrVqhXapsoVTpaWlgBykmNlZSVzNDk9YDExMXB0dCyyyqWSY371i/nVL+ZXv5hf/WJ+9Yv51S/mV78MKb+JiYnw8PDQ1giFqXKFU+7wPCsrK4MpnNLT02FlZSX7hVMZMb/6xfzqF/OrX8yvfjG/+sX86hfzq1+GmN/i3MJjGJESEREREREZMBZORERERERERWDhREREREREVIQqd48TEREREb08SZKQnZ0NjUYjdyhlThRFZGVlIT093WDuwalMyju/xsbGUCqVL30cFk5EREREVCKZmZmIiIhAamqq3KHohSRJEEURSUlJXPdTD8o7v4IgoFq1arCwsHip47BwIiIiIqJiE0URoaGhUCqVcHNzg4mJSaUrLnJ704yMjCrdezME5ZlfSZIQExODhw8fom7dui/V88TCSUYaUcLZu7G4/TAOdZKVaFXLAUoFfziJiIjIcGVmZkIURXh4eMDMzEzucPSChZN+lXd+HR0dERYWhqysLBZOFdG+qxGY/WcwIhLSn+4Jhau1GjMDGqJ7Y1dZYyMiIiIqCu/9oYqirIozXvEy2Hc1AmM2XnyuaMoRmZCOMRsvYt/VCJkiIyIiIiKi/LBwKmcaUcLsP4Mh5fNc7r7ZfwZDI+bXgoiIiIiI5MDCqZydC43L09P0PAlAREI6zoXGlV9QRERERDLQiBJO34nFzsBHOH0ntkL+4bhmzZpYunRpsdsfOXIEgiAgPj5ebzGRfshaOB07dgwBAQFwc3ODIAjYsWNHoe1///13dO3aFY6OjrCyskLr1q2xf//+8gm2jEQnFVw0laYdERERUUW072oE2n39NwauPoOJWwIxcPUZtPv6b73dsiAIQqGPWbNmleq458+fx/vvv1/s9m3atEFERASsra1Ldb7iYoFW9mQtnFJSUuDj44OVK1cWq/2xY8fQtWtX7NmzBxcuXEDnzp0REBCAS5cu6TnSsuNkqS7TdkREREQVjRz3e0dERGgfS5cuhZWVlc6+KVOmaNvmzvpWHI6OjiWaXdDExAQuLi6cra8CkrVw6tGjB/773/+ib9++xWq/dOlSfPrpp2jRogXq1q2LuXPnom7duvjzzz/1HGnZaelpB1drNYr6UbkangCxAnZXExERUdUjSRJSM7OL9UhKz8LMXdcKvd971q5gJKVnFet4klS8z0suLi7ah7W1NQRB0G6HhITA0tISe/fuhZ+fH9RqNU6ePIk7d+6gd+/ecHZ2hoWFBVq0aIFDhw7pHPfFoXqCIODHH39E3759YWZmhrp162LXrl3a51/sCVq/fj1sbGywf/9+eHl5wcLCAt27d0dExLPiMTs7GxMmTICNjQ3s7e0xdepUDB8+HH369CnWe8/PkydPMGzYMNja2sLMzAw9evTArVu3tM/fu3cPAQEBsLW1hbm5ORo1aoQ9e/ZoXzt48GA4OjrC1NQUdevWxbp160odS0VRoacjz11x2M7OrsA2GRkZyMjI0G4nJiZqXyuKot5jfJEAYEYvL4zddAkCkO8vDQD4avd1HA6OwoJ+3qhmWznXSCgPoihqV6emssf86hfzq1/Mr34xv/olZ35zz537AIDUzGw0mnmgTI4vAYhMTIf3rOId79rsbjAzKdlH2ty4X/x32rRp+Oabb1CrVi1YWFggMjJS+4d+lUqFDRs2ICAgACEhIahevbrO8Z4v4GbPno2vv/4aCxYswPLlyzF48GCEhYXBzs5O55y5j9TUVCxcuBAbNmyAQqHA0KFDMWXKFGzcuBEAMH/+fPzyyy9Yu3YtvLy8sGzZMuzYsQOdO3cusHB88TwvGjFiBG7duoWdO3fCysoK06ZNQ8+ePXHt2jUYGxtj7NixyMzMxNGjR2Fubo7g4GCYm5tDkiR8/vnnCA4Oxp49e+Dg4IDbt28jLS2t2EVsfrnXp9wc5Pf5vyQ/QxW6cFq4cCGSk5PRv3//AtvMmzcPs2fPzrM/JiYG6eny3EfUzEmBua/XwpIjDxCdnKXd72xhjEkdq+FJmgb/O/YQZ0Lj0H3ZcUzu6IFeDe3ZpVsKoigiISEBkiRxvQk9YH71i/nVL+ZXv5hf/ZIzv1lZWRBFEdnZ2drhbMUd1qYP2dnZyC5hCnI/LOfGrdFoAABffPGFthjRaDSws7NDo0aNtK+bOXMm/vjjD+zYsQMfffSRzvGez8HQoUPx9ttvAwC+/PJLLF++HKdPn4a/v7/2XLn5E0URWVlZWL58OWrXrg0AGDNmDL766ivtMVesWIFPP/0UAQEBAHJGYe3duzfPeZ/34nmed+vWLezatQtHjx5F69atAeT0fNWqVQvbt29Hv379cO/ePfTt2xdeXl4AoC0Us7Ozce/ePfj4+MDX1xcAUK1aNZ18FiU3v0DZrbFUmNw8x8bGwtjYWOe5pKSkYh+nwhZOmzZtwuzZs7Fz5044OTkV2G769OmYPHmydjsxMREeHh7aCSbkMsDJCf1eqYezd2NxJzwGtd0c0aqWPZSKnIunR9OamPJbEC7ce4L/HryH0w/SMLdvYzhaqmSLuSISRRGCIMDR0ZH/cesB86tfzK9+Mb/6xfzql5z5TU9PR1JSEoyMjGBklPNR0lKpxLXZ3Yr1+nOhcRi5/t8i260b0RwtPQseVZTL1FhZ4g/fuTnLjV+pVAIAWrVqpd0H5IxcmjVrFvbs2YOIiAhkZ2cjLS0NDx8+1GmnUCh0tn19fbXb1tbWsLKyQmxsLIyMjLTnys2fQqGAmZkZ6tevr329u7s7oqOjYWRkhISEBERFReGVV17RHtPIyAh+fn4QRVHnvM978TzPu3XrFoyMjNCmTRttO2dnZ9SvXx83b96EkZERJkyYgI8++giHDx/Ga6+9hrfeegtNmjQBAHz00Ufo168fAgMD0bVrV/Tp0wdt2rQp0fcAQJ4iRl9y82xvbw+1WncegRe3Cz1OWQdWHrZs2YJRo0Zh27Zt6NKlS6FtVSoVVKq8xYZCoZD9F7lCAbSp44A6ViKcnBx04vF0tMTWD1pj9fG7WHzgJg6HRKP7suP4qq83enq7yhh1xSMIgkF8vysr5le/mF/9Yn71i/nVL7nyq1AodGajy43FXFW8ODrUc4KrtRqRCen53rIgAHCxVqNDPSftH5TL2vNxP/+vhYUFBEGAJEkQBAFTpkzBoUOHsHDhQtSpUwempqbo168fsrKydIq153MB5EwA8eLzucd8/py5D2NjY532CoWiwPYFvZfC3uOLbYp6ThAEjB49Gt27d8fu3btx4MABzJ8/H4sWLcL48ePRs2dP3Lt3D3v27MHBgwfRpUsXjB07FgsXLiww58/LfW+FxV+Wct9Tfj8vJfn5qXC/yTZv3oyRI0di8+bN6NWrl9zh6JVSIeDDjrWxa3xbeLla4UlqFj765SImbbmEhNSsog9AREREZGCUCgEzAxoCQJ7JsnK3ZwY01FvRVBKnTp3CiBEj0LdvX3h7e8PFxQVhYWHlGoO1tTWcnZ1x/vx57T6NRoOLFy+W+pheXl7Izs7G2bNntftiY2Nx48YNNGzYULvPw8MDH374IX7//Xd88sknWL16tfY5R0dHDB8+HBs3bsTSpUvxww8/lDqeikLWHqfk5GTcvn1bux0aGorAwEDY2dmhevXqmD59Oh49eoQNGzYAyBmeN3z4cCxbtgytWrVCZGQkAMDU1FTvc+HLqYGLFXaObYv/Hb6Fb4/cxo7AcJy5G4cF/ZqgQz1HucMjIiIiKpHujV3x3ZBmmP1nsM6U5C7WaswMaIjujQ1jdE3dunXx+++/IyAgAIIgYMaMGbJMyDF+/HjMmzcPderUQYMGDbB8+XI8efKkWL01QUFBsLS01G4LggAfHx/07t0bo0ePxvfffw9LS0tMmzYN7u7u6N27NwBg0qRJ6NGjB+rVq4cnT57gn3/+0d7v9MUXX8DPzw+NGjVCRkYG/vrrL+1zlZmshdO///6Lzp07a7dz70UaPnw41q9fj4iICNy/f1/7/A8//IDs7GyMHTsWY8eO1e7PbV+ZmRgpMMW/Pl71csInWy8j9HEKhq09hyGvVMdnPb1KPJsMERERkZy6N3ZF14YuOBcah+ikdDhZqtHS084geppyLVq0CO+99x7atGkDBwcHTJ06VTtDc3maOnUqIiMjMWzYMCiVSrz//vvw9/fX3p9UmA4dOuhsK5VKZGdnY926dZg4cSJef/11ZGZmokOHDtizZ4/2viONRoOxY8fi4cOHsLKyQvfu3bFkyRIAOUMRp0+fjrCwMJiamqJ9+/bYsmVL2b9xAyNI5TEHoAFJTEyEtbU1EhISZJ0cIpcoioiOjoaTk1Oxx1imZWrw9b4QrD8VBgCoYW+Gxf194Fej6Bsoq5rS5JeKj/nVL+ZXv5hf/WJ+9UvO/KanpyM0NBSenp4lurG+IsldANfIyMggZzUWRRFeXl7o378/5syZI3c4JVbe+S3smi1JbcDfZBWQqYkSs95ohF9GtYKbtRr3YlPx9qrTmL83BBnZGrnDIyIiIqIydO/ePaxevRo3b95EUFAQxowZg9DQUAwaNEju0KoUFk4VWNs6Dtj3cQe81awaRAlYdfQOeq84ieDw8u9CJiIiIiL9UCgUWL9+PVq0aIG2bdsiKCgIhw4dqhL3FRkS3hhTwVmpjbGovw+6NXLGZ78HISQyCb1XnsCkLvXwQYdaMFKyNiYiIiKqyDw8PHDy5Em5w6jy+Km6kvBv5IL9H3eAfyNnZGkkfLP/Bt7+/jRCH6fIHRoRERERUYXHwqkScbBQYdUQPyx62weWKiNcuh+PHsuO4adTYRDFKjUHCBERERFRmWLhVMkIgoC3/Kph/8cd0LaOPdKzRMzcdQ3D1p5DeHya3OEREREREVVILJwqKTcbU/z8bivMfqMR1MYKnLj9GP5Lj+H3iw9RxWagJyIiIiJ6aSycKjGFQsDwNjWxZ0J7NK1ug6T0bEzeehkfbryA2OQMucMjIiIiIqowWDhVAbUcLbDtg9b4j399GCsF7L8WhW5LjmH/tUi5QyMiIiIiqhBYOFURRkoFxnaugx1j26K+syViUzLxwc8X8MnWy0hMz5I7PCIiIqpK4h8A4YEFP+IfyBhcydSsWRNLly4tdvsjR45AEATEx8frLSbSD67jVMU0crPGrvFtseTgLfxw7A62X3yI03ceY+HbPmhTx0Hu8IiIiKiyi38ArPADsgu5bcBIBYy7ANh4lNlpBUEo9PmZM2di1qxZJT7u+fPnYW5uXuz2bdq0QUREBKytrUt8rtJq0KABQkNDce/ePbi4uJTbeSsb9jhVQSojJab1aICtH7RGDXszhCekY9CPZzFr1zWkZWrkDo+IiIgqs9TYwosmIOf51NgyPW1ERIT2sXTpUlhZWensmzJliratJEnIzs4u1nEdHR1hZmZW7DhMTEzg4uJSZCFXVk6cOIG0tDT069cPP/30U7mcszBZWRV3pBMLpyqseU077JnQHkNeqQ4AWH8qDL3+dxyX7j+ROTIiIiKqUCQJyEwp3iO7mMujZKcV73jFnC3YxcVF+7C2toYgCNrtkJAQWFpaYu/evfDz84NarcbJkydx584d9O7dG87OzrCwsECLFi1w6NAhneO+OFRPEAT8+OOP6Nu3L8zMzFC3bl3s2rVL+/yLQ/XWr18PGxsb7N+/H15eXrCwsED37t0RERHxLBXZ2ZgwYQJsbGxgb2+PqVOnYvjw4ejTp0+R73vNmjUYNGgQhg4dirVr1+Z5/uHDhxg4cCDs7Oxgbm6O5s2b4+zZs9rn//zzT7Ro0QJqtRoODg7o27evznvdsWOHzvFsbGywfv16AEBYWBgEQcCvv/6Kjh07Qq1W45dffkFsbCyGDBmCatWqwczMDN7e3ti8ebPOcURRxIIFC1CnTh2oVCpUr14dX331FQDg1Vdfxbhx43Tax8TEwMTEBIcPHy4yJ6XFoXpVnLnKCP/t442uDV3w6W+XcfdxCt767hQ+6lQHE16rCxMj1tZERERUhKxUYK5b2R5zbffitfssHDAp/lC5wkybNg0LFy6Ep6cnLC0tERERgZ49e+Krr76CSqXChg0bEBAQgBs3bqB69eoFHmf27NlYsGABvvnmGyxfvhyDBw/GvXv3YGdnl2/71NRULFy4ED///DMUCgWGDBmCKVOm4JdffgEAfP311/jll1+wbt06eHl5YdmyZdixYwc6d+5c6PtJSkrCtm3bcPbsWTRo0AAJCQk4fvw42rdvDwBITk5Gx44d4e7ujl27dsHFxQUXL16EKIoAgN27d6Nv3774v//7P2zYsAGZmZnYs2dPqfK6aNEiNG3aFGq1Gunp6WjWrBmmTZsGa2tr7N69G0OHDkXt2rXRsmVLAMD06dOxevVqLFmyBO3atUNERARCQkIAAKNGjcK4ceOwaNEiqFQqAMDGjRvh7u6OV199tcTxFRcLJwIAdKzniAOTOmLmrqvYERiOFf/cxt8h0VgywBf1XSzlDo+IiIhI77788kt07dpVO1TPyckJvr6+2ufnzJmDP/74A7t27crT4/G8ESNGYODAgQCAuXPn4n//+x/OnTuH7t3zLwazsrKwatUq1K5dGwAwbtw4fPnll9rnly9fjunTp2t7e1asWFGsAmbLli2oW7cuGjVqBAB45513sGbNGm3htGnTJsTExOD8+fPaoq5OnTra13/11Vd45513MHv2bO0+Hx+fIs/7okmTJuHNN9/UbkuShMmTJ8PIyAiCIGD8+PHYv38/tm7dipYtWyIpKQnLli3DihUrMHz4cABA7dq10a5dOwDAm2++iXHjxmHnzp3o378/gJyeuxEjRuh1CCQLJ9KyNjPG0neaolsjF/zfH0EIjkhEwPITmNytHka3rwWlonzG4hIREVEFY2yW0/NTHJFXiteb9O4+wKVJ8c5dRpo3b66znZycjNmzZ2P37t2IiIhAdnY20tLScP/+/UKP06TJs7jNzc1hZWWF6OjoAtubmZlpiyYAcHV11bZPSEhAVFSUticGAJRKJfz8/LQ9QwVZu3YthgwZot0eMmQIOnbsiOXLl8PS0hKBgYFo2rRpgT1hgYGBGD16dKHnKI4X86rRaPDVV19h+/btePToETIzM5GRkaG9V+z69evIyMjAa6+9lu/x1Gq1duhh//79cfHiRVy9elVnSKQ+sHCiPHp6u6J5TVtM3x6EwyHRmL83BIeCo7Covw9q2JdNVzgRERFVIoJQ/OFyRqbFb1dGQ/CK68XZ8aZMmYJDhw5h4cKFqFOnDkxNTdGvXz9kZmYWehxjY2OdbUEQCi1y8msvFfPerYIEBwfjzJkzOHfuHKZOnardr9FosGXLFowePRqmpoV/L4p6Pr8485v84cW8fvPNN1ixYgWWLFmCJk2awNzcHJMmTdLmtajzAjnD9Xx9ffHw4UOsW7cOr776KmrUqFHk614Gb2ChfDlZqvHj8OZY8FYTWKiM8O+9J+ix7Dg2nrn30j/IRERERBXBqVOnMGLECPTt2xfe3t5wcXFBWFhYucZgbW0NZ2dnnD9/XrtPo9Hg4sWLhb5uzZo16NChAy5fvozAwEDtY/LkyVizZg2AnJ6xwMBAxMXF5XuMJk2aFDrZgqOjo84kFrdu3UJqamqR7+nUqVMICAjAkCFD4OPjg1q1auHmzZva5+vWrQtTU9NCz+3t7Y3mzZtj9erV2LRpE959990iz/uyWDhRgQRBQP8WHtg7sT1aedohNVODz3dcxYh15xGZkC53eERERFQRmdnnrNNUGCNVTjuZ1a1bF7///jsCAwNx+fJlDBo0qMjhcfowfvx4zJs3Dzt37sSNGzcwceJEPHnypMD7ebKysvDzzz9j4MCBaNy4sc5j1KhROHv2LK5du4aBAwfCxcUFffr0wcmTJ3H37l1s374dp0+fBpCzttXmzZsxc+ZMXL9+HUFBQfj666+153n11VexYsUKXLp0Cf/++y8+/PDDPL1n+alTpw4OHz6MU6dO4fr16/jggw8QFRWlfV6tVmPq1Kn49NNPsWHDBty5cwdnzpzRFny5Ro0ahfnz50OSJJ3Z/vSFhRMVycPODJtHv4IZrzeEykiBozdj0G3JUewMfMTeJyIiIioZG4+cxW3fP1rwo4wXvy2tRYsWwdbWFm3atEFAQAD8/f3RrFmzco9j6tSpGDhwIIYNG4bWrVvDwsIC/v7+UKvV+bbftWsXYmNj8y0mvLy84OXlhTVr1sDExAQHDhyAk5MTevbsCW9vb8yfPx9KpRIA0KlTJ2zbtg27du2Cr68vXn31VZw7d057rEWLFsHDwwPt27fHoEGDMGXKlGKtafX555/D19cX3bt3R6dOnbTF2/NmzJiBTz75BF988QW8vLwwYMCAPPeJDRw4EEZGRhg4cGCBuShLglTFPvkmJibC2toaCQkJsLKykjsciKKI6OhoODk5QaEw/Dr2dnQSJm+9jCsPEwAAvbxdMadPY9iZm8gcWf4qWn4rGuZXv5hf/WJ+9Yv51S8585ueno7Q0FB4enqWy4dVOeTOqpc765uhEUURXl5e6N+/P+bMmSN3OCVWVvkNCwtD7dq1cf78+UIL2sKu2ZLUBvxNRiVSx8kS28e0wcdd6sFIIWB3UAS6LTmGw9ejin4xEREREZXYvXv3sHr1aty8eRNBQUEYM2YMQkNDMWjQILlDk0VWVhYiIyPx+eef45VXXim3XkAWTlRixkoFJnapiz8+aou6ThZ4nJyB9376F1N/u4Kk9LwzqRARERFR6SkUCqxfvx4tWrRA27ZtERQUhEOHDsHLy0vu0GRx8uRJuLq64vz581i1alW5nZfTkVOpeVezxp/j22HRgRv48UQofv33AU7cfoxF/X3wSi35b+gkIiIiqgw8PDxw8uRJucMwGJ06dZLlPnv2ONFLURsr8X+9GmLL6FdQzdYUj+LTMHD1Gcz5KxjpWRq5wyMiIiIiKhMsnKhMtKplj32TOuCdFh6QJGDNiVC8vvwErjyMlzs0IiIi0oMqNr8YVWBlda2ycKIyY6Eywvy3mmDtiOZwtFThdnQy+n57CksO3kSWpvzXPCAiIqKyl7tOT3EWOiUyBJmZmQCgnWa9tHiPE5W5Vxs448AkW8zYeRV/XYnAssO38HdINBb390FdZ0u5wyMiIqKXoFQqYWNjo11Tx8zMzCCn7H4Zhj4deUVXnvkVRRExMTEwMzODkdHLlT4snEgvbM1NsGJQM3RrFI4ZO64i6FECei0/gU/96+Pdtp5QKPhLiIiIqKJycXEBgDwLklYWkiRBFEUoFAoWTnpQ3vlVKBSoXr36S5+LhRPp1Rs+bmjlaYdPf7uCozdj8N/d13EgOAqL3vaBh13RK0sTERGR4REEAa6urnByckJWVuVbikQURcTGxsLe3p4LOOtBeefXxMSkTM7Dwon0ztlKjfUjW2DzuQf47+5gnAuNQ/elx/BFQEP0b+7Bv+QQERFVUEql8qXvGzFEoijC2NgYarWahZMeVNT8VpxIqUITBAGDWlXHvokd0KKmLVIyNZi6PQjv/fQvohPT5Q6PiIiIiKhQLJyoXFW3N8OW91vjs54NYKJU4O+QaHRbegx/XQmXOzQiIiIiogKxcKJyp1QIeL9Dbfw5vh0auVkhPjUL4zZdwoTNlxCfmil3eEREREREebBwItnUd7HEHx+1xYRX60CpELDrcjj8lx7DkRuVc4YeIiIiIqq4WDiRrEyMFJjcrT62j2mDWo7miErMwIh15/HZH0FIyciWOzwiIiIiIgAsnMhA+HrYYPf49hjZtiYAYNPZ++ix7DjOh8XJGxgREREREVg4kQExNVFiZkAjbBrVCu42prgfl4r+35/GvD3XkZ6lkTs8IiIiIqrCWDiRwWlTxwF7J7XH237VIEnA98fu4o0VJ3D1UYLcoRERERFRFSVr4XTs2DEEBATAzc0NgiBgx44dRb7myJEjaNasGVQqFerUqYP169frPU4qf1ZqY3zztg9WD2sOBwsT3IxKRp+VJ7H88C1ka0S5wyMiIiKiKkbWwiklJQU+Pj5YuXJlsdqHhoaiV69e6Ny5MwIDAzFp0iSMGjUK+/fv13OkJJeuDZ2xf1IHdG/kgmxRwqKDN/HWqtO4E5Msd2hEREREVIUYyXnyHj16oEePHsVuv2rVKnh6emLRokUAAC8vL5w4cQJLliyBv79/vq/JyMhARkaGdjsxMREAIIoiRFH+ngtRFCFJkkHEYqhszYyxcpAvdgaGY+afwbj8IB69/nccn/rXx7BXakChEAp8LfOrX8yvfjG/+sX86hfzq1/Mr34xv3qQ8ABIzZn0SxRFKJ88gZhlCyie9uOY2QHWHuUeVkm+x7IWTiV1+vRpdOnSRWefv78/Jk2aVOBr5s2bh9mzZ+fZHxMTg/T09LIOscREUURCQgIkSYJCwVvOCtPG3RgbB3vhq4NhOHc/CV/+dR17Lj/E511rwsXKJN/XML/6xfzqF/OrX8yvfjG/+sX86hfzW7YUSeFw3OIPQZOZsw3A8YU2ktIEMe/sh2jpVq6xJSUlFbtthSqcIiMj4ezsrLPP2dkZiYmJSEtLg6mpaZ7XTJ8+HZMnT9ZuJyYmwsPDA46OjrCystJ7zEURRRGCIMDR0ZE/mMXg5ARsruWOjWfvY/7eG/j3QRKG/HIdMwO88GZTdwiCbu8T86tfzK9+Mb/6xfzqF/OrX8yvfjG/ZUwToS2aCiJoMuFgJuR82CtHarW62G0rVOFUGiqVCiqVKs9+hUJhMD8IgiAYVDwVwfA2nuhQzwmfbA3Exfvx+M9vQTgQHI15b3rDwUL3+8386hfzq1/Mr34xv/rF/OoX86tfzO8LRBEQs4t4aPLfjgkp1ikUgvBs6F45Kcn3t0IVTi4uLoiKitLZFxUVBSsrq3x7m6hy83Qwx7YP2+D7Y3ew5OBNHAyOwoV7TzC3b2N0b+wqd3hERERUmUhSwYVBcYqHPNtZZXCMQvZpXvaYL2xDkvs7ILsKVTi1bt0ae/bs0dl38OBBtG7dWqaISG5KhYCPOtVBp3pOmLw1ECGRSfhw40W82dQdM99oBEuVUu4QiciQxD8AUmNzvpYkGMXFAZoIIHeYr5k9YFP+NycTVRiSVPwP+ZqXKQyK26aMi42n24KYDafsTAjSc70skkbu7BsmhdHThzGgUD63bfRsW8wG4u/JHelLk7VwSk5Oxu3bt7XboaGhCAwMhJ2dHapXr47p06fj0aNH2LBhAwDgww8/xIoVK/Dpp5/i3Xffxd9//42tW7di9+7dcr0FMhAN3aywc1xbLDt0C6uO3sHvlx7h9N1YzH/TG/Wt5Y6OiAxC/ANghR+QnTPTqgKAw4ttjFTAuAssnqhw2iFLei4MCukxEDRZsExJgqA2eeF1JSkeskpebEhVY5Y54emjeI0V+RcLpdoupPgocLsE51Aav8Qx8tknKJ794akw4YHADx1L/w0xELIWTv/++y86d+6s3c6dxGH48OFYv349IiIicP/+fe3znp6e2L17Nz7++GMsW7YM1apVw48//ljgVORUtaiMlPi0ewO85uWMT7YGIiw2FcPXnUc/H0fM6msHC3X+M+8RURWRGqstmgqUnZHTjoVT4YozZEmTBeWTKACPcz5sl6rH4CV7EQrs8aj4Q5YEAOZyB/E8oRTFgvZD/MsWGwV9yC99YSAKCsQ+SYC9ozMUL8b5/LagLPd7ckg+shZOnTp1giQV/Mtn/fr1+b7m0qVLeoyKKjq/GrbYM7E95u8NwYbT9/Db5Ricf3gSi/v7wq+GrdzhEVFlUZwhS0XeY/CyPRNlVRgU0KagwqMYQ5bym2640sv3Q3lBf+F/uV4FSVAiJT0D5pbWEArsRXiJnokCez8KeE1xeh0qElGERooGbJ1YGJGWrIUTkb6YmRjhy96N8VoDJ0zZFoh7sal4e9UpfNixNiZ1qQcTI/4SJKp6itlL8OvgnA+DHLJUcs8NWZIUSkhQQjAyhlAmvQjFGcJUyoJEWfqeiWc9D8UcslRGJFFEcnQ0zJycIPCDPRk6M/ucodCF9fobqXLaGTAWTlSpta/rgE1DGuLbszH441I4vj1yB//ciMHi/j7wcpV/HS8i0qPkaCD8EvDoYs6/D84V73UJD1/uvKUZsvRSH+JL8iFfXz0TuY9nH+AlUUR0dDSc+MGeiGw8cu4ffTo5jyhJiIuLg52dXc4U5ECFmJyHhRNVepZqIyx62wf+jVzw2R9XcT0iEW+sOIGPu9bDBx1qQ6moZMMLiKqi1Lic4uj5R+Kj0h0rYDng1KD0BUhlG7JERFQWbDyeFUaiiGxldM5itxXoDyssnKjK6N7YFX417PDZH0E4GByFBftu4PD1aCx62wc1HQzqFlsiKkx6AhBx+VmB9OhiAdPcCoBDPcC9GeDWFDCxAHZ+VPTxXZsAbr5lHTUREVVwLJyoSnG0VOGHoX747cJDzP4zGBfuPUGPZcfxWc8GGPJKDQj8SzGRYclMASKu6PYkxd7Kv61drZwCye1poeTaBFBZPns+PLBcQiYiosqJhRNVOYIg4O3mHmhd2x7/2XYFp+/GYsbOazgQHIUF/ZrA1dpU7hCJqqasdCDqGhB+8VmRFBOS/yQM1tVzeoXcmub0KLn6AKZFzJpZSW5OJiIiebBwoiqrmq0ZfhnVCj+dDsP8vSE4fusxui05hi97N0IfX3f2PhHpkyYLiA7WnbwhOvjpGjkvsHR92pP03MPcoeTnrCQ3JxMRkTxYOFGVplAIGNnWE+3rOuKTrYG4/DABH/96GQeuReG/fRrD3kIld4hEFZ+oAWJuPDfc7iIQeRXQ5NPzY2b/bKhd7sPKtexiqQQ3JxMRkTxYOBEBqONkge1j2uC7I3ew7PAt7L0aifNhcZj3ZhN0begsd3hEFYcoAnF3dO9JirgMZKXmbauyzhlu5/5coWTtwVnpiIjIILFwInrKSKnA+NfqonMDJ0zeGoibUckYveFfvO1XDV8ENISl2ljuEIkMiyTlzGb36KJukZSRmLetiUXOfUjP9yTZ1WKRREREFQYLJ6IXNHa3xq5x7bDk4E38cPwutl14iFN3YvHN203QpnYp7qsgqgwkCUgMfzbULrdQSnuSt62RGnBp8qxAcm8G2NfJWeOIiIiogmLhRJQPtbES03t64TUvZ3yyLRAP4tIwaPVZjGxbE1O7N4DamB8AqZJLjtZdJyn8EpASnbedwhhwaazbk+ToBSj53wsREVUu/J+NqBAtPe2wd2IHfLX7Ojafu491J8Nw9GYMFvf3ha+HjdzhEZWN1Djde5LCLwGJj/K2E5SAk5dukeTcKGcKbyIiokqOhRNRESxURpj3pje6NXLG1N+u4G5MCt767hTGdqqN8a/VhbGSs3FRBZKeCEQE6hZJT8LyaSgADvWeDbVzawo4NwZMzMo5YCIiIsPAwomomDrXd8KBjzvgi53XsOtyOP739238fSMai/v7op6zpdzhEeWVmQJEBulO3hB7K/+2drWe60lqBrg2AVS8romIiHKxcCIqARszE/xvYFN0a+SMz3dcxdVHiXh9+QlM6VYP77WrBaWCM4SRTLLSgahruhM3xIQAkpi3rXX1nGnAtYWSL2BqW94RExERVSgsnIhK4fUmbmhZ0w5Tt1/BPzdiMHdPCA4FR2Ph2z6obs+hTKRnmiwgOlh38oboYEDMztvWwkV3nSRXX8DCsdxDJiIiquhYOBGVkpOVGmtHtMCv5x9gzl/BOBcWh+7LjuHzXg0xsKUHBK5PQ2VB1OQURRGXnxVKkUGAJiNvWzP7Z0PtcgslK9fyj5mIiKgSYuFE9BIEQcA7LaujbR0HfLLtMs6FxuGzP4JwIDgSX7/VBM5WarlDpIpEFIG4u9rhdsKji3CKuAxFdlretirrZ8PtcnuUrD24oCwREZGesHAiKgMedmbYMvoVrD0ZigX7b+DIjRh0W3IMc/o0xhs+bnKHR4ZIkoD4e7rrJEVcBjIStU2Epw/J2ByCzj1JTQFbT0DBGR2JiIjKCwsnojKiUAgY1b4WOtZzxMdbA3H1USImbL6EA9ciMad3Y9iam8gdIslFkoDE8KdD7Z6bvCHtSd62RmrApQng1hSiqy9i1TVgX7clBCPj8o+biIiItFg4EZWxus6W+OOjtljx922s+Oc2/roSgbOhcVjwVhN0buAkd3hUHpKjdddJenQRSInO205hnLOA7POTNzg2AJRPiyRRhCY6GlAoyzd+IiIiyoOFE5EeGCsV+LhrPbzawAmTtwbiTkwKRq4/j4EtPfB/vRrCQsUfvUojNU63SAoPBBIf5m0nKAEnL93hds6NACNVuYdMREREJcdPb0R65ONhg90T2uOb/Tew9mQoNp97gBO3H2NhPx+0qmUvd3hUUumJQESgbqH0JCyfhgLgUE934gbnxoAJp6onIiKqqFg4EemZ2liJGa83RBcvZ0zZdhkP4tLwzuozGNXOE590qw+1MYdhGaTMlJxpv5+fvCH2Vv5t7Wrp9iS5+gAqy/KNl4iIiPSKhRNROWld2x77JrXHnL+CsfXfh1h9PBRHbsRgcX9feFezlju8qi0rHYi69nTihsCcf2NCAEnM29a6+rNpwN2a5nxtalvOARMREVF5Y+FEVI4s1cZY0M8H3Rq6YNrvQbgVnYy+357E+Ffr4qPOtWGs5PTSeqfJyllQ9vmJG6KDATE7b1sLF92JG1x9AQvHcg+ZiIiI5MfCiUgGXRo640ANW3y+Iwh7giKx5NBNHA6JwuL+vqjjZCF3eJWHqAFibujekxQZBGgy8rY1s39aID1XKFm5ln/MREREZJBYOBHJxM7cBCsHNcOuy+GYseMqrjxMQK//Hcen3RtgZJuaUCgEuUOsWEQRiLuru1ZSxGUgKzVvW5X1s+F2uT1K1h6AwJwTERFR/lg4EclIEAT09nVHK097fLr9Co7djMGcv4JxMDgS3/TzgYcdZ2HLlyQB8fd0h9tFXAYyEvO2NTZ/4Z6kpoCtJ6DgsEgiIiIqPhZORAbAxVqNn0a2wC9n7+Or3ddx5m4ceiw7ji8CGuJtv2oQqnJPiCQBieHPDbd72puU9iRvWyM14NJEt0hyqMsFZImIiOilsXAiMhCCIGDIKzXQro4DPtl2GRfuPcGnv13BgWuRmPumN5ws1XKHWD6So19YUPYSkByVt53COGcB2ecnb3BsACiNyz9mIiIiqvRYOBEZmJoO5tj6QWusPn4Xiw/cxKHr0biw5Bi+6uuNnt6VbLKC1LgXiqRAIPFh3naCEnDyejrkLndB2UaAkaq8IyYiIqIqioUTkQFSKgR82LE2OtV3xMe/Xsb1iER89MtF9PF1w+w3GsParAL2qqQn5tyHlDvULvwS8CQsn4YC4FDvWS+SezPAuTFgwvu9iIiISD4snIgMWAMXK+wc2xb/O3wL3x65jR2B4ThzNw4L+jVBh3oGvJ5QZkrOtN+5EzeEXwJib+Xf1q6W7j1Jrj6AyrJ84yUiIiIqAgsnIgNnYqTAFP/6eNXLCZ9svYzQxykYtvYchrxSHZ/19IKZicw/xtkZwKMLQERgzlC78ItATAggiXnbWnvoFkluvoCpbTkHTERERFRyLJyIKohm1W2xZ0J7fL0vBOtPhWHjmfs4fusxFr3tg+Y17conCE0WEB2sHWonhF+Cc1QwBDErb1sL55z7kXInb3D1BSwMuJeMiIiIqBAsnIgqEFMTJWa90QhdGzpjyrbLuBebiv7fn8b7HWrj4651oTIqw2m3RQ3w+OazoXbhl3KG32kytE1yJ0mXzOwhaHuRnhZKVpVsIgsiIiKq0mQvnFauXIlvvvkGkZGR8PHxwfLly9GyZcsC2y9duhTfffcd7t+/DwcHB/Tr1w/z5s2DWl1FpmomAtC2jgP2TeqA2X9ew+8XH2HV0Ts4ciMai/v7oqGbVckPKIpA3F3ddZIiLgNZqXnbqqy1C8qKrr54rKoOh1q+EJRcK4mIiIgqL1kLp19//RWTJ0/GqlWr0KpVKyxduhT+/v64ceMGnJyc8rTftGkTpk2bhrVr16JNmza4efMmRowYAUEQsHjxYhneAZF8rE2Nsbi/L/wbueCz34MQEpmE3itPYFKXevigQy0YKRX5v1CSgPh7z3qRHl3MKZIyEvO2NTbXFknah60noHh6bFGEGB0NVOUFeomIiKhKkLVwWrx4MUaPHo2RI0cCAFatWoXdu3dj7dq1mDZtWp72p06dQtu2bTFo0CAAQM2aNTFw4ECcPXu2XOMmMiT+jVzgV8MWn/0ehAPBUfhm/w0cuh6Fxf194WlvBiSG511QNi0u74GM1ICL97Ohdm5NAYe6gII9SURERESyFU6ZmZm4cOECpk+frt2nUCjQpUsXnD59Ot/XtGnTBhs3bsS5c+fQsmVL3L17F3v27MHQoUMLPE9GRgYyMp7dk5GYmPNXdVEUIYr5zPpVzkRRhCRJBhFLZVRV8mtnZozvBjfFnjNXsPfAPtR9dAthy0PhrLoPs8zHedpLCuOcBWTdfCG5NcuZuMGxAaDMZ32oQnJXVfIrF+ZXv5hf/WJ+9Yv51S/mV78MKb8liUG2wunx48fQaDRwdnbW2e/s7IyQkJB8XzNo0CA8fvwY7dq1gyRJyM7OxocffojPPvuswPPMmzcPs2fPzrM/JiYG6enpL/cmyoAoikhISIAkSVAoChhaRaVWmfMrpMfDOOYajGOCYBxzFcYxVxGQHIEAAUBu/ZMJaKBApk0dSC7eyHJsjCxHb2Tb1weUJroHjH1S4hgqc34NAfOrX8yvfjG/+sX86hfzq1+GlN+kpKRit5V9coiSOHLkCObOnYtvv/0WrVq1wu3btzFx4kTMmTMHM2bMyPc106dPx+TJk7XbiYmJ8PDwgKOjI6ysSnETfRkTRRGCIMDR0VH2C6cyqjT5zUgEIq5opwBHxCUIT8LyNJMgAA71ILn64nxmDSwNNselrOowjjfHzHYN0bepG4QyvB+p0uTXQDG/+sX86hfzq1/Mr34xv/plSPktyQRzshVODg4OUCqViIqK0tkfFRUFFxeXfF8zY8YMDB06FKNGjQIAeHt7IyUlBe+//z7+7//+L9/Eq1QqqFSqPPsVCoXs36hcgiAYVDyVTYXLb2ZKzrTfz0/eEHsr/7a2ns/WSXJrCsGlCaC2ggCgFYCvYpIxeetlBD6Ix5TfruDg9SjM7esNe4u8PxOlVeHyW8Ewv/rF/OoX86tfzK9+Mb/6ZSj5Lcn5ZSucTExM4Ofnh8OHD6NPnz4AcqrPw4cPY9y4cfm+JjU1Nc+bUz6dAlmSJL3GS6QX2RlA5NWnU4AH5hRKMdcBKZ/xttYeT2e4yy2UfAFT20IPX8vRAr992BrfH7uLpYduYv+1KPwb9gRz3/SGf6P8/0BBRERERHnJOlRv8uTJGD58OJo3b46WLVti6dKlSElJ0c6yN2zYMLi7u2PevHkAgICAACxevBhNmzbVDtWbMWMGAgICtAUUkcHSZAHR15+tkxR+CYgKBsSsvG0tnJ8VSO5PJ2+wcCzVaY2UCoztXAed6jti8q+XcSMqCR/8fAFvNauGmW80hJU6nwkhiIiIiEiHrIXTgAEDEBMTgy+++AKRkZHw9fXFvn37tBNG3L9/X6eH6fPPP4cgCPj888/x6NEjODo6IiAgAF999ZVcb4Eof6IGeHzz2VC78Es5w+80GXnbmtrpDLeDWzPAyrXMQ2rkZo1d49tiycFb+P7YHWy/+BCn7zzGN2/7oG0dhzI/HxEREVFlIkhVbIxbYmIirK2tkZCQYDCTQ0RHR8PJyUn2MZ6VUbnkVxSBuLvPrZN0MWcih6yUvG1V1roLyro3yxmCV84LyP4bFodPtl3GvdhUAMCINjUxtXsDmJqUrOeW169+Mb/6xfzqF/OrX8yvfjG/+mVI+S1JbVChZtUjkp0kAfH3XlhQ9jKQkZC3rbE54Oqj25tk6wkYwC/g5jXtsGdCe8zbex0bz9zH+lNhOHYzBov6+6Bp9cLvmyIiIiKqilg4ERVEkoCkiGdD7XIfaXF52xqpARfv5yZuaAo41AUUhnvvnbnKCP/t442uDV3w6W+XcfdxCt767hQ+6lQHE16rCxMj+Qs8IiIiIkPBwokoV3LMs6F2uUVSclTedgpjwLnRs6F2bk0BxwaAsmJOstCxniMOTOqIL3Zdxc7AcKz45zb+DonGkgG+qO9iKXd4RERERAaBhRNVTalxQETgc5M3BAKJD/O2E5SAk5fufUnOjQGjslsHyRBYmxlj2TtN4d/IBf/3RxCCIxIRsPwEJnerh9Hta0GpKN97sIiIiIgMDQsnqvSEzGQg7OazQin8IvAkLL+WgEO952a3a5oz/M7ErJwjlk9Pb1c0r2mL6duDcDgkGvP3huBQcBQW9fdBDXtzucMjIiIikg0LJ6pcMlNypv1+OtROCL8Ep8e3ICCfySNtPXUnbnBpAqjln2lRbk6Wavw4vDm2/fsQX/4VjH/vPUGPZcfxWU8vDG5VHUI5zwBIREREZAhYOFHFlZ0BRF59ek9SYE6xFHMdkERtk9yP+JJ1NQi5ayS5Nc2Z7c7MTpawKwJBENC/hQda17bHlG2XcTY0Dp/vuIqDwVH4+q0mcLFWyx0iERERUbli4UQVgyYLiL6uO3lDVDAgZuVta+GsLZBEV188NqkGhxpeEAxgGvCKxsPODJtHv4J1p8KwYF8Ijt6MQbclRzGnT2O84eMmd3hERERE5YaFExkeUQM8vvlsZrtHF3OG32ky8rY1tdMdbufWDLByfe5YIsTo6PKLvRJSKAS8184THes5YPLWy7jyMAETtwTiwLUozH6jodzhEREREZULFk4kL1EE4u4+t07SRSDiCpCVkretylp3dju3poBNdYD33JSLOk6W2D6mDVb+cxsr/r6N3UEROBsai2mveuBNJye5wyMiIiLSKxZOVH4kCYi/r7tOUvhlICMhb1tj85z7kJ7vTbL1BDjcTlbGSgUmdamH1xo4Y/LWQNyKTsaUXXdwLjwDM15vCEt1xVzLioiIiKgoLJxIPyQJSIp4ukbSpWePtLi8bY3UOdN+Pz95g0NdQKEs/7ipWLyrWePP8e2wcP8NrDkRiq3/PsTJ27FY1N8Hr9Sylzs8IiIiojLHwonKRnKM7sQN4ZeA5Ki87RTGgHOjZ71I7s0AxwaAkj0VFY3aWInPejZAMxdjzD38AA+fpGHg6jN4t60n/uNfH2pjFr5ERERUebxU4ZSZmYnQ0FDUrl0bRkaswaqM1Lhni8k+ejoVeOLDvO0EJeDkpXtfknNjwEhVzgGTPjWrZok9E9ph7p4QbDn/AGtOhOLozRgs7u+DJtVs5A6PiIiIqEyUqtpJTU3F+PHj8dNPPwEAbt68iVq1amH8+PFwd3fHtGnTyjRIklF6IhBxWXfyhidh+TQUcobXuT2/oKw3YGJW3hGTDCxURpj/VhN0a+SMqduDcDs6GX2/PYVxnetg3Kt1YKzkvWlERERUsZWqcJo+fTouX76MI0eOoHv37tr9Xbp0waxZs1g4VVSZqUDkFd17kh7fAiDlbWvr+WyonVtTwKUJoLYq95DJsLzawBkHJtni851XsftKBJYdvoW/Q6KxuL8P6jpbyh0eERERUamVqnDasWMHfv31V7zyyisQnpsKulGjRrhz506ZBUd6lJ0BRF19NtQu/BIQcx2QxLxtrT2eG27XLGe2OzO78o6YKghbcxOsHNQM/o3CMWPHVQQ9SkCv5SfwqX99vNvWEwoFp48nIiKiiqdUhVNMTAyc8lm3JSUlRaeQIgOhyQKir+tO3hAVDIhZedtaOOsOt3NrClg4ln/MVOG94eOGVp52+PS3Kzh6Mwb/3X0dB4KjsOhtH3jYcQgnERERVSylKpyaN2+O3bt3Y/z48QCgLZZ+/PFHtG7duuyio5ITNcDjm7rD7SKDgOz0vG1N7XTXSXJrCli5lX/MVGk5W6mxfmQLbD73AP/dHYxzoXHovvQYvghoiP7NPfiHFiIiIqowSlU4zZ07Fz169EBwcDCys7OxbNkyBAcH49SpUzh69GhZx1j5xD8AUmNzvpYkGMXFAZoIIPdDpJk9YONR9HFEEYi7q1skRVwGslLytlVZ685u59YUsKn+7JxEeiIIAga1qo62dewxZdtlnA97gqnbg7D/WhTmv+kNJyu13CESERERFalUhVO7du1w+fJlzJs3D97e3jhw4ACaNWuG06dPw9vbu6xjrFziHwAr/HLuMQKgAODwYhsjFTDugm7xJElA/H3ddZLCLwMZCXnPYWyecx/S85M32HoCCs5sRvKpYW+OLe+3xpoTd7Fw/038HRKNbkuP4b99GuP1JuzpJCIiIsNW4sIpKysLH3zwAWbMmIHVq1frI6bKLTVWWzQVKDsDiAnJmeHu0XOFUlpc3rZG6pxpv7U9Sc1ypgVXcPFRMjxKhYD3O9RGx3pOmLw1ENfCEzFu0yUcuBaFL3s3go2ZidwhEhEREeWrxIWTsbExtm/fjhkzZugjHsr1S7+8+xTGgHMj3eF2Tl6A0rj84yN6CfVdLPHHR22x/O9b+PbIHey6HI6zobH4+q0m6FQ/78QzRERERHIr1VC9Pn36YMeOHfj444/LOh7SEgCnhoD780VSI8CY94NQ5WBipMAn3erjNS9nTN4aiLsxKRix7jwGtaqO/+vpBXNVqX49EREREelFqT6Z1K1bF19++SVOnjwJPz8/mJub6zw/YcKEMgmuSnt3H1D9FbmjINI7Xw8b7B7fHgv2h2DdyTBsOnsfJ249xqL+PmhRk+uFERERkWEoVeG0Zs0a2NjY4MKFC7hw4YLOc4IgsHAqC0bsWaKqw9REiZkBjdDVyxn/+e0K7selov/3p/F++1r4uGs9qI15zx4RERHJq1SFU2hoaFnHQUSENnUcsHdSe3z5ZzB+u/AQ3x+7i39uRGNxf180dreWOzwiIiKqwl56fmpJkiBJUlnEQkQEK7UxFr7tg9XDmsPBwgQ3o5LRZ+VJLD98C9kaUe7wiIiIqIoqdeG0YcMGeHt7w9TUFKampmjSpAl+/vnnsoytcjKzz1mnqTBGqpx2RFVY14bO2D+pA7o3ckG2KGHRwZt4a9Vp3IlJljs0IiIiqoJKNVRv8eLFmDFjBsaNG4e2bdsCAE6cOIEPP/wQjx8/5mx7hbHxyFncNjUWACBKEuLi4mBnZweFIOS0MbPXXfyWqIqyt1DhuyHNsCPwEb7YeQ2XH8Sj1/+OY2r3BhjeuiYUCkHuEImIiKiKKFXhtHz5cnz33XcYNmyYdt8bb7yBRo0aYdasWSycimLj8awwEkVkK6MBJydA8dIjJ4kqHUEQ0LdpNbTytMfU7Vdw/NZjzP4zGAeDo/DN2z5wtzGVO0QiIiKqAkr1ST0iIgJt2rTJs79NmzaIiIh46aCIiF7kZmOKDe+2xJzejaA2VuDUnVh0X3IM2/59wPssiYiISO9KVTjVqVMHW7duzbP/119/Rd26dV86KCKi/AiCgKGta2LvxA5oVt0GSRnZ+M9vV/D+zxfwODlD7vCIiIioEivVUL3Zs2djwIABOHbsmPYep5MnT+Lw4cP5FlRERGXJ08Ec2z5sg++P3cGSgzdxMDgKF+49wdy+jdG9savc4REREVElVKoep7feegtnz56Fg4MDduzYgR07dsDBwQHnzp1D3759yzpGIqI8lAoBH3Wqg51j26GBiyXiUjLx4caLmPxrIBLSsuQOj4iIiCqZUvU4AYCfnx82btxYlrEQEZVYQzcr7BzXFksP3cL3R+/g90uPcPpuLBb0a4L2dR3lDo+IiIgqiVL1OO3Zswf79+/Ps3///v3Yu3fvSwdFRFQSKiMlpnZvgG0ftkZNezNEJKRj6Jpz+GLnVaRmZssdHhEREVUCpSqcpk2bBo1Gk2e/JEmYNm3aSwdFRFQafjXssGdiewxrXQMAsOH0PfRcdhwX7j2ROTIiIiKq6EpVON26dQsNGzbMs79Bgwa4ffv2SwdFRFRaZiZG+LJ3Y/z8Xku4WKkRFpuKt1edwoJ9IcjMFuUOj4iIiCqoUhVO1tbWuHv3bp79t2/fhrm5eYmOtXLlStSsWRNqtRqtWrXCuXPnCm0fHx+PsWPHwtXVFSqVCvXq1cOePXtKdE4iqvza13XE/kkd0LepO0QJ+PbIHfReeRLXIxLlDo2IiIgqoFIVTr1798akSZNw584d7b7bt2/jk08+wRtvvFHs4/z666+YPHkyZs6ciYsXL8LHxwf+/v6Ijo7Ot31mZia6du2KsLAw/Pbbb7hx4wZWr14Nd3f30rwNIqrkrM2MsWSAL1YNaQY7cxNcj0jEGytO4Nsjt6ERuWguERERFV+pCqcFCxbA3NwcDRo0gKenJzw9PdGgQQPY29tj4cKFxT7O4sWLMXr0aIwcORINGzbEqlWrYGZmhrVr1+bbfu3atYiLi8OOHTvQtm1b1KxZEx07doSPj09p3gYRVRHdG7ti/6QO6OLljCyNhAX7buDtVacQ+jhF7tCIiIiogijVdOTW1tY4deoUDh48iMuXL8PU1BQ+Pj5o3759sY+RmZmJCxcuYPr06dp9CoUCXbp0wenTp/N9za5du9C6dWuMHTsWO3fuhKOjIwYNGoSpU6dCqVTm+5qMjAxkZGRotxMTc4bpiKIIUZT/fgdRFCFJkkHEUhkxv/pVkfJrb26M74c0xfaLj/DlX8G4eD8ePZcdx7Qe9TGkVXUIgiB3iHlUpPxWRMyvfjG/+sX86hfzq1+GlN+SxFCiwun06dOIjY3F66+/DkEQ0K1bN0RERGDmzJlITU1Fnz59sHz5cqhUqiKP9fjxY2g0Gjg7O+vsd3Z2RkhISL6vuXv3Lv7++28MHjwYe/bswe3bt/HRRx8hKysLM2fOzPc18+bNw+zZs/Psj4mJQXp6ejHetX6JooiEhARIkgSFolQdgFQI5le/KmJ+O3iY4OfBXvjvgXu48DAJM3cFY0/gQ/xf1xpwsjSROzwdFTG/FQnzq1/Mr34xv/rF/OqXIeU3KSmp2G1LVDh9+eWX6NSpE15//XUAQFBQEEaPHo3hw4fDy8sL33zzDdzc3DBr1qwSBVxcoijCyckJP/zwA5RKJfz8/PDo0SN88803BRZO06dPx+TJk7XbiYmJ8PDwgKOjI6ysrPQSZ0mIoghBEODo6Cj7hVMZMb/6VVHz6+QE/FqrGjacuYev993A2fuJGPzLdcwOaIjevm4G0/tUUfNbUTC/+sX86hfzq1/Mr34ZUn7VanWx25aocAoMDMScOXO021u2bEHLli2xevVqAICHhwdmzpxZrMLJwcEBSqUSUVFROvujoqLg4uKS72tcXV1hbGysMyzPy8sLkZGRyMzMhIlJ3r8Wq1SqfHvAFAqF7N+oXIIgGFQ8lQ3zq18VNb8KBfBuu1roUM8Jn2wNxOWHCZi87QoOXo/Gf/s0hr1F0T3n5aGi5reiYH71i/nVL+ZXv5hf/TKU/Jbk/CWK9MmTJzpD644ePYoePXpot1u0aIEHDx4U61gmJibw8/PD4cOHtftEUcThw4fRunXrfF/Ttm1b3L59W2cs4s2bN+Hq6ppv0UREVJQ6ThbYPqYNPulaD0YKAXuvRsJ/6TEcDI4q+sVERERUZZSocHJ2dkZoaCiAnMkdLl68iFdeeUX7fFJSEoyNjYt9vMmTJ2P16tX46aefcP36dYwZMwYpKSkYOXIkAGDYsGE6k0eMGTMGcXFxmDhxIm7evIndu3dj7ty5GDt2bEneBhGRDiOlAuNfq4sdY9uinrMFHidnYvSGf/GfbZeRlJ4ld3hERERkAEo0VK9nz56YNm0avv76a+zYsQNmZmY6M+lduXIFtWvXLvbxBgwYgJiYGHzxxReIjIyEr68v9u3bp+3Vun//vk73mYeHB/bv34+PP/4YTZo0gbu7OyZOnIipU6eW5G0QEeWrsbs1do1rh8UHb2L18bvYduEhTt2JxTdvN0Gb2g5yh0dEREQyKlHhNGfOHLz55pvo2LEjLCws8NNPP+kMkVu7di26detWogDGjRuHcePG5fvckSNH8uxr3bo1zpw5U6JzEBEVl9pYic96eqGLlzM+2RaIB3FpGLT6LEa2rYmp3RtAbZz/0gdERERUuZWocHJwcMCxY8eQkJAACwuLPGsnbdu2DRYWFmUaIBGRHFp62mHvxA74avd1bD53H+tOhuHozRgs7u8LXw8bucMjIiKiclaqaSysra3zXXDWzs6OkzQQUaVhoTLCvDe9sW5kCzhZqnA3JgVvfXcKiw/cQGa2/Iv2ERERUfnh/IpEREXoXN8JBz7ugAAfN2hECf/7+zb6fnsSN6OKv2geERERVWwsnIiIisHGzATLBzbFikFNYWNmjGvhiXh9+Qn8cOwONKIkd3hERESkZyyciIhK4PUmbjgwqQM613dEZraIuXtCMPCHM7gfmyp3aERERKRHLJyIiErIyUqNtSNaYN6b3jA3UeJcWBy6LzuGTWfvQ5LY+0RERFQZsXAiIioFQRAwsGV17JvUAS097ZCaqcFnfwRh5PrziEpMlzs8IiIiKmMsnIiIXoKHnRm2jH4Fn/fygomRAkduxKDbkmPYdTlc7tCIiIioDLFwIiJ6SQqFgFHta+Gv8e3Q2N0KCWlZmLD5EsZtuognKZlyh0dERERlgIUTEVEZqedsiT8+aosJr9WFUiHgrysR6Lb0GP4JiZY7NCIiInpJLJyIiMqQsVKByV3r4fcxbVDb0RwxSRkYuf48pv9+BckZ2XKHR0RERKXEwomISA98PGywe0J7vNfOEwCw+dwD9Fh2DGfvxsocGREREZUGCyciIj1RGysx4/WG2Dz6FbjbmOJBXBreWX0GX+0ORnqWRu7wiIiIqARYOBER6Vnr2vbYN6k9+jevBkkCVh8PRcDyEwh6mCB3aERERFRMLJyIiMqBpdoYC/r54MdhzeFgocKt6GT0/fYklh26hSyNKHd4REREVAQWTkRE5ahLQ2cc+LgDenq7IFuUsOTQTbz13Sncjk6WOzQiIiIqBAsnIqJyZmdugpWDmmHZO76wUhvhysME9Prfcaw5EQpRlOQOj4iIiPLBwomISAaCIKC3rzsOfNwR7es6ICNbxJy/gjHoxzN4EJcqd3hERET0AhZOREQycrFWY8O7LTGnT2OYGitx5m4ceiw7jq3/PoAksfeJiIjIULBwIiKSmSAIGPpKDeyd2B5+NWyRnJGNT3+7gvd/vojYlCy5wyMiIiKwcCIiMhg1Hcyx9YPWmNajAUyUChwOicagn69hT1CE3KERERFVeSyciIgMiFIh4MOOtbFrfFt4uVoiIV2DcZsDMWnLJSSksveJiIhILiyciIgMUAMXK/wxpg2Gt3CBQgB2BIbDf+kxHLsZI3doREREVRILJyIiA2VipMCYtu7Y9sEr8HQwR2RiOoatPYfPdwQhNTNb7vCIiIiqFBZOREQGrml1W+yZ0B7DW9cAAGw8cx89lh3Hv2FxMkdGRERUdbBwIiKqAExNlJjduzE2vtcKrtZq3ItNRf/vT2P+3hBkZGvkDo+IiKjSY+FERFSBtKvrgH2TOuDNZu4QJWDV0TvoveIkgsMT5Q6NiIioUmPhRERUwVibGmNxf1+sGuIHe3MThEQmoffKE1j5z21ka0S5wyMiIqqUWDgREVVQ3Ru7YP/HHdCtoTOyNBK+2X8Db39/GndjkuUOjYiIqNJh4UREVIE5WKjw/VA/LHrbB5YqI1y6H4+e/zuOn06FQRQlucMjIiKqNFg4ERFVcIIg4C2/atj3cQe0rWOP9CwRM3ddw7C15xAenyZ3eERERJUCCyciokrC3cYUP7/bCrMCGkJtrMCJ24/hv/QYtl94CEli7xMREdHLYOFERFSJKBQCRrT1xJ4J7eHrYYOk9Gx8su0yPtx4AbHJGXKHR0REVGGxcCIiqoRqOVrgtw9b4z/+9WGsFLD/WhS6LTmG/dci5Q6NiIioQmLhRERUSRkpFRjbuQ52jG2L+s6WiE3JxAc/X8AnWy8jMT1L7vCIiIgqFBZORESVXCM3a+wa3xYfdKwFQQC2X3yI7kuO4eTtx3KHRkREVGGwcCIiqgJURkpM7+GFbR+0Rg17M4QnpGPwj2cxa9c1pGVq5A6PiIjI4LFwIiKqQprXtMOeCe0xuFV1AMD6U2Ho9b/juHT/icyRERERGTYWTkREVYy5yghf9fXG+pEt4Gylwt3HKXjru1NYuP8GMrNFucMjIiIySAZROK1cuRI1a9aEWq1Gq1atcO7cuWK9bsuWLRAEAX369NFvgERElVCn+k44MKkjevu6QZSAFf/cRp+VJxESmSh3aERERAZH9sLp119/xeTJkzFz5kxcvHgRPj4+8Pf3R3R0dKGvCwsLw5QpU9C+fftyipSIqPKxNjPGsneaYuWgZrA1M0ZwRCLeWH4Sq47egUbkorlERES5ZC+cFi9ejNGjR2PkyJFo2LAhVq1aBTMzM6xdu7bA12g0GgwePBizZ89GrVq1yjFaIqLKqVcTV+z/uANea+CETI2I+XtDMOD707gXmyJ3aERERAbBSM6TZ2Zm4sKFC5g+fbp2n0KhQJcuXXD69OkCX/fll1/CyckJ7733Ho4fP17oOTIyMpCRkaHdTkzMGYIiiiJEUf6x/KIoQpIkg4ilMmJ+9Yv51a/yzq+DuQl+GNoM2y48xJy/ruPfe0/QY9lxTO/RAINaekAQhHKJo7zw+tUv5le/mF/9Yn71y5DyW5IYZC2cHj9+DI1GA2dnZ539zs7OCAkJyfc1J06cwJo1axAYGFisc8ybNw+zZ8/Osz8mJgbp6ekljrmsiaKIhIQESJIEhUL2DsBKh/nVL+ZXv+TKb6fqKtQb7IU5B8Jw6VEyZuy8ht2BD/BZ1xpwsjAptzj0jdevfjG/+sX86hfzq1+GlN+kpKRit5W1cCqppKQkDB06FKtXr4aDg0OxXjN9+nRMnjxZu52YmAgPDw84OjrCyspKX6EWmyiKEAQBjo6Osl84lRHzq1/Mr37JmV8nJ2Bb7WpYfzoMC/bfxJl7iRiy8Tpmv9EIb/i4VoreJ16/+sX86hfzq1/Mr34ZUn7VanWx28paODk4OECpVCIqKkpnf1RUFFxcXPK0v3PnDsLCwhAQEKDdl9u9ZmRkhBs3bqB27do6r1GpVFCpVHmOpVAoZP9G5RIEwaDiqWyYX/1ifvVLzvwqFMCo9rXRqb4TJm+9jCsPE/Dx1ss4dD0ac/o0hp15xe994vWrX8yvfjG/+sX86peh5Lck55c1UhMTE/j5+eHw4cPafaIo4vDhw2jdunWe9g0aNEBQUBACAwO1jzfeeAOdO3dGYGAgPDw8yjN8IqIqoY6TJbaPaYNJXerCSCFgd1AEui05hsPXo4p+MRERUSUh+1C9yZMnY/jw4WjevDlatmyJpUuXIiUlBSNHjgQADBs2DO7u7pg3bx7UajUaN26s83obGxsAyLOfiIjKjrFSgUld6uG1Bs74eGsgbkcn472f/sWA5h74/HUvWKqN5Q6RiIhIr2QvnAYMGICYmBh88cUXiIyMhK+vL/bt26edMOL+/fuyd+EREVEO72rW+Gt8OyzcfwNrTobi138f4MTtx1j4tg9a17aXOzwiIiK9ESRJqlIrHCYmJsLa2hoJCQkGMzlEdHQ0nJycWCDqAfOrX8yvfhl6fs/cjcWUbZfx8EkaAOC9dp74j399qI2VMkdWPIae34qO+dUv5le/mF/9MqT8lqQ24JVARESl8kote+yb1AHvtMi5v3TNiVC8vvwErjyMlzcwIiIiPWDhREREpWahMsL8t5pg7YjmcLRU4XZ0Mvp+ewpLDt5Elkb+hQ2JiIjKCgsnIiJ6aa82cMaBSR3Qq4krNKKEZYdv4c1vT+FWVPEXFiQiIjJkLJyIiKhM2JqbYOWgZvjfwKawNjVG0KME9Fp+Aj8evwtRrFK30xIRUSXEwomIiMrUGz5uOPBxB3Ss54jMbBH/3X0d76w+gwdxqXKHRkREVGosnIiIqMw5W6mxfmQLfNW3McxMlDgXGofuS4/h1/P3UcUmcyUiokqChRMREemFIAgY3KoG9k5sjxY1bZGSqcHU7UF476d/EZ2YLnd4REREJcLCiYiI9KqGvTm2vN8an/VsABOlAn+HRKPb0mP460q43KEREREVGwsnIiLSO6VCwPsdauPP8e3QyM0K8alZGLfpEsZvvoT41Ey5wyMiIioSCyciIio39V0s8cdHbTH+1TpQKgT8eTkc3ZYcw5Eb0XKHRkREVCgWTkREVK5MjBT4pFt9bB/TBrUczBGdlIER687jsz+CkJKRLXd4RERE+WLhREREsvD1sMHuCe0xok1NAMCms/fRY9lxnA+LkzcwIiKifLBwIiIi2ZiaKDHrjUbYNKoV3G1McT8uFf2/P415e64jPUsjd3hERERaLJyIiEh2beo4YO+k9ujnVw2SBHx/7C7eWHECVx8lyB0aERERABZORERkIKzUxlj4tg9+GOoHBwsT3IxKRp+VJ7H88C1ka0S5wyMioiqOhRMRERmUbo1csH9SB3Rv5IJsUcKigzfx1qrTuBOTLHdoRERUhbFwIiIig2NvocJ3Q5phyQAfWKqNcPlBPHouO451J0MhipLc4RERURXEwomIiAySIAjo27Qa9k/qgHZ1HJCRLWL2n8EYsuYsHsWnyR0eERFVMSyciIjIoLnZmGLDuy3xZe9GUBsrcOpOLLovOYZt/z6AJLH3iYiIygcLJyIiMngKhYBhrWti78QOaFbdBkkZ2fjPb1fw/s8X8Dg5Q+7wiIioCmDhREREFYangzm2fdgGn3avD2OlgIPBUei25Bj2XY2QOzQiIqrkWDgREVGFolQI+KhTHewc2w4NXCwRl5KJDzdexORfA5GQliV3eEREVEmxcCIiogqpoZsVdo5rizGdakMhAL9feoTuS4/h+K0YuUMjIqJKiIUTERFVWCojJaZ2b4BtH7ZGDXszRCSkY+iac/hi51WkZmbLHR4REVUiLJyIiKjC86thh70T22PoKzUAABtO30PPZcdx4d4TmSMjIqLKgoUTERFVCmYmRpjTpzE2vNsSLlZqhMWm4u1Vp7BgXwgysjVyh0dERBUcCyciIqpUOtRzxP5JHdC3qTtECfj2yB30XnES1yMStW00ooQzd2NxICQOZ+7GQiNyPSgiIiqckdwBEBERlTVrM2MsGeCLbg2d8dkfQQiJTMIbK07g4671UNPOHHN2ByMiIf1p61C4WqsxM6Ahujd2lTVuIiIyXOxxIiKiSquHtysOfNwRXbyckaWRsGDfDXy06eJzRVOOyIR0jNl4ketBERFRgVg4ERFRpeZoqcLqYX74+i1vCAW0yR2oN/vPYA7bIyKifLFwIiKiSk8QBFS3M0dhJZEEICIhHQv2heD4rRjcjk5CcganNCciohy8x4mIiKqE6KT0ohsB+P7YXXx/7K5221JlBBdrNVys1XCzNoWLtRquT7ddn25bqY0gCAX1ZxERUWXAwomIiKoEJ0t1sdo1qWaN9CwNIhLSkZSejaSMbCRFJ+NWdHKBrzEzUWoLKldr0+cKKzVcrHK2bcyMWVwREVVgLJyIiKhKaOlpB1drNSIT0vMdsicAcLFW44+P2kKpyClwkjOyEZmQjsiEdEQkpCEyIR3hCemITEhDREI6IhPTEZ+ahdRMDe7GpOBuTEqB51cZKbQFlW7P1bNCy87MBAoFiysiIkPEwomIiKoEpULAzICGGLPxIgRAp3jKLVVmBjTUFk0AYKEyQh0nC9RxsijwuGmZGkQmPiusIhJ0v45MSEdsSiYyskWExaYiLDa1wGOZKBVwtlbB1coUrjZPe62snhVXrtZq2FuodGIkIqLywcKJiIiqjO6NXfHdkGaY/WewzpTkLi+xjpOpiRKeDubwdDAvsE16lgbRiRk5BVXi0+Iq/lmvVURCOh4nZyBTI+JBXBoexKUVeCwjhQBnK/VzQwN1e61crdVwtFDBSMn5n4iIyhILJyIiqlK6N3ZF14YuOHv3MW4/jEGdao5oVctBr704amMlqtubobq9WYFtMrNFRCel6/RU5fZe5W5HJ6UjW5TwKD4Nj+ILLq4UQs49XS7WarjZPLvP6vmJLZyt1DBmcUVEVGwsnIiIqMpRKgS8UssetSw0cHKyN4j7ikyMFKhma4ZqtgUXV9kaETHJGTqFVe79Vrn7ohJziqvIxJzerMAH+R9LEAAHC9XTCSzUcLN57r4rq5xJLpytVVAZKfX0jomIKhYWTkRERBWEkVLxdNY+0wLbaEQJsU+LK21hlZiOiPinxVZiGqIScoYFxiRlICYpA1eQUODx7M1NdGYMzDMdu5UapiYsroio8jOIwmnlypX45ptvEBkZCR8fHyxfvhwtW7bMt+3q1auxYcMGXL16FQDg5+eHuXPnFtieiIioKlEqBDhZqeFkpYaPR/5tRFFCXGpmnl6rnFkDn01skZEtIjYlE7EpmbgWnljgOW3MjJ/2UqnhamMKVys1nK1UUEvp8IIZ3GzNYK4yiI8cRESlJvtvsV9//RWTJ0/GqlWr0KpVKyxduhT+/v64ceMGnJyc8rQ/cuQIBg4ciDZt2kCtVuPrr79Gt27dcO3aNbi7u8vwDoiIiCoWhUKAg4UKDhYqNHa3zreNJEmIT816OoFFWr73XUXEpyMtS4P41CzEp2YhJDIpnyPdAgBYqo20E1m46dxv9ez+K0sVFxImIsMlSJKU33IW5aZVq1Zo0aIFVqxYAQAQRREeHh4YP348pk2bVuTrNRoNbG1tsWLFCgwbNqzI9omJibC2tkZCQgKsrKxeOv6XJYoioqOj4eTkBIWCN+mWNeZXv5hf/WJ+9Yv5fXmSJCExPVtnnavne64exiYjJiUbyRnZxTqeuXYhYdMCZwy0NuVCwgCvX31jfvXLkPJbktpA1h6nzMxMXLhwAdOnT9fuUygU6NKlC06fPl2sY6SmpiIrKwt2dnb5Pp+RkYGMjAztdmJizlADURQhiuJLRF82RFGEJEkGEUtlxPzqF/OrX8yvfjG/ZcNSpYSlkznqOulOxy6KImJiYuDo6IiUTA2iEjO0069HaourZ9sJaVlIydTgTkwK7hSykLDaWPFsWODTe65crFTa4srFSg07c5NKX1zx+tUv5le/DCm/JYlB1sLp8ePH0Gg0cHZ21tnv7OyMkJCQYh1j6tSpcHNzQ5cuXfJ9ft68eZg9e3ae/TExMUhPT8/nFeVLFEUkJCRAkiTZK+7KiPnVL+ZXv5hf/WJ+9evF/FoBsLIG6lubAB4mAHT/spuWpUFMchaikzMRnfT0X+12ztfxadlIzyrOQsICHC2M4WRhAidLEzjlfm1hAifLnK9tzYygqMDFFa9f/WJ+9cuQ8puUlN8Q4/zJfo/Ty5g/fz62bNmCI0eOQK1W59tm+vTpmDx5snY7MTERHh4ecHR0NJiheoIgwNHRUfYLpzJifvWL+dUv5le/mF/9Kk1+axTxfEaWRttDFaHtucpAZOKzSS0eJ2ciUyPhUUImHiVkFngsY6WgXevK9blp2J+fkt3RUqXX9b1eBq9f/WJ+9cuQ8ltQDZEfWQsnBwcHKJVKREVF6eyPioqCi4tLoa9duHAh5s+fj0OHDqFJkyYFtlOpVFCpVHn2KxQK2b9RuQRBMKh4KhvmV7+YX/1ifvWL+dWvss6vqUoBT0djeDpaFtgmM1tE1NM1rPLOGJizHZ2UgSxN0QsJKxUCnCxVOQsJ55mKPef+KydLlWwLCfP61S/mV78MJb8lOb+shZOJiQn8/Pxw+PBh9OnTB0BOBXr48GGMGzeuwNctWLAAX331Ffbv34/mzZuXU7RERERk6EyMFPCwM4OHXcELCWc9XcPq2UyBadperIj4nK+jkjKgESXteliXEJ/vsQQBcLRQ6axt9eLXTlZcSJioMpB9qN7kyZMxfPhwNG/eHC1btsTSpUuRkpKCkSNHAgCGDRsGd3d3zJs3DwDw9ddf44svvsCmTZtQs2ZNREZGAgAsLCxgYWEh2/sgIiKiisFYqYCbjSncbApfSPhxckaeXqvnp2OPSkxHlkZCdFIGopMycPlhwQsJO1iYPJ28whRuNs8PCXxWaKmNWVwRGTLZC6cBAwYgJiYGX3zxBSIjI+Hr64t9+/ZpJ4y4f/++Thfad999h8zMTPTr10/nODNnzsSsWbPKM3QiIiKqpJQKAc5WajhbqQEPm3zbiKKE2JTMZ71Wic9Nxx7/bDszW8Tj5Ew8Ts7E1UcFLyRsa2asnX69oOnYzUxk/+hGVGUZxE/fuHHjChyad+TIEZ3tsLAw/QdEREREVASFQoCjpQqOlip4Vyt4IeEnqVl51rnSLiwcn/N1WpYGT1Kz8CQ1C9cjCi6urNRGT6dhV8HGBPB0ToCbjZnO/VeWamN9vWWiKs0gCiciIiKiykgQBNiZm8DO3ASN3AourhLTshGRqDskMHeIYERCzr1XKZkaJKZnIzE9CTeink6hfPVxnuNZqIx0Zwt8odfK1coUVqZGlX6tK6KyxsKJiIiISEaCIMDazBjWZsZo4FLwUilJ6Vnaoio8PhV3wmORpDHKGRIYnzNcMDE9G8kZ2bgdnYzb0ckFHsvUWFnAhBbP9tmaGbO4InoOCyciIiKiCsBSbQxLtTHqOltCFEVER6vg5OSkcy94Skb2s7WuCpiO/UlqFtKyNLj7OAV3H6cUeD4TI4V2TSs3G1OdNa5cn07Pbm9uAoWBrnVFVNZYOBERERFVEuYqI9R2tEBtx4JnGk7P0ujeZ/XCjIHahYSzRdyLTcW92NQCj2WszJlEI3dIoNsL61y5WqvhYGG4CwkTlQQLJyIiIqIqRG2sRE0Hc9R0MC+wTUa2BtGJGTrFlLbAerreVUxyzkLCD5+k4eGTNABP8j2WUiHA+elCwq42pnC1ejYcMLfIcrJUwUimhYSJiouFExERERHpUBkpi7WQcHRSRp51rnKnZ89d60ojSgh/OlQQ9+PzPZZCABwtVTm9VFZquNq8MLHF06nhTYxYXJF8WDgRERERUYkZKxVwtzGFeyELCWdrctaw0um1StS9/yp3IeGoxAxEJWbgciHndLBQaSewcMtnxkBnKy4kTPrDwomIiIiI9MJIqYDL08KmIKIo4XFKRt51rp6bjj0yIR2ZGhGPkzPwODkDQY8SCjyenbnJ0wkscnuuTLXbuUMETU1YXFHJsXAiIiIiItkoFAKcLNVwslSjSbX820iShLiUTJ37rJ4fIpgza2Aa0rNExKVkIi4lE8GFLCRsbWpc4HTsufdcEb2IhRMRERERGTRBEGBvoYK9hQqN3QteSDghLSvfXqvc4YG5CwknpGUhIS0LIZFJBZ7T3EQBNxuzp8MCTfOsc+VirYaVmgsJVyUsnIiIiIiowhMEATZmJrAxM4GXa/4LCUuShKSM7HzXuYp4rucqKT0bKZkibkUn41YhCwmbmSif66l6YSFhq5xtGy4kXGmwcCIiIiKiKkEQBFipjWGlNkY9Z8sC2yWmZSI4NByZRmaISsx4bnjgs/Wu4lOzkJqpwd2YFNyNKXghYVXuQsJ5eq6eFVp2ZlxIuCJg4URERERE9BwLlRFq2qnh5OQAhSL/KdDTMjVPhwCm5Z3YIvHZQsIZ2SLCYlMRVshCwiZKBZytVXC1elpY2aifrndlqr3vyp4LCcuOhRMRERERUQmZmijh6WAOz0IWEk7Pyl1IOO25adifFVvhCel4nJyBTI2IB3FpeBCXVuCxjBQCnJ8uHlzQdOyOFlxIWJ9YOBERERER6YHaWInq9maobl/wQsKZ2SKik15cQDin1yp3OyoxHdmihEfxaXgUX3BxpRAAJ0t1wfddPV3rypjFVamwcCIiIiIikomJkQLVbM1Qzbbg4ipbIyImOSP/GQOfbucWV5GJObMIBj7I/1iC8NxCwlZquNk8d9+VVU6x5WytgspIP2tdaUQJZ+/G4vbDONRJVqJVLYcKMwSRhRMRERERkQEzUiqe9h6ZFthGI0qIfVpcaQurRN2erNyFhGOSMhCTlIErKHghYXtzE52eqzzTsVupS7yQ8L6rEZj9ZzAiEtKf7gmFq7UaMwMaontj1xIdSw4snIiIiIiIKjilQoCTlRpOVmr4eOTfRhQlxKVmFjwde2I6wuPTkJEtIjYlE7EpmbgWXvBCwjZmxk97qdRwtTF9OqGFbqFlrsopN/ZdjcCYjRchvXCMyIR0jNl4Ed8NaWbwxRMLJyIiIiKiKkChEOBgoYJDEQsJx6dm5bnP6vmJLSIS0pGaqUF8ahbiUwtfSNhSbQQXKxXuxaXlKZoAQAIgAJj9ZzC6NnQx6GF7LJyIiIiIiAhAzlpXtuYmsDU3QUO3ghcSTkzP1pkhUFtYJT7ryUpKz9Y+CiMBiEhIx7nQOLSuba+Hd1U2WDgREREREVGxCYIAa1NjWJsao75LwQsJJ2dkIzIhDb9ffIRvj9wp8rjRSelFtpET5yIkIiIiIqIyZ6EyQh0nS7Sv61is9k6Waj1H9HJYOBERERERkd609LSDq7UaBd29JABwtVajpaddeYZVYiyciIiIiIhIb5QKATMDGgJAnuIpd3tmQEODnhgCYOFERERERER61r2xK74b0gwu1rrD8Vys1RViKnKAk0MQEREREVE56N7YFV0buuDs3ce4/TAGdao5olUtB4PvacrFwomIiIiIiMqFUiHglVr2qGWhgZOTPRQVpGgCOFSPiIiIiIioSCyciIiIiIiIisDCiYiIiIiIqAgsnIiIiIiIiIrAwomIiIiIiKgILJyIiIiIiIiKUOWmI5ckCQCQmJgocyQ5RFFEUlIS1Go1FArWsWWN+dUv5le/mF/9Yn71i/nVL+ZXv5hf/TKk/ObWBLk1QmGqXOGUlJQEAPDw8JA5EiIiIiIiMgRJSUmwtrYutI0gFae8qkREUUR4eDgsLS0hCPIvuJWYmAgPDw88ePAAVlZWcodT6TC/+sX86hfzq1/Mr34xv/rF/OoX86tfhpRfSZKQlJQENze3Inu/qlyPk0KhQLVq1eQOIw8rKyvZL5zKjPnVL+ZXv5hf/WJ+9Yv51S/mV7+YX/0ylPwW1dOUi4M2iYiIiIiIisDCiYiIiIiIqAgsnGSmUqkwc+ZMqFQquUOplJhf/WJ+9Yv51S/mV7+YX/1ifvWL+dWviprfKjc5BBERERERUUmxx4mIiIiIiKgILJyIiIiIiIiKwMKJiIiIiIioCCyciIiIiIiIisDCqYytXLkSNWvWhFqtRqtWrXDu3LlC22/btg0NGjSAWq2Gt7c39uzZo/O8JEn44osv4OrqClNTU3Tp0gW3bt3S51swaCXJ7+rVq9G+fXvY2trC1tYWXbp0ydN+xIgREARB59G9e3d9vw2DVZL8rl+/Pk/u1Gq1Thtev7pKkt9OnTrlya8gCOjVq5e2Da/fZ44dO4aAgAC4ublBEATs2LGjyNccOXIEzZo1g0qlQp06dbB+/fo8bUr6O72yKml+f//9d3Tt2hWOjo6wsrJC69atsX//fp02s2bNynP9NmjQQI/vwnCVNL9HjhzJ9/dDZGSkTjtevzlKmt/8frcKgoBGjRpp2/D6fWbevHlo0aIFLC0t4eTkhD59+uDGjRtFvq4ifgZm4VSGfv31V0yePBkzZ87ExYsX4ePjA39/f0RHR+fb/tSpUxg4cCDee+89XLp0CX369EGfPn1w9epVbZsFCxbgf//7H1atWoWzZ8/C3Nwc/v7+SE9PL6+3ZTBKmt8jR45g4MCB+Oeff3D69Gl4eHigW7duePTokU677t27IyIiQvvYvHlzebwdg1PS/AI5K34/n7t79+7pPM/r95mS5vf333/Xye3Vq1ehVCrx9ttv67Tj9ZsjJSUFPj4+WLlyZbHah4aGolevXujcuTMCAwMxadIkjBo1SufDfWl+Jiqrkub32LFj6Nq1K/bs2YMLFy6gc+fOCAgIwKVLl3TaNWrUSOf6PXHihD7CN3glzW+uGzdu6OTPyclJ+xyv32dKmt9ly5bp5PXBgwews7PL8/uX12+Oo0ePYuzYsThz5gwOHjyIrKwsdOvWDSkpKQW+psJ+BpaozLRs2VIaO3asdluj0Uhubm7SvHnz8m3fv39/qVevXjr7WrVqJX3wwQeSJEmSKIqSi4uL9M0332ifj4+Pl1QqlbR582Y9vAPDVtL8vig7O1uytLSUfvrpJ+2+4cOHS7179y7rUCukkuZ33bp1krW1dYHH4/Wr62Wv3yVLlkiWlpZScnKydh+v3/wBkP74449C23z66adSo0aNdPYNGDBA8vf3126/7PessipOfvPTsGFDafbs2drtmTNnSj4+PmUXWCVRnPz+888/EgDpyZMnBbbh9Zu/0ly/f/zxhyQIghQWFqbdx+u3YNHR0RIA6ejRowW2qaifgdnjVEYyMzNx4cIFdOnSRbtPoVCgS5cuOH36dL6vOX36tE57APD399e2Dw0NRWRkpE4ba2trtGrVqsBjVlalye+LUlNTkZWVBTs7O539R44cgZOTE+rXr48xY8YgNja2TGOvCEqb3+TkZNSoUQMeHh7o3bs3rl27pn2O1+8zZXH9rlmzBu+88w7Mzc119vP6LZ2ifv+WxfeMnhFFEUlJSXl+/966dQtubm6oVasWBg8ejPv378sUYcXk6+sLV1dXdO3aFSdPntTu5/VbttasWYMuXbqgRo0aOvt5/eYvISEBAPL8vD+von4GZuFURh4/fgyNRgNnZ2ed/c7OznnGHOeKjIwstH3uvyU5ZmVVmvy+aOrUqXBzc9P5IezevTs2bNiAw4cP4+uvv8bRo0fRo0cPaDSaMo3f0JUmv/Xr18fatWuxc+dObNy4EaIook2bNnj48CEAXr/Pe9nr99y5c7h69SpGjRqls5/Xb+kV9Ps3MTERaWlpZfI7h55ZuHAhkpOT0b9/f+2+Vq1aYf369di3bx++++47hIaGon379khKSpIx0orB1dUVq1atwvbt27F9+3Z4eHigU6dOuHjxIoCy+T+TcoSHh2Pv3r15fv/y+s2fKIqYNGkS2rZti8aNGxfYrqJ+BjaS7cxE5Wj+/PnYsmULjhw5ojOBwTvvvKP92tvbG02aNEHt2rVx5MgRvPbaa3KEWmG0bt0arVu31m63adMGXl5e+P777zFnzhwZI6t81qxZA29vb7Rs2VJnP69fqgg2bdqE2bNnY+fOnTr34PTo0UP7dZMmTdCqVSvUqFEDW7duxXvvvSdHqBVG/fr1Ub9+fe12mzZtcOfOHSxZsgQ///yzjJFVPj/99BNsbGzQp08fnf28fvM3duxYXL16tdLe78UepzLi4OAApVKJqKgonf1RUVFwcXHJ9zUuLi6Fts/9tyTHrKxKk99cCxcuxPz583HgwAE0adKk0La1atWCg4MDbt++/dIxVyQvk99cxsbGaNq0qTZ3vH6feZn8pqSkYMuWLcX6j7iqXr+lUdDvXysrK5iampbJzwQBW7ZswahRo7B169Y8w3JeZGNjg3r16vH6LaWWLVtqc8frt2xIkoS1a9di6NChMDExKbQtr19g3Lhx+Ouvv/DPP/+gWrVqhbatqJ+BWTiVERMTE/j5+eHw4cPafaIo4vDhwzp/lX9e69atddoDwMGDB7XtPT094eLiotMmMTERZ8+eLfCYlVVp8gvkzMgyZ84c7Nu3D82bNy/yPA8fPkRsbCxcXV3LJO6KorT5fZ5Go0FQUJA2d7x+n3mZ/G7btg0ZGRkYMmRIkeepqtdvaRT1+7csfiaqus2bN2PkyJHYvHmzzjT6BUlOTsadO3d4/ZZSYGCgNne8fsvG0aNHcfv27WL94aoqX7+SJGHcuHH4448/8Pfff8PT07PI11TYz8CyTUtRCW3ZskVSqVTS+vXrpeDgYOn999+XbGxspMjISEmSJGno0KHStGnTtO1PnjwpGRkZSQsXLpSuX78uzZw5UzI2NpaCgoK0bebPny/Z2NhIO3fulK5cuSL17t1b8vT0lNLS0sr9/cmtpPmdP3++ZGJiIv32229SRESE9pGUlCRJkiQlJSVJU6ZMkU6fPi2FhoZKhw4dkpo1aybVrVtXSk9Pl+U9yqmk+Z09e7a0f/9+6c6dO9KFCxekd955R1Kr1dK1a9e0bXj9PlPS/OZq166dNGDAgDz7ef3qSkpKki5duiRdunRJAiAtXrxYunTpknTv3j1JkiRp2rRp0tChQ7Xt7969K5mZmUn/+c9/pOvXr0srV66UlEqltG/fPm2bor5nVUlJ8/vLL79IRkZG0sqVK3V+/8bHx2vbfPLJJ9KRI0ek0NBQ6eTJk1KXLl0kBwcHKTo6utzfn9xKmt8lS5ZIO3bskG7duiUFBQVJEydOlBQKhXTo0CFtG16/z5Q0v7mGDBkitWrVKt9j8vp9ZsyYMZK1tbV05MgRnZ/31NRUbZvK8hmYhVMZW758uVS9enXJxMREatmypXTmzBntcx07dpSGDx+u037r1q1SvXr1JBMTE6lRo0bS7t27dZ4XRVGaMWOG5OzsLKlUKum1116Tbty4UR5vxSCVJL81atSQAOR5zJw5U5IkSUpNTZW6desmOTo6SsbGxlKNGjWk0aNHV8n/VHKVJL+TJk3StnV2dpZ69uwpXbx4Ued4vH51lfT3Q0hIiARAOnDgQJ5j8frVlTs984uP3JwOHz5c6tixY57X+Pr6SiYmJlKtWrWkdevW5TluYd+zqqSk+e3YsWOh7SUpZ/p3V1dXycTERHJ3d5cGDBgg3b59u3zfmIEoaX6//vprqXbt2pJarZbs7OykTp06SX///Xee4/L6zVGa3w/x8fGSqamp9MMPP+R7TF6/z+SXWwA6v1Mry2dgQZIkSW/dWURERERERJUA73EiIiIiIiIqAgsnIiIiIiKiIrBwIiIiIiIiKgILJyIiIiIioiKwcCIiIiIiIioCCyciIiIiIqIisHAiIiIiIiIqAgsnIiIiIiKiIrBwIiIiKgFBELBjxw65wyAionLGwomIiCqMESNGQBCEPI/u3bvLHRoREVVyRnIHQEREVBLdu3fHunXrdPapVCqZoiEioqqCPU5ERFShqFQquLi46DxsbW0B5Ayj++6779CjRw+YmpqiVq1a+O2333ReHxQUhFdffRWmpqawt7fH+++/j+TkZJ02a9euRaNGjaBSqeDq6opx48bpPP/48WP07dsXZmZmqFu3Lnbt2qXfN01ERLJj4URERJXKjBkz8NZbb+Hy5csYPHgw3nnnHVy/fh0AkJKSAn9/f9ja2uL8+fPYtm0bDh06pFMYfffddxg7dizef/99BAUFYdeuXahTp47OOWbPno3+/fvjypUr6NmzJwYPHoy4uLhyfZ9ERFS+BEmSJLmDICIiKo4RI0Zg48aNUKvVOvs/++wzfPbZZxAEAR9++CG+++477XOvvPIKmjVrhm+//RarV6/G1KlT8eDBA5ibmwMA9uzZg4CAAISHh8PZ2Rnu7u4YOXIk/vvf/+YbgyAI+PzzzzFnzhwAOcWYhYUF9u7dy3utiIgqMd7jREREFUrnzp11CiMAsLOz037dunVrnedat26NwMBAAMD169fh4+OjLZoAoG3bthBFETdu3IAgCAgPD8drr71WaAxNmjTRfm1ubg4rKytER0eX9i0REVEFwMKJiIgqFHNz8zxD58qKqalpsdoZGxvrbAuCAFEU9RESEREZCN7jRERElcqZM2fybHt5eQEAvLy8cPnyZaSkpGifP3nyJBQKBerXrw9LS0vUrFkThw8fLteYiYjI8LHHiYiIKpSMjAxERkbq7DMyMoKDgwMAYNu2bWjevDnatWuHX375BefOncOaNWsAAIMHD8bMmTMxfPhwzJo1CzExMRg/fjyGDh0KZ2dnAMCsWbPw4YcfwsnJCT169EBSUhJOnjyJ8ePHl+8bJSIig8LCiYiIKpR9+/bB1dVVZ1/9+vUREhICIGfGuy1btuCjjz6Cq6srNm/ejIYNGwIAzMzMsH//fkycOBEtWrSAmZkZ3nrrLSxevFh7rOHDhyM9PR1LlizBlClT4ODggH79+pXfGyQiIoPEWfWIiKjSEAQBf/zxB/r06SN3KEREVMnwHiciIiIiIqIisHAiIiIiIiIqAu9xIiKiSoOjz4mISF/Y40RERERERFQEFk5ERERERERFYOFERERERERUBBZORERERERERWDhREREREREVAQWTkREREREREVg4URERERERFQEFk5ERERERERF+H+mDzYuUirNCAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Training complete!\n"
          ]
        }
      ],
      "source": [
        "# CELL 7C: MULTIMODAL FINE-TUNING\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"üöÄ Initializing Training ...\")\n",
        "\n",
        "# Device Setup\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üñ•Ô∏è Using device: {DEVICE}\")\n",
        "\n",
        "# 1. DATASET WITH TOKENIZATION\n",
        "class RealFashionDataset(Dataset):\n",
        "    def __init__(self, items, processor, transform=None):\n",
        "        self.items = items\n",
        "        self.processor = processor\n",
        "        self.transform = transform\n",
        "\n",
        "        # Create Label Mapping (Category Name -> Integer ID)\n",
        "        self.categories = sorted(list(set(item['category'] for item in items)))\n",
        "        self.label_map = {name: i for i, name in enumerate(self.categories)}\n",
        "        self.num_classes = len(self.categories)\n",
        "\n",
        "        print(f\"   üì¶ Training on {self.num_classes} classes: {self.categories[:5]}...\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            item = self.items[idx]\n",
        "            image = item['image']\n",
        "            text = f\"A photo of a {item['name']}\"\n",
        "\n",
        "            # 1. Image Transform (Augmentation)\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            # 2. Text Tokenization (CLIP Tokenizer)\n",
        "            text_inputs = self.processor(text=[text], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=77)\n",
        "\n",
        "            label_id = self.label_map[item['category']]\n",
        "\n",
        "            return {\n",
        "                'image': image,\n",
        "                'input_ids': text_inputs['input_ids'].squeeze(0),\n",
        "                'attention_mask': text_inputs['attention_mask'].squeeze(0),\n",
        "                'label': torch.tensor(label_id, dtype=torch.long)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error at idx {idx}: {type(e).__name__}: {e}\")\n",
        "            # Return next valid sample to prevent crash\n",
        "            return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "# 2. AUGMENTATION PIPELINE\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # 256px thumbnail ‚Üí 224px for CLIP\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.481, 0.457, 0.408], std=[0.268, 0.261, 0.275])\n",
        "])\n",
        "\n",
        "# 3. FUSION MODEL\n",
        "class RealFusionNetwork(nn.Module):\n",
        "    def __init__(self, backbone_model, num_classes):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone_model\n",
        "\n",
        "        # Freeze Backbone (Transfer Learning)\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Fusion Head\n",
        "        self.fusion_mlp = nn.Sequential(\n",
        "            nn.Linear(1024, 512),  # 512 (Image) + 512 (Text) = 1024\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, images, input_ids, attention_mask):\n",
        "        # 1. Get Real Features from Backbone (No Grad)\n",
        "        with torch.no_grad():\n",
        "            img_features = self.backbone.get_image_features(pixel_values=images)\n",
        "            txt_features = self.backbone.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Normalize\n",
        "            img_features = img_features / img_features.norm(p=2, dim=-1, keepdim=True)\n",
        "            txt_features = txt_features / txt_features.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "        # 2. Concatenate\n",
        "        combined = torch.cat((img_features, txt_features), dim=1)\n",
        "\n",
        "        # 3. Classify\n",
        "        logits = self.fusion_mlp(combined)\n",
        "        return logits\n",
        "\n",
        "# 4. TRAINING LOOP\n",
        "def train_fusion(items, clip_model, clip_processor, epochs=2):\n",
        "    print(f\"üèãÔ∏è Starting Real Fine-Tuning (Head Training)...\")\n",
        "\n",
        "    try:\n",
        "        # Setup Data\n",
        "        dataset = RealFashionDataset(items, clip_processor, transform=train_transforms)\n",
        "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "        # Setup Model\n",
        "        model = RealFusionNetwork(clip_model, dataset.num_classes).to(DEVICE)\n",
        "\n",
        "        # Optimizer (Only optimize the fusion_mlp parameters)\n",
        "        optimizer = optim.Adam(model.fusion_mlp.parameters(), lr=1e-3)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Metric History\n",
        "        history = {'loss': [], 'accuracy': []}\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "            for batch in progress_bar:\n",
        "                # Move to GPU\n",
        "                images = batch['image'].to(DEVICE)\n",
        "                input_ids = batch['input_ids'].to(DEVICE)\n",
        "                mask = batch['attention_mask'].to(DEVICE)\n",
        "                labels = batch['label'].to(DEVICE)\n",
        "\n",
        "                # Zero Grad\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward\n",
        "                outputs = model(images, input_ids, mask)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Backward\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Metrics\n",
        "                epoch_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                progress_bar.set_postfix({'loss': loss.item(), 'acc': correct/total})\n",
        "\n",
        "            avg_loss = epoch_loss / len(dataloader)\n",
        "            avg_acc = correct / total\n",
        "            history['loss'].append(avg_loss)\n",
        "            history['accuracy'].append(avg_acc)\n",
        "\n",
        "            print(f\"   ‚úÖ Epoch {epoch+1} Summary: Loss={avg_loss:.4f}, Accuracy={avg_acc:.4f}\")\n",
        "\n",
        "        return model, history\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Training failed: {type(e).__name__}: {e}\")\n",
        "        raise\n",
        "\n",
        "# --- EXECUTE TRAINING ---\n",
        "if 'clip_encoder' in globals():\n",
        "    raw_model = clip_encoder.model\n",
        "    raw_processor = clip_encoder.processor\n",
        "\n",
        "    trained_fusion_model, train_history = train_fusion(\n",
        "        processed_items,\n",
        "        raw_model,\n",
        "        raw_processor,\n",
        "        epochs=3\n",
        "    )\n",
        "\n",
        "    # Evaluation Plots\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(train_history['loss'], label='Training Loss', marker='o')\n",
        "    plt.plot(train_history['accuracy'], label='Training Accuracy', marker='s')\n",
        "    plt.title(\"Fine-Tuning Performance\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"‚úÖ Training complete!\")\n",
        "\n",
        "else:\n",
        "    raise RuntimeError(\"‚ö†Ô∏è 'clip_encoder' not found. Please run Cell 4 first!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lvRFRu4WiBn",
        "outputId": "63bc5a43-9ec5-4c95-ec4b-c32a4ffd682d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   üì¶ Training on 19 classes: ['cape', 'cardigan', 'coat', 'dress', 'glasses']...\n",
            "üìä Evaluation Metrics:\n",
            "   Precision: 0.9826\n",
            "   Recall:    0.9907\n",
            "   F1-Score:  0.9864\n"
          ]
        }
      ],
      "source": [
        "# CELL 7D: EVALUATION METRICS\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. DEFINE FUNCTION FIRST\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    \"\"\"Compute Precision, Recall, F1 on validation set\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            images = batch['image'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(images, input_ids, mask)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "    print(f\"üìä Evaluation Metrics:\")\n",
        "    print(f\"   Precision: {precision:.4f}\")\n",
        "    print(f\"   Recall:    {recall:.4f}\")\n",
        "    print(f\"   F1-Score:  {f1:.4f}\")\n",
        "\n",
        "    return {'precision': precision, 'recall': recall, 'f1': f1}\n",
        "\n",
        "# 2. THEN CREATE DATALOADER AND RUN\n",
        "eval_dataset = RealFashionDataset(processed_items, raw_processor, transform=train_transforms)\n",
        "dataloader = DataLoader(eval_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "if 'trained_fusion_model' in globals():\n",
        "    eval_metrics = evaluate_model(trained_fusion_model, dataloader, DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "457eb99a20dd42dbabcb2bab688d21fd",
            "2bf74ba6a6e541c080d5bcfe1f782409",
            "2724bab267d74eb688c703fd28d21404",
            "24e0f80387f3457981c730f33015023f",
            "91d2cb26971d48989bdac10ca6adcbf3",
            "5123de99605645168d1e758e299a66ec",
            "d5c4fc30f42a4bca9602c0fd5dd2a1f3",
            "e5afeeea2528400f82f7ddd8ca816e45",
            "e847f560edc94ec9bb85705cc4040582",
            "45e57d2bf0f54a1eafa2e00038cf9df1",
            "aca1953bd47b4e18953e21f439c491b2",
            "1ae3f1ab201045bebdc9ff8823ec35d6",
            "70acb816bce448139e2da53b9c7128b5",
            "f2c8000997c04677b8e6a1561db30292",
            "8012a8302cfa4d15857485bed93b2296",
            "22f5cf678cb447878a9b0e9c9afec107",
            "ef244afcada041bb9bb1bfc1e985c63a",
            "cf3a171741c64f869712a4ba7a803dda",
            "82c391512c5c42c59338edef00c8f0af",
            "57adeebbea7e4b1c92f6cc7f9e910854",
            "b4fc6a609312422f80ecfcb7b5729058",
            "20477765165748a3b68a9049b8f840cc",
            "f85a8031893444a08fc748c1c0a60d4a",
            "6ae88cc3af6946e6b51f1382b1cb72c0",
            "494790749dc34213870c5da1fd4bb7c7"
          ]
        },
        "id": "Cl2QKHEVCPA4",
        "outputId": "ef60fe2a-be10-4ef8-8fa7-fb3e26b27fc6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-868140246.py:8: UserWarning: WARNING: Unsloth should be imported before [transformers] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  from unsloth import FastLanguageModel\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "Loading 1B LLM...\n",
            "==((====))==  Unsloth 2026.1.4: Fast Llama patching. Transformers: 4.57.6.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.34. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "457eb99a20dd42dbabcb2bab688d21fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.03G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ae3f1ab201045bebdc9ff8823ec35d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f85a8031893444a08fc748c1c0a60d4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ae88cc3af6946e6b51f1382b1cb72c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "494790749dc34213870c5da1fd4bb7c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " LLM loaded!\n",
            "‚úÖ Ready!\n"
          ]
        }
      ],
      "source": [
        "# CELL 8: LLM 1B MODEL\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "from typing import List, Dict\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 1B model i(is the smaller)\n",
        "LLM_MODEL = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
        "\n",
        "print(\"Loading 1B LLM...\")\n",
        "\n",
        "llm_model, llm_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=LLM_MODEL,\n",
        "    max_seq_length=1024,\n",
        "    load_in_4bit=True,\n",
        "    dtype=None,\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(llm_model)\n",
        "print(\" LLM loaded!\")\n",
        "\n",
        "def generate_response(user_query: str, retrieved_products: List[Dict], intent: str) -> str:\n",
        "    # --- LAYER 1: CLEAN THE INPUT ---\n",
        "    # Build the context string carefully. Ensure we strictly select only text fields.\n",
        "    # Do NOT dump the whole dictionary, which might contain scores like {'score': 0.52}.\n",
        "    product_context_list = []\n",
        "    for i, p in enumerate(retrieved_products[:3]):\n",
        "        # Fallbacks for missing keys so the code doesn't break\n",
        "        name = p.get('name', 'Unknown Item')\n",
        "        category = p.get('category', 'Fashion Item')\n",
        "        # Ensure 'details' doesn't accidentally contain numbers from your database\n",
        "        details = p.get('description', 'A great style choice.')\n",
        "\n",
        "        entry = f\"Option {i+1}: {name} ({category}) - {details}\"\n",
        "        product_context_list.append(entry)\n",
        "\n",
        "    product_context = \"\\n\".join(product_context_list)\n",
        "\n",
        "    # --- LAYER 2: STRICT PROMPT ---\n",
        "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "You are a fashion stylist assistant.\n",
        "Your Goal: Recommend the items listed below to the user based on their request.\n",
        "\n",
        "STRICT RULES:\n",
        "1. Do NOT mention percentages, confidence scores, or numbers like \"52%\".\n",
        "2. Do NOT mention \"match score\" or technical details.\n",
        "3. Only discuss the style, color, and fabric of the items.\n",
        "4. Keep the response natural and helpful.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "User Request: \"{user_query}\"\n",
        "\n",
        "Available Items:\n",
        "{product_context}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        inputs = llm_tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "        outputs = llm_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=300,\n",
        "            temperature=0.6,\n",
        "            repetition_penalty=1.1,\n",
        "            do_sample=True,\n",
        "            pad_token_id=llm_tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        raw_response = llm_tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"assistant\")[-1].strip()\n",
        "\n",
        "        # --- LAYER 3: CLEAN THE OUTPUT (The Fail-Safe) ---\n",
        "        # This regex removes any number followed by a % sign (e.g., \"52%\", \"100%\")\n",
        "        # and replaces it with an empty string.\n",
        "        clean_response = re.sub(r'\\b\\d+%\\b', '', raw_response)\n",
        "\n",
        "        # Optional: Remove double spaces created by the deletion\n",
        "        clean_response = re.sub(r'\\s+', ' ', clean_response).strip()\n",
        "\n",
        "        return clean_response\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Check out this option: {retrieved_products[0]['name']}\"\n",
        "\n",
        "print(\"‚úÖ Ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cUL4W6stDbE6",
        "outputId": "f50d6de9-193a-422e-93e1-263f7c302ec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Initializing chatbot...\n",
            " Chatbot ready!\n",
            "\n",
            "üìù Test 1: Text-only query\n",
            "Intent: RECOMMEND\n",
            "Products found: 0\n",
            "Response: I couldn't find any fashion items matching that description. Can you try something else?...\n"
          ]
        }
      ],
      "source": [
        "# CELL 9:\n",
        "\n",
        "from typing import Dict, List, Optional\n",
        "import logging\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "TOP_K_RESULTS = 5\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "INTENT_LABELS = {0: \"SEARCH\", 1: \"RECOMMEND\", 2: \"COMPARE\", 3: \"DETAILS\", 4: \"SIMILAR\"}\n",
        "\n",
        "# Load tokenizer again\n",
        "intent_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "class FashionChatbot:\n",
        "    \"\"\"Complete multimodal fashion assistant\"\"\"\n",
        "\n",
        "    def __init__(self, clip_encoder, intent_classifier, search_index, audio_transcriber=None, fusion_model=None):\n",
        "        self.clip = clip_encoder\n",
        "        self.intent_model = intent_classifier\n",
        "        self.index = search_index\n",
        "        self.audio = audio_transcriber\n",
        "        self.fusion_model = fusion_model # Add fusion_model to init\n",
        "        print(\" Chatbot ready!\")\n",
        "\n",
        "    def classify_intent(self, text: str) -> str:\n",
        "        \"\"\"Classify user intent using BERT model\"\"\"\n",
        "        self.intent_model.eval()\n",
        "        with torch.no_grad():\n",
        "            inputs = intent_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128).to(DEVICE)\n",
        "            outputs = self.intent_model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "            pred = torch.argmax(outputs, dim=1).item()\n",
        "        return INTENT_LABELS.get(pred, \"SEARCH\")\n",
        "\n",
        "    def predict_category_fusion(self, image, text):\n",
        "\n",
        "        if not self.fusion_model:\n",
        "            return \"Unknown\"\n",
        "\n",
        "        # This part assumes a specific structure for fusion_model prediction\n",
        "        # For now, it's a placeholder. A proper implementation would need\n",
        "        # to align with the RealFusionNetwork's forward method.\n",
        "        # Example (simplified, assuming fusion_model expects raw image and tokenized text):\n",
        "        try:\n",
        "            # Preprocess image and text similarly to RealFashionDataset __getitem__\n",
        "            # This might require passing raw_processor to chatbot or making clip_processor accessible\n",
        "            # For simplicity, using clip encoder here, but fusion model might need different inputs\n",
        "\n",
        "            # If fusion model needs image processed for it:\n",
        "            img_processed = self.clip.processor(images=image, return_tensors=\"pt\").pixel_values.to(DEVICE)\n",
        "            txt_tokenized = self.clip.processor(text=[text], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=77).to(DEVICE)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.fusion_model(img_processed, txt_tokenized['input_ids'], txt_tokenized['attention_mask'])\n",
        "                _, predicted_idx = torch.max(outputs, 1)\n",
        "                # Need access to the dataset's category list to map idx back to name\n",
        "                # For now, return a placeholder or 'Unknown'\n",
        "                return \"Predicted_Category_Placeholder\" # Replace with actual mapping\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Fusion model prediction failed: {e}\")\n",
        "            return \"Unknown\"\n",
        "\n",
        "    def process_query(self, text: str = None, image = None, audio_path: str = None,\n",
        "                          weights={'image': 0.50, 'text': 0.35, 'audio': 0.15}) -> Dict:\n",
        "\n",
        "            # --- 1. PRE-PROCESSING & FUSION (Keep your Weighted Logic) ---\n",
        "            emb_dim = 512\n",
        "            img_emb = np.zeros(emb_dim, dtype='float32')\n",
        "            txt_emb = np.zeros(emb_dim, dtype='float32')\n",
        "            aud_emb = np.zeros(emb_dim, dtype='float32')\n",
        "            active_modalities = []\n",
        "            transcribed_text = \"\"\n",
        "\n",
        "            # Process Inputs\n",
        "            if audio_path and self.audio:\n",
        "                transcribed_text = self.audio.transcribe_file(audio_path) # Use transcribe_file for path\n",
        "                if transcribed_text:\n",
        "                    aud_emb = self.clip.encode_text(transcribed_text)\n",
        "                    active_modalities.append('audio')\n",
        "                    if not text: text = transcribed_text\n",
        "\n",
        "            if text:\n",
        "                txt_emb = self.clip.encode_text(text)\n",
        "                active_modalities.append('text')\n",
        "\n",
        "            if image is not None:\n",
        "                img_emb = self.clip.encode_image(image)\n",
        "                active_modalities.append('image')\n",
        "\n",
        "            if not active_modalities:\n",
        "                return {\"error\": \"No input\", \"response\": \"Please provide input.\", \"products\": []}\n",
        "\n",
        "            # Weighted Average\n",
        "            current_weights = {k: weights[k] for k in active_modalities}\n",
        "            total_weight = sum(current_weights.values())\n",
        "            final_embedding = np.zeros(emb_dim, dtype='float32')\n",
        "\n",
        "            if 'image' in active_modalities: final_embedding += img_emb * (weights['image']/total_weight)\n",
        "            if 'text' in active_modalities: final_embedding += txt_emb * (weights['text']/total_weight)\n",
        "            if 'audio' in active_modalities: final_embedding += aud_emb * (weights['audio']/total_weight)\n",
        "\n",
        "            norm = np.linalg.norm(final_embedding)\n",
        "            if norm > 0: final_embedding = final_embedding / norm\n",
        "\n",
        "            # --- 2. SEARCH ---\n",
        "            products = self.index.search(final_embedding, k=TOP_K_RESULTS)\n",
        "\n",
        "            # --- 3. FILTERING (The Fix) ---\n",
        "            valid_products = [p for p in products if p['score'] > 0.27]\n",
        "\n",
        "            intent = self.classify_intent(text if text else \"SEARCH\")\n",
        "\n",
        "            if not valid_products:\n",
        "                 return {\n",
        "                    \"intent\": intent,\n",
        "                    \"predicted_category\": None,\n",
        "                    \"query\": text,\n",
        "                    \"products\": [],\n",
        "                    \"response\": \"I couldn't find any fashion items matching that description. Can you try something else?\"\n",
        "                }\n",
        "\n",
        "            # --- 4. PREDICTION & GENERATION ---\n",
        "            predicted_category = \"Unknown\"\n",
        "            # Check if fusion_model exists before trying to use it\n",
        "            if self.fusion_model is not None and image is not None and text is not None:\n",
        "                try:\n",
        "                    cat_pred = self.predict_category_fusion(image, text)\n",
        "                    if cat_pred: predicted_category = cat_pred\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error during fusion model prediction: {e}\")\n",
        "                    pass # Keep 'Unknown'\n",
        "\n",
        "            context_text = text if text else transcribed_text\n",
        "            if predicted_category != \"Unknown\":\n",
        "                context_text += f\" (Looking for: {predicted_category})\"\n",
        "\n",
        "            # Pass 'valid_products' to the generator\n",
        "            response_text = generate_response(context_text, valid_products, intent)\n",
        "\n",
        "            return {\n",
        "                \"intent\": intent,\n",
        "                \"predicted_category\": predicted_category,\n",
        "                \"query\": text,\n",
        "                \"products\": valid_products, # Return the filtered list\n",
        "                \"response\": response_text\n",
        "            }\n",
        "\n",
        "# Initialize\n",
        "print(\"ü§ñ Initializing chatbot...\")\n",
        "chatbot = FashionChatbot(\n",
        "    clip_encoder=clip_encoder,\n",
        "    intent_classifier=intent_classifier,\n",
        "    search_index=search_index,\n",
        "    audio_transcriber=audio_transcriber if 'audio_transcriber' in globals() else None, # Use globals() to check for variable existence\n",
        "    fusion_model=trained_fusion_model if 'trained_fusion_model' in globals() else None # Pass the trained fusion model\n",
        ")\n",
        "\n",
        "# Test\n",
        "print(\"\\nüìù Test 1: Text-only query\")\n",
        "result = chatbot.process_query(text=\"I need a casual summer outfit\")\n",
        "print(f\"Intent: {result['intent']}\")\n",
        "print(f\"Products found: {len(result['products'])}\")\n",
        "print(f\"Response: {result['response'][:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hkGMWgAgiWYA",
        "outputId": "d6d014f9-71b4-4578-99bb-63bdc2da284b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "# app.py - Multimodal Fashion Assistant\n",
        "\n",
        "import streamlit as st\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import faiss\n",
        "import pickle\n",
        "import io\n",
        "import tempfile\n",
        "import librosa\n",
        "from transformers import CLIPProcessor, CLIPModel, WhisperProcessor, WhisperForConditionalGeneration\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "st.set_page_config(page_title=\"Fashion Assistant\", layout=\"wide\", page_icon=\"üëó\")\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ===== CACHED MODEL LOADING =====\n",
        "@st.cache_resource\n",
        "def load_prebuilt_data():\n",
        "    with open('fashion_data.pkl', 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    index = faiss.deserialize_index(data['index'])\n",
        "    products = []\n",
        "    for p in data['products']:\n",
        "        prod = {'name': p['name'], 'category': p['category']}\n",
        "        if p.get('image_bytes'):\n",
        "            prod['image'] = Image.open(io.BytesIO(p['image_bytes']))\n",
        "        products.append(prod)\n",
        "    return index, products\n",
        "\n",
        "@st.cache_resource\n",
        "def load_clip():\n",
        "    clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
        "    proc = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    return clip, proc\n",
        "\n",
        "@st.cache_resource\n",
        "def load_whisper():\n",
        "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\").to(DEVICE)\n",
        "    proc = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
        "    return model, proc\n",
        "\n",
        "@st.cache_resource\n",
        "def load_llm():\n",
        "    llm, tok = FastLanguageModel.from_pretrained(\n",
        "        \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\", max_seq_length=512, load_in_4bit=True\n",
        "    )\n",
        "    FastLanguageModel.for_inference(llm)\n",
        "    return llm, tok\n",
        "\n",
        "# Load models\n",
        "with st.spinner(\"üîÑ Loading models... Please wait\"):\n",
        "    faiss_index, products = load_prebuilt_data()\n",
        "    clip, clip_proc = load_clip()\n",
        "    whisper, whisper_proc = load_whisper()\n",
        "    llm, llm_tok = load_llm()\n",
        "\n",
        "# ===== HELPER FUNCTIONS =====\n",
        "def encode_text(text):\n",
        "    q = clip_proc(text=[\"a photo of \" + text], return_tensors=\"pt\", padding=True).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        e = clip.get_text_features(**q)\n",
        "        e = e / e.norm(dim=-1, keepdim=True)\n",
        "    return e.cpu().numpy()\n",
        "\n",
        "def encode_image(img):\n",
        "    inputs = clip_proc(images=img, return_tensors=\"pt\").to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        e = clip.get_image_features(**inputs)\n",
        "        e = e / e.norm(dim=-1, keepdim=True)\n",
        "    return e.cpu().numpy()\n",
        "\n",
        "def transcribe_audio(audio_bytes):\n",
        "    try:\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as f:\n",
        "            f.write(audio_bytes)\n",
        "            audio, sr = librosa.load(f.name, sr=16000)\n",
        "        inputs = whisper_proc(audio, sampling_rate=16000, return_tensors=\"pt\").to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            ids = whisper.generate(**inputs)\n",
        "        return whisper_proc.batch_decode(ids, skip_special_tokens=True)[0]\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def build_context(messages, max_turns=5):\n",
        "    \"\"\"Build conversation context from recent messages\"\"\"\n",
        "    recent = messages[-max_turns*2:] if len(messages) > max_turns*2 else messages\n",
        "    context = \"\"\n",
        "    for msg in recent:\n",
        "        role = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
        "        context += role + \": \" + msg[\"text\"] + chr(10)\n",
        "    return context\n",
        "\n",
        "def is_meta_command(text):\n",
        "    \"\"\"Detect if text is a command about the image, not a fashion description\"\"\"\n",
        "    meta_phrases = [\n",
        "        \"similar to the image\", \"like this\", \"like the image\",\n",
        "        \"find similar\", \"search similar\", \"match this\",\n",
        "        \"something like this\", \"this style\", \"similar to this\",\n",
        "        \"like the photo\", \"similar to the photo\", \"match the image\",\n",
        "        \"parecido\", \"similar\", \"como este\", \"como la imagen\"\n",
        "    ]\n",
        "    text_lower = text.lower()\n",
        "    return any(phrase in text_lower for phrase in meta_phrases)\n",
        "\n",
        "def generate_response(query, prods, context=\"\"):\n",
        "    \"\"\"Generate LLM response with conversation context\"\"\"\n",
        "    prod_list = \", \".join([p[\"name\"] for p in prods[:3]])\n",
        "\n",
        "    prompt = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\"\n",
        "    prompt += \"You are a helpful fashion assistant. Give brief, friendly suggestions based on the products found. \"\n",
        "    prompt += \"Use the conversation context to understand user preferences (gender, style, occasion). \"\n",
        "    prompt += \"Never refuse reasonable fashion requests.<|eot_id|>\"\n",
        "    prompt += \"<|start_header_id|>user<|end_header_id|>\"\n",
        "\n",
        "    if context:\n",
        "        prompt += \"Conversation history:\" + chr(10) + context + chr(10) + chr(10)\n",
        "\n",
        "    prompt += \"Current request: \" + query + chr(10)\n",
        "    prompt += \"Available products: \" + prod_list\n",
        "    prompt += \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
        "\n",
        "    try:\n",
        "        out = llm.generate(**llm_tok(prompt, return_tensors=\"pt\").to(DEVICE), max_new_tokens=150)\n",
        "        response = llm_tok.decode(out[0], skip_special_tokens=True).split(\"assistant\")[-1].strip()\n",
        "        return response if response else \"Here are some options that might interest you!\"\n",
        "    except:\n",
        "        return \"Here are some fashion items I found for you!\"\n",
        "\n",
        "def perform_search(text_query, image_query, voice_query):\n",
        "    \"\"\"\n",
        "    Multimodal search with weighted embeddings:\n",
        "    - Image only: 100% image\n",
        "    - Text only: 100% text\n",
        "    - Voice only: 100% voice\n",
        "    - Image + Text: 70% image, 30% text\n",
        "    - Image + Voice: 70% image, 30% voice\n",
        "    - Text + Voice: 50% text, 50% voice\n",
        "    - All three: 50% image, 25% text, 25% voice\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "    weights = []\n",
        "    description_parts = []\n",
        "\n",
        "    # Detect meta-commands\n",
        "    text_is_meta = text_query and is_meta_command(text_query)\n",
        "\n",
        "    # === IMAGE ===\n",
        "    if image_query is not None:\n",
        "        img_emb = encode_image(image_query)\n",
        "        embeddings.append(img_emb)\n",
        "        description_parts.append(\"üì∑ Image\")\n",
        "\n",
        "        if text_is_meta:\n",
        "            weights.append(1.0)\n",
        "        elif text_query and voice_query:\n",
        "            weights.append(0.50)\n",
        "        elif text_query or voice_query:\n",
        "            weights.append(0.70)\n",
        "        else:\n",
        "            weights.append(1.0)\n",
        "\n",
        "    # === TEXT ===\n",
        "    if text_query and not text_is_meta:\n",
        "        txt_emb = encode_text(text_query)\n",
        "        embeddings.append(txt_emb)\n",
        "        description_parts.append(\"üìù Text\")\n",
        "\n",
        "        if image_query is not None and voice_query:\n",
        "            weights.append(0.25)\n",
        "        elif image_query is not None:\n",
        "            weights.append(0.30)\n",
        "        elif voice_query:\n",
        "            weights.append(0.50)\n",
        "        else:\n",
        "            weights.append(1.0)\n",
        "\n",
        "    # === VOICE ===\n",
        "    if voice_query:\n",
        "        voice_emb = encode_text(voice_query)\n",
        "        embeddings.append(voice_emb)\n",
        "        description_parts.append(\"üé§ Voice\")\n",
        "\n",
        "        if image_query is not None and text_query and not text_is_meta:\n",
        "            weights.append(0.25)\n",
        "        elif image_query is not None:\n",
        "            weights.append(0.30)\n",
        "        elif text_query:\n",
        "            weights.append(0.50)\n",
        "        else:\n",
        "            weights.append(1.0)\n",
        "\n",
        "    if not embeddings:\n",
        "        return None, \"\", \"\"\n",
        "\n",
        "    # Combine embeddings with weights\n",
        "    query_emb = np.zeros_like(embeddings[0])\n",
        "    for emb, w in zip(embeddings, weights):\n",
        "        query_emb += emb * w\n",
        "    query_emb = query_emb / np.linalg.norm(query_emb)\n",
        "\n",
        "    # Build final text for LLM\n",
        "    final_text = \"\"\n",
        "    if voice_query:\n",
        "        final_text = voice_query\n",
        "    if text_query and not text_is_meta:\n",
        "        final_text = (final_text + \" \" + text_query).strip()\n",
        "    if not final_text:\n",
        "        final_text = \"Similar items to uploaded image\"\n",
        "\n",
        "    query_desc = \" + \".join(description_parts)\n",
        "\n",
        "    return query_emb, final_text, query_desc\n",
        "\n",
        "def do_search(text_q, img_q, voice_q):\n",
        "    \"\"\"Execute search and return results\"\"\"\n",
        "    query_emb, final_text, query_desc = perform_search(text_q, img_q, voice_q)\n",
        "\n",
        "    if query_emb is None:\n",
        "        return None, None, None, None\n",
        "\n",
        "    scores, ids = faiss_index.search(query_emb, 10)\n",
        "    results = []\n",
        "    for s, i in zip(scores[0], ids[0]):\n",
        "        if 0 <= i < len(products):\n",
        "            p = products[i].copy()\n",
        "            p[\"similarity\"] = max(0, min(100, float(s) * 100))\n",
        "            results.append(p)\n",
        "    results = results[:5]\n",
        "\n",
        "    return results, final_text, query_desc, query_emb\n",
        "\n",
        "# ===== SESSION STATE =====\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "if \"current_image\" not in st.session_state:\n",
        "    st.session_state.current_image = None\n",
        "if \"current_voice\" not in st.session_state:\n",
        "    st.session_state.current_voice = None\n",
        "if \"current_text\" not in st.session_state:\n",
        "    st.session_state.current_text = \"\"\n",
        "if \"file_uploader_key\" not in st.session_state:\n",
        "    st.session_state.file_uploader_key = 0\n",
        "if \"audio_processed\" not in st.session_state:\n",
        "    st.session_state.audio_processed = False\n",
        "if \"image_processed\" not in st.session_state:\n",
        "    st.session_state.image_processed = False\n",
        "if \"auto_search_voice\" not in st.session_state:\n",
        "    st.session_state.auto_search_voice = False\n",
        "if \"auto_search_image\" not in st.session_state:\n",
        "    st.session_state.auto_search_image = False\n",
        "\n",
        "# ===== MAIN UI =====\n",
        "st.title(\"üëó Fashion Assistant\")\n",
        "st.caption(\"üõçÔ∏è \" + str(len(products)) + \" products loaded | Multimodal Search: Text, Image & Voice\")\n",
        "\n",
        "# ===== SIDEBAR =====\n",
        "with st.sidebar:\n",
        "    st.header(\"üéØ How to Search\")\n",
        "\n",
        "    st.info(\"\"\"\n",
        "    **Use any input method:**\n",
        "    - üìù **Text** - Describe what you want\n",
        "    - üì∑ **Image** - Upload a photo (auto-search)\n",
        "    - üé§ **Voice** - Record your request (auto-search)\n",
        "\n",
        "    **Or combine them** for better results!\n",
        "    \"\"\")\n",
        "\n",
        "    st.divider()\n",
        "\n",
        "    # --- TEXT INPUT ---\n",
        "    st.subheader(\"üìù Text Query\")\n",
        "    text_input = st.text_area(\n",
        "        \"Describe what you want\",\n",
        "        value=st.session_state.current_text,\n",
        "        placeholder=\"e.g., blue summer dress, casual jacket for wedding...\",\n",
        "        height=80,\n",
        "        key=\"text_area_\" + str(st.session_state.file_uploader_key),\n",
        "        help=\"Type any fashion item, style, or occasion\"\n",
        "    )\n",
        "    if text_input:\n",
        "        st.session_state.current_text = text_input.strip()\n",
        "\n",
        "    st.divider()\n",
        "\n",
        "    # --- IMAGE INPUT (AUTO-SEARCH) ---\n",
        "    st.subheader(\"üì∑ Image Search\")\n",
        "    st.caption(\"Upload a photo ‚Üí Auto search similar items\")\n",
        "    uploaded_image = st.file_uploader(\n",
        "        \"Upload fashion image\",\n",
        "        type=[\"jpg\", \"jpeg\", \"png\"],\n",
        "        key=\"img_\" + str(st.session_state.file_uploader_key),\n",
        "        help=\"Upload a photo to find similar styles automatically\"\n",
        "    )\n",
        "\n",
        "    if uploaded_image and not st.session_state.image_processed:\n",
        "        img = Image.open(uploaded_image).convert(\"RGB\")\n",
        "        st.session_state.current_image = img\n",
        "        st.image(img, caption=\"‚úÖ Image loaded - Searching...\", use_container_width=True)\n",
        "        st.session_state.image_processed = True\n",
        "        st.session_state.auto_search_image = True\n",
        "        st.rerun()\n",
        "    elif st.session_state.current_image:\n",
        "        st.image(st.session_state.current_image, caption=\"‚úÖ Current image\", use_container_width=True)\n",
        "\n",
        "    st.divider()\n",
        "\n",
        "    # --- VOICE INPUT (AUTO-TRANSCRIBE + AUTO-SEARCH) ---\n",
        "    st.subheader(\"üé§ Voice Input\")\n",
        "    st.caption(\"Upload audio ‚Üí Auto transcribe & search\")\n",
        "    uploaded_audio = st.file_uploader(\n",
        "        \"Upload voice recording\",\n",
        "        type=[\"wav\", \"mp3\", \"m4a\"],\n",
        "        key=\"audio_\" + str(st.session_state.file_uploader_key),\n",
        "        help=\"Upload audio - will transcribe and search automatically\"\n",
        "    )\n",
        "\n",
        "    if uploaded_audio and not st.session_state.audio_processed:\n",
        "        st.audio(uploaded_audio)\n",
        "        with st.spinner(\"üéôÔ∏è Transcribing...\"):\n",
        "            transcribed = transcribe_audio(uploaded_audio.read())\n",
        "            if transcribed and transcribed.strip():\n",
        "                st.session_state.current_voice = transcribed.strip()\n",
        "                st.session_state.audio_processed = True\n",
        "                st.success(\"‚úÖ \" + transcribed)\n",
        "                st.session_state.auto_search_voice = True\n",
        "                st.rerun()\n",
        "            else:\n",
        "                st.error(\"‚ùå Could not transcribe audio. Please try again.\")\n",
        "\n",
        "    if st.session_state.current_voice:\n",
        "        st.success(\"üé§ \" + st.session_state.current_voice)\n",
        "\n",
        "    st.divider()\n",
        "\n",
        "    # --- ACTIVE INPUTS SUMMARY ---\n",
        "    active_inputs = []\n",
        "    if st.session_state.current_text:\n",
        "        active_inputs.append(\"üìù Text\")\n",
        "    if st.session_state.current_image:\n",
        "        active_inputs.append(\"üì∑ Image\")\n",
        "    if st.session_state.current_voice:\n",
        "        active_inputs.append(\"üé§ Voice\")\n",
        "\n",
        "    if active_inputs:\n",
        "        st.success(\"**Active inputs:** \" + \" + \".join(active_inputs))\n",
        "    else:\n",
        "        st.warning(\"üí° Add text, image, or voice to search\")\n",
        "\n",
        "    # --- BUTTONS ---\n",
        "    col1, col2 = st.columns(2)\n",
        "    with col1:\n",
        "        search_button = st.button(\"üîç Search\", type=\"primary\", use_container_width=True)\n",
        "    with col2:\n",
        "        clear_button = st.button(\"üóëÔ∏è Clear All\", use_container_width=True)\n",
        "\n",
        "    if clear_button:\n",
        "        st.session_state.messages = []\n",
        "        st.session_state.current_image = None\n",
        "        st.session_state.current_voice = None\n",
        "        st.session_state.current_text = \"\"\n",
        "        st.session_state.audio_processed = False\n",
        "        st.session_state.image_processed = False\n",
        "        st.session_state.auto_search_voice = False\n",
        "        st.session_state.auto_search_image = False\n",
        "        st.session_state.file_uploader_key += 1\n",
        "        st.rerun()\n",
        "\n",
        "    st.divider()\n",
        "    st.caption(\"üí° **Tip:** The more inputs you provide, the better the results!\")\n",
        "\n",
        "# ===== CHAT HISTORY =====\n",
        "for msg in st.session_state.messages:\n",
        "    with st.chat_message(msg[\"role\"]):\n",
        "        st.write(msg[\"text\"])\n",
        "        if msg.get(\"products\"):\n",
        "            cols = st.columns(min(5, len(msg[\"products\"])))\n",
        "            for i, p in enumerate(msg[\"products\"][:5]):\n",
        "                with cols[i]:\n",
        "                    if p.get(\"image\"):\n",
        "                        st.image(p[\"image\"], use_container_width=True)\n",
        "                    st.caption(p[\"name\"])\n",
        "                    similarity = int(min(100, max(0, p.get(\"similarity\", 50))))\n",
        "                    st.progress(similarity)\n",
        "                    st.caption(str(similarity) + \"% match\")\n",
        "\n",
        "# ===== AUTO-SEARCH: IMAGE =====\n",
        "if st.session_state.auto_search_image and st.session_state.current_image:\n",
        "    st.session_state.auto_search_image = False\n",
        "\n",
        "    results, final_text, query_desc, _ = do_search(\"\", st.session_state.current_image, \"\")\n",
        "\n",
        "    if results:\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"text\": \"üì∑ [Image search]\"})\n",
        "\n",
        "        context = build_context(st.session_state.messages)\n",
        "        response = generate_response(final_text, results, context)\n",
        "\n",
        "        st.session_state.messages.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"text\": response,\n",
        "            \"products\": results\n",
        "        })\n",
        "\n",
        "        # Clear image after search\n",
        "        st.session_state.current_image = None\n",
        "        st.session_state.image_processed = False\n",
        "        st.session_state.file_uploader_key += 1\n",
        "        st.rerun()\n",
        "\n",
        "# ===== AUTO-SEARCH: VOICE =====\n",
        "if st.session_state.auto_search_voice and st.session_state.current_voice:\n",
        "    st.session_state.auto_search_voice = False\n",
        "\n",
        "    results, final_text, query_desc, _ = do_search(\"\", None, st.session_state.current_voice)\n",
        "\n",
        "    if results:\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"text\": \"üé§ \" + st.session_state.current_voice})\n",
        "\n",
        "        context = build_context(st.session_state.messages)\n",
        "        response = generate_response(final_text, results, context)\n",
        "\n",
        "        st.session_state.messages.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"text\": response,\n",
        "            \"products\": results\n",
        "        })\n",
        "\n",
        "        # Clear voice after search\n",
        "        st.session_state.current_voice = None\n",
        "        st.session_state.audio_processed = False\n",
        "        st.session_state.file_uploader_key += 1\n",
        "        st.rerun()\n",
        "\n",
        "# ===== MANUAL SEARCH BUTTON =====\n",
        "if search_button:\n",
        "    text_q = st.session_state.current_text\n",
        "    img_q = st.session_state.current_image\n",
        "    voice_q = st.session_state.current_voice\n",
        "\n",
        "    if not text_q and not img_q and not voice_q:\n",
        "        st.warning(\"‚ö†Ô∏è Please add at least one input (text, image, or voice)!\")\n",
        "    else:\n",
        "        results, final_text, query_desc, _ = do_search(text_q, img_q, voice_q)\n",
        "\n",
        "        if results:\n",
        "            # Build user message\n",
        "            user_msg_parts = []\n",
        "            if voice_q:\n",
        "                user_msg_parts.append(\"üé§ \" + voice_q)\n",
        "            if text_q:\n",
        "                user_msg_parts.append(\"üìù \" + text_q)\n",
        "            if img_q:\n",
        "                user_msg_parts.append(\"üì∑ [image]\")\n",
        "            user_msg = \" | \".join(user_msg_parts)\n",
        "\n",
        "            st.session_state.messages.append({\"role\": \"user\", \"text\": user_msg})\n",
        "\n",
        "            context = build_context(st.session_state.messages)\n",
        "            response = generate_response(final_text, results, context)\n",
        "\n",
        "            st.session_state.messages.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"text\": response,\n",
        "                \"products\": results\n",
        "            })\n",
        "\n",
        "            # Clear all inputs\n",
        "            st.session_state.current_image = None\n",
        "            st.session_state.current_voice = None\n",
        "            st.session_state.current_text = \"\"\n",
        "            st.session_state.audio_processed = False\n",
        "            st.session_state.image_processed = False\n",
        "            st.session_state.file_uploader_key += 1\n",
        "            st.rerun()\n",
        "        else:\n",
        "            st.error(\"‚ùå No results found. Try a different search.\")\n",
        "\n",
        "# ===== QUICK CHAT INPUT =====\n",
        "st.divider()\n",
        "prompt = st.chat_input(\"üí¨ Quick text search (or use sidebar for multimodal)\")\n",
        "\n",
        "if prompt:\n",
        "    # Check if there's an image in sidebar to combine\n",
        "    img_q = st.session_state.current_image\n",
        "\n",
        "    results, final_text, query_desc, _ = do_search(prompt, img_q, None)\n",
        "\n",
        "    if results:\n",
        "        user_msg = prompt\n",
        "        if img_q:\n",
        "            user_msg += \" üì∑\"\n",
        "\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"text\": user_msg})\n",
        "\n",
        "        context = build_context(st.session_state.messages)\n",
        "        response = generate_response(final_text, results, context)\n",
        "\n",
        "        st.session_state.messages.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"text\": response,\n",
        "            \"products\": results\n",
        "        })\n",
        "\n",
        "        # Clear image if used\n",
        "        if img_q:\n",
        "            st.session_state.current_image = None\n",
        "            st.session_state.image_processed = False\n",
        "            st.session_state.file_uploader_key += 1\n",
        "\n",
        "        st.rerun()\n",
        "    else:\n",
        "        st.warning(\"No results found. Try different keywords.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyJ4Ml3e3NG9"
      },
      "outputs": [],
      "source": [
        "# CELL 11: STREAMLIT WITH NGROK\n",
        "\n",
        "# Install ngrok\n",
        "!pip install -q pyngrok\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "\n",
        "NGROK_TOKEN = \"374PsiGvVhW7UqpMna673SvzuWP_3EpvNX9W4uxYFo2iccwBX\"\n",
        "\n",
        "if NGROK_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "# Kill any existing Streamlit processes\n",
        "!pkill -f streamlit\n",
        "\n",
        "# Start Streamlit in background\n",
        "process = subprocess.Popen(\n",
        "    ['streamlit', 'run', 'app.py', '--server.port', '8501'],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE\n",
        ")\n",
        "\n",
        "# Waiting to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Create ngrok tunnel\n",
        "try:\n",
        "    # Close existing tunnels\n",
        "    tunnels = ngrok.get_tunnels()\n",
        "    for tunnel in tunnels:\n",
        "        ngrok.disconnect(tunnel.public_url)\n",
        "\n",
        "    # Create new tunnel\n",
        "    public_url = ngrok.connect(8501)\n",
        "    print(f\"\\n Fashion Chatbot is live at:\")\n",
        "    print(f\"   {public_url}\")\n",
        "    print(f\"\\n Note: Give some time while models initialize\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to create tunnel: {e}\")\n",
        "    print(\"Alternative: Use localtunnel with: !npx localtunnel --port 8501\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ae3f1ab201045bebdc9ff8823ec35d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70acb816bce448139e2da53b9c7128b5",
              "IPY_MODEL_f2c8000997c04677b8e6a1561db30292",
              "IPY_MODEL_8012a8302cfa4d15857485bed93b2296"
            ],
            "layout": "IPY_MODEL_22f5cf678cb447878a9b0e9c9afec107"
          }
        },
        "20477765165748a3b68a9049b8f840cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22f5cf678cb447878a9b0e9c9afec107": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24e0f80387f3457981c730f33015023f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45e57d2bf0f54a1eafa2e00038cf9df1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_aca1953bd47b4e18953e21f439c491b2",
            "value": "‚Äá1.03G/1.03G‚Äá[00:09&lt;00:00,‚Äá277MB/s]"
          }
        },
        "2724bab267d74eb688c703fd28d21404": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5afeeea2528400f82f7ddd8ca816e45",
            "max": 1027676737,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e847f560edc94ec9bb85705cc4040582",
            "value": 1027676737
          }
        },
        "2bf74ba6a6e541c080d5bcfe1f782409": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5123de99605645168d1e758e299a66ec",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d5c4fc30f42a4bca9602c0fd5dd2a1f3",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "457eb99a20dd42dbabcb2bab688d21fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2bf74ba6a6e541c080d5bcfe1f782409",
              "IPY_MODEL_2724bab267d74eb688c703fd28d21404",
              "IPY_MODEL_24e0f80387f3457981c730f33015023f"
            ],
            "layout": "IPY_MODEL_91d2cb26971d48989bdac10ca6adcbf3"
          }
        },
        "45e57d2bf0f54a1eafa2e00038cf9df1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5123de99605645168d1e758e299a66ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57adeebbea7e4b1c92f6cc7f9e910854": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "70acb816bce448139e2da53b9c7128b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef244afcada041bb9bb1bfc1e985c63a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_cf3a171741c64f869712a4ba7a803dda",
            "value": "generation_config.json:‚Äá100%"
          }
        },
        "8012a8302cfa4d15857485bed93b2296": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4fc6a609312422f80ecfcb7b5729058",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_20477765165748a3b68a9049b8f840cc",
            "value": "‚Äá234/234‚Äá[00:00&lt;00:00,‚Äá14.3kB/s]"
          }
        },
        "82c391512c5c42c59338edef00c8f0af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91d2cb26971d48989bdac10ca6adcbf3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aca1953bd47b4e18953e21f439c491b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4fc6a609312422f80ecfcb7b5729058": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf3a171741c64f869712a4ba7a803dda": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5c4fc30f42a4bca9602c0fd5dd2a1f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5afeeea2528400f82f7ddd8ca816e45": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e847f560edc94ec9bb85705cc4040582": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef244afcada041bb9bb1bfc1e985c63a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2c8000997c04677b8e6a1561db30292": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82c391512c5c42c59338edef00c8f0af",
            "max": 234,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_57adeebbea7e4b1c92f6cc7f9e910854",
            "value": 234
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}